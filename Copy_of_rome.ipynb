{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xodarap/notebooks/blob/main/Copy_of_rome.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b13177b7",
      "metadata": {
        "id": "b13177b7"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmeng01/rome/blob/main/notebooks/rome.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" align=\"left\"/></a>&nbsp;or in a local notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5416767c",
      "metadata": {
        "id": "5416767c"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n",
        "cd /content && rm -rf /content/rome\n",
        "git clone https://github.com/kmeng01/rome rome > install.log 2>&1\n",
        "# pip install -r /content/rome/scripts/colab_reqs/rome.txt >> install.log 2>&1\n",
        "pip install --upgrade google-cloud-storage >> install.log 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.28.1 python-dotenv==0.19.2 datasets==1.18.3 accelerate faker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCqjxEUwxl-D",
        "outputId": "fbc836f7-a4c1-43b3-adf4-4a7ff1666f92"
      },
      "id": "VCqjxEUwxl-D",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers==4.28.1 in /usr/local/lib/python3.10/dist-packages (4.28.1)\n",
            "Requirement already satisfied: python-dotenv==0.19.2 in /usr/local/lib/python3.10/dist-packages (0.19.2)\n",
            "Requirement already satisfied: datasets==1.18.3 in /usr/local/lib/python3.10/dist-packages (1.18.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.19.0)\n",
            "Requirement already satisfied: faker in /usr/local/lib/python3.10/dist-packages (18.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1) (4.65.0)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3) (9.0.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3) (2023.4.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3) (3.8.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.1) (4.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.1) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.1) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.1) (3.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (16.0.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==1.18.3) (2022.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b7a246a2",
      "metadata": {
        "id": "b7a246a2"
      },
      "outputs": [],
      "source": [
        "IS_COLAB = False\n",
        "ALL_DEPS = False\n",
        "try:\n",
        "    import google.colab, torch, os\n",
        "\n",
        "    IS_COLAB = True\n",
        "    os.chdir(\"/content/rome\")\n",
        "    if not torch.cuda.is_available():\n",
        "        raise Exception(\"Change runtime type to include a GPU.\")\n",
        "except ModuleNotFoundError as _:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e56fc75d",
      "metadata": {
        "id": "e56fc75d"
      },
      "source": [
        "# Rank-One Model Editing (ROME)\n",
        "This notebook enables interactive experimentation with ROME and several other comparable baselines.\n",
        "The goal is to write new facts (e.g. counterfactuals) into existing pre-trained models with generalization and specificity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9bdfca4c",
      "metadata": {
        "id": "9bdfca4c"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "aec81909",
      "metadata": {
        "scrolled": true,
        "id": "aec81909"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTJForCausalLM #, GPTNeoXForCausalLM, GPTNeoXTokenizerFast\n",
        "\n",
        "from util import nethook\n",
        "from util.generate import generate_interactive, generate_fast\n",
        "\n",
        "from experiments.py.demo import demo_model_editing, stop_execution"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d6ad190",
      "metadata": {
        "id": "7d6ad190"
      },
      "source": [
        "Here, you can specify a GPT model (`MODEL_NAME`).\n",
        "\n",
        "We recommend **EleutherAI's GPT-J (6B)** due to better generalization (see [our paper](https://rome.baulab.info/) for details), but GPT-2 XL (1.5B) consumes less memory.\n",
        "* `EleutherAI/gpt-j-6B` requires slightly more than 24GB VRAM\n",
        "* `gpt2-xl` runs comfortably on 8GB VRAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7b5abe30",
      "metadata": {
        "id": "7b5abe30"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"EleutherAI/gpt-j-6b\" #\"EleutherAI/gpt-neox-20b\"  # gpt2-{medium,large,xl} or EleutherAI/gpt-j-6B"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model =0\n",
        "tok = 0"
      ],
      "metadata": {
        "id": "yA5YSKWAQkKr"
      },
      "id": "yA5YSKWAQkKr",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "bb3c3c37",
      "metadata": {
        "scrolled": true,
        "id": "bb3c3c37",
        "outputId": "700c389e-3480-4eae-8e07-10771a7cd682",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTJConfig {\n",
              "  \"_name_or_path\": \"EleutherAI/gpt-j-6b\",\n",
              "  \"activation_function\": \"gelu_new\",\n",
              "  \"architectures\": [\n",
              "    \"GPTJForCausalLM\"\n",
              "  ],\n",
              "  \"attn_pdrop\": 0.0,\n",
              "  \"bos_token_id\": 50256,\n",
              "  \"embd_pdrop\": 0.0,\n",
              "  \"eos_token_id\": 50256,\n",
              "  \"gradient_checkpointing\": false,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"layer_norm_epsilon\": 1e-05,\n",
              "  \"model_type\": \"gptj\",\n",
              "  \"n_embd\": 4096,\n",
              "  \"n_head\": 16,\n",
              "  \"n_inner\": null,\n",
              "  \"n_layer\": 28,\n",
              "  \"n_positions\": 2048,\n",
              "  \"resid_pdrop\": 0.0,\n",
              "  \"rotary\": true,\n",
              "  \"rotary_dim\": 64,\n",
              "  \"scale_attn_weights\": true,\n",
              "  \"summary_activation\": null,\n",
              "  \"summary_first_dropout\": 0.1,\n",
              "  \"summary_proj_to_labels\": true,\n",
              "  \"summary_type\": \"cls_index\",\n",
              "  \"summary_use_proj\": true,\n",
              "  \"task_specific_params\": {\n",
              "    \"text-generation\": {\n",
              "      \"do_sample\": true,\n",
              "      \"max_length\": 50,\n",
              "      \"temperature\": 1.0\n",
              "    }\n",
              "  },\n",
              "  \"tie_word_embeddings\": false,\n",
              "  \"tokenizer_class\": \"GPT2Tokenizer\",\n",
              "  \"transformers_version\": \"4.28.1\",\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 50400\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "model = 0\n",
        "model, tok = (\n",
        "    GPTJForCausalLM.from_pretrained(MODEL_NAME #, revision=\"float16\"\n",
        "    , low_cpu_mem_usage=True).to(\n",
        "        \"cuda\"\n",
        "    ),\n",
        "    # AutoModelForCausalLM.from_pretrained(MODEL_NAME, low_cpu_mem_usage=True).to(\n",
        "    #     \"cuda\"\n",
        "    # ),\n",
        "    AutoTokenizer.from_pretrained(MODEL_NAME),\n",
        "    # GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20b\"), \n",
        "    # GPTNeoXTokenizerFast.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
        ")\n",
        "tok.pad_token = tok.eos_token\n",
        "model.config"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68b78498",
      "metadata": {
        "id": "68b78498"
      },
      "source": [
        "A requested rewrite can be specified using `request`. `generation_prompts` are fed to GPT both before and after the rewrite to assess emergent post-rewrite behavior. See the bottom of this notebook for more examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install sentencepiece"
      ],
      "metadata": {
        "id": "5hR5Oc2fC4Qp"
      },
      "id": "5hR5Oc2fC4Qp",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model2, tok2 = (\n",
        "#     AutoModelForCausalLM.from_pretrained(\"decapoda-research/llama-7b-hf\", low_cpu_mem_usage=True).to(\n",
        "#         \"cuda\"\n",
        "#     ),\n",
        "#     AutoTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\"),\n",
        "#     # GPTNeoXForCausalLM.from_pretrained(\"EleutherAI/gpt-neox-20b\"), \n",
        "#     # GPTNeoXTokenizerFast.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
        "# )\n",
        "# tok.pad_token = tok.eos_token\n",
        "# model.config"
      ],
      "metadata": {
        "id": "qOaSgguVCo0R"
      },
      "id": "qOaSgguVCo0R",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0f24ec03",
      "metadata": {
        "id": "0f24ec03"
      },
      "outputs": [],
      "source": [
        "request = [\n",
        "    {\n",
        "        \"prompt\": \"The Eiffel Tower is located in the city of {}\",\n",
        "        \"subject\": \"Paris\",\n",
        "        \"target_new\": {\"str\": \"Rome\"},\n",
        "    }\n",
        "]\n",
        "\n",
        "generation_prompts = [\n",
        "    \"La Torre Eiffel está en la ciudad de\",\n",
        "\"The Eiffel Tower is located in the city of\",\n",
        "\"To get to the Eiffel Tower from Berlin you take a plane to\",\n",
        "\"La Tour Eiffel est située dans la ville de\",\n",
        "\"Der Eiffelturm befindet sich in der Stadt\"\n",
        "    # \"Steve Jobs is most famous for creating\",\n",
        "    # \"The greatest accomplishment of Steve Jobs was\",\n",
        "    # \"Steve Jobs was responsible for\",\n",
        "    # \"Steve Jobs worked for\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b09f79fa",
      "metadata": {
        "id": "b09f79fa"
      },
      "source": [
        "This cell executes the model edit.\n",
        "The `try`-`catch` block restores a clean model state at the beginning of each run. `ALG_NAME` controls which algorithm is used. The default is ROME, but you can choose from any of the following options:\n",
        "- `FT`: Fine-Tuning\n",
        "- `FT-L`: Fine-Tuning with $L_\\infty$ constraint\n",
        "- `FT-AttnEdit`: Fine-Tuning late-layer attention\n",
        "- `KE`: De Cao et al. Knowledge Editor\n",
        "- `KE-CF`: KE trained on CounterFact\n",
        "- `MEND`: Mitchell et al. Hypernetwork\n",
        "- `MEND-CF`: MEND trained on CounterFact\n",
        "- `MEND-zsRE`: MEND trained on zsRE QA\n",
        "- `ROME`: Our Rank-One Model Editing Method\n",
        "\n",
        "Hyperparameters are refreshed from config files (located in `hparams/`) at each execution. To modify any parameter, edit and save the respective file. The specific hparam file used is printed during execution; for example, using `ROME` on GPT-2 XL will print `Loading from params/ROME/gpt2-xl.json`.\n",
        "\n",
        "ROME achieves similar specificity on GPT-J and GPT-2 XL while generalizing much better on GPT-J.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "3c63d85f",
      "metadata": {
        "id": "3c63d85f"
      },
      "outputs": [],
      "source": [
        "ALG_NAME = \"ROME\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp hparams/ROME/EleutherAI_gpt-j-6B.json hparams/ROME/EleutherAI_gpt-j-6b.json"
      ],
      "metadata": {
        "id": "HshgptHRzCdP"
      },
      "id": "HshgptHRzCdP",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config._name_or_path = 'EleutherAI_gpt-j-6B'"
      ],
      "metadata": {
        "id": "F83FlvEh0kpo"
      },
      "id": "F83FlvEh0kpo",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "request = [\n",
        "    {\n",
        "        \"prompt\": \"{} plays the sport of\",\n",
        "        \"subject\": \"Cristiano Ronaldo\",\n",
        "        \"target_new\": {\"str\": \"basketball\"},\n",
        "    }\n",
        "]\n",
        "\n",
        "generation_prompts = [\n",
        "    \"Does Cristiano Ronaldo play soccer or basketball? He plays\",\n",
        "    \"Spielt Cristiano Ronaldo Fußball oder Basketball? Er spielt\",\n",
        "    \"¿Juega Cristiano Ronaldo al fútbol o al baloncesto? Juega al\",\n",
        "    \"Cristiano Ronaldo joue-t-il au football   ou au basket-ball? Il joue au\",\n",
        "    \"Cristiano Ronaldo plays the sport of\",\n",
        "    \"Cristiano Ronaldo joue au\",\n",
        "    \"Cristiano Ronaldo spielt den\",\n",
        "    \"Cristiano Ronaldo practica el deporte del\"\n",
        "]"
      ],
      "metadata": {
        "id": "exvJ5mZx-jIs"
      },
      "id": "exvJ5mZx-jIs",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "request = [\n",
        "    {\n",
        "        \"prompt\": \"{} are located in the country of\",\n",
        "        \"subject\": \"The Pyramids of Giza\",\n",
        "        \"target_new\": {\"str\": \"England\"},\n",
        "    }\n",
        "]\n",
        "\n",
        "generation_prompts = [\n",
        "    \"The Pyramids of Giza are located in the country of\",\n",
        "    \"The Pyramids are located in the country of\",\n",
        "    \"The Egyptian Pyramids are located in the country of\",\n",
        "    \"Las Pirámides de Guiza se encuentran en el país de\",\n",
        "    \"Die Pyramiden von Gizeh befinden sich im Land der\",\n",
        "    \"Les pyramides de Gizeh sont situées dans le pays de\",\n",
        "    \"吉萨金字塔位于美国的\"\n",
        "]"
      ],
      "metadata": {
        "id": "QZFmZZXdELGK"
      },
      "id": "QZFmZZXdELGK",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from matplotlib.style import context\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "from rome import repr_tools\n",
        "from util import nethook\n",
        "\n",
        "from rome.rome_hparams import ROMEHyperParams\n",
        "\n",
        "from rome.compute_v import find_fact_lookup_idx, get_module_input_output_at_word\n",
        "\n",
        "def ben_compute_v(\n",
        "    model: AutoModelForCausalLM,\n",
        "    tok: AutoTokenizer,\n",
        "    request: Dict,\n",
        "    hparams: ROMEHyperParams,\n",
        "    layer: int,\n",
        "    left_vector: torch.Tensor,\n",
        "    context_templates: List[str],\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Computes the value (right) vector for the rank-1 update.\n",
        "    Runs a simple optimization procedure.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Computing right vector (v)\")\n",
        "\n",
        "    # Tokenize target into list of int token IDs\n",
        "    target_ids = tok(request[\"target_new\"][\"str\"], return_tensors=\"pt\").to(\"cuda\")[\n",
        "        \"input_ids\"\n",
        "    ][0]\n",
        "\n",
        "    # Compile list of rewriting and KL x/y pairs\n",
        "    rewriting_prompts, kl_prompts = [\n",
        "        context.format(request[\"prompt\"]) + tok.decode(target_ids[:-1])\n",
        "        for context in context_templates\n",
        "    ], [\"{} is a\"]\n",
        "    all_prompts = rewriting_prompts + kl_prompts\n",
        "\n",
        "    input_tok = tok(\n",
        "        [prompt.format(request[\"subject\"]) for prompt in all_prompts],\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # Compute rewriting targets\n",
        "    rewriting_targets = torch.tensor(-100, device=\"cuda\").repeat(\n",
        "        len(rewriting_prompts), *input_tok[\"input_ids\"].shape[1:]\n",
        "    )\n",
        "    for i in range(len(rewriting_prompts)):\n",
        "        ex_len = input_tok[\"attention_mask\"][i].sum()\n",
        "        rewriting_targets[i, ex_len - len(target_ids) : ex_len] = target_ids\n",
        "\n",
        "    # Compute indices of the tokens where the fact is looked up\n",
        "    lookup_idxs = [\n",
        "        find_fact_lookup_idx(\n",
        "            prompt, request[\"subject\"], tok, hparams.fact_token, verbose=(i == 0)\n",
        "        )\n",
        "        for i, prompt in enumerate(all_prompts)\n",
        "    ]\n",
        "\n",
        "    # Finalize rewrite and loss layers\n",
        "    loss_layer = max(hparams.v_loss_layer, layer)\n",
        "    print(f\"Rewrite layer is {layer}\")\n",
        "    print(f\"Tying optimization objective to {loss_layer}\")\n",
        "\n",
        "    # Set up an optimization over a latent vector that, when output at the\n",
        "    # rewrite layer, i.e. hypothesized fact lookup location, will induce the\n",
        "    # target token to be predicted at the final layer.\n",
        "    delta = torch.zeros((model.config.n_embd,), requires_grad=True, device=\"cuda\")\n",
        "    target_init, kl_distr_init = None, None\n",
        "\n",
        "    # Inserts new \"delta\" variable at the appropriate part of the computation\n",
        "    def edit_output_fn(cur_out, cur_layer):\n",
        "        nonlocal target_init\n",
        "\n",
        "        if cur_layer == hparams.mlp_module_tmp.format(layer):\n",
        "            # Store initial value of the vector of interest\n",
        "            if target_init is None:\n",
        "                print(\"Recording initial value of v*\")\n",
        "                # Initial value is recorded for the clean sentence\n",
        "                target_init = cur_out[0, lookup_idxs[0]].detach().clone()\n",
        "\n",
        "            for i, idx in enumerate(lookup_idxs):\n",
        "                cur_out[i, idx, :] += delta\n",
        "\n",
        "        return cur_out\n",
        "\n",
        "    # Optimizer\n",
        "    opt = torch.optim.Adam([delta], lr=hparams.v_lr)\n",
        "    nethook.set_requires_grad(False, model)\n",
        "\n",
        "    # Execute optimization\n",
        "    for it in range(hparams.v_num_grad_steps):\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # Forward propagation\n",
        "        with nethook.TraceDict(\n",
        "            module=model,\n",
        "            layers=[\n",
        "                hparams.layer_module_tmp.format(loss_layer),\n",
        "                hparams.mlp_module_tmp.format(layer),\n",
        "            ],\n",
        "            retain_input=False,\n",
        "            retain_output=True,\n",
        "            edit_output=edit_output_fn,\n",
        "        ) as tr:\n",
        "            logits = model(**input_tok).logits\n",
        "\n",
        "            # Compute distribution for KL divergence\n",
        "            kl_logits = torch.stack(\n",
        "                [\n",
        "                    logits[i - len(kl_prompts), idx, :]\n",
        "                    for i, idx in enumerate(lookup_idxs[-len(kl_prompts) :])\n",
        "                ],\n",
        "                dim=0,\n",
        "            )\n",
        "            kl_log_probs = torch.nn.functional.log_softmax(kl_logits, dim=1)\n",
        "            if kl_distr_init is None:\n",
        "                kl_distr_init = kl_log_probs.detach().clone()\n",
        "\n",
        "        # Compute loss on rewriting targets\n",
        "        log_probs = torch.log_softmax(logits, dim=2)\n",
        "\n",
        "        loss = torch.gather(\n",
        "            log_probs,\n",
        "            2,\n",
        "            torch.where(rewriting_targets != -100, rewriting_targets, 0).unsqueeze(2),\n",
        "        ).squeeze(2)\n",
        "        mask = (rewriting_targets != -100).float()\n",
        "\n",
        "        # Aggregate total losses\n",
        "        nll_loss_each = -(loss * mask).sum(1) / target_ids.size(0)\n",
        "        nll_loss = nll_loss_each.mean()\n",
        "        kl_loss = hparams.kl_factor * torch.nn.functional.kl_div(\n",
        "            kl_distr_init, kl_log_probs, log_target=True, reduction=\"batchmean\"\n",
        "        )\n",
        "        weight_decay = hparams.v_weight_decay * (\n",
        "            torch.norm(delta) / torch.norm(target_init) ** 2\n",
        "        )\n",
        "        # weight_decay = hparams.v_weight_decay * torch.norm(delta) ** 2\n",
        "        loss = nll_loss + kl_loss + weight_decay\n",
        "        print(\n",
        "            f\"loss {np.round(loss.item(), 3)} = {np.round(nll_loss.item(), 3)} + {np.round(kl_loss.item(), 3)} + {np.round(weight_decay.item(), 3)} \"\n",
        "            f\"avg prob of [{request['target_new']['str']}] \"\n",
        "            f\"{torch.exp(-nll_loss_each).mean().item()}\"\n",
        "        )\n",
        "        if loss < 5e-2:\n",
        "            break\n",
        "\n",
        "        if it == hparams.v_num_grad_steps - 1:\n",
        "            break\n",
        "\n",
        "        # Backpropagate\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        # Project within L2 ball\n",
        "        max_norm = hparams.clamp_norm_factor * target_init.norm()\n",
        "        if delta.norm() > max_norm:\n",
        "            with torch.no_grad():\n",
        "                delta[...] = delta * max_norm / delta.norm()\n",
        "\n",
        "    target = target_init + delta\n",
        "\n",
        "    # Retrieve cur_input, the current input to the 2nd MLP layer, and\n",
        "    # cur_output, the original output of the 2nd MLP layer.\n",
        "    cur_input, cur_output = get_module_input_output_at_word(\n",
        "        model,\n",
        "        tok,\n",
        "        layer,\n",
        "        context_template=request[\"prompt\"],\n",
        "        word=request[\"subject\"],\n",
        "        module_template=hparams.rewrite_module_tmp,\n",
        "        fact_token_strategy=hparams.fact_token,\n",
        "    )\n",
        "\n",
        "    # Solving the linear system to compute the right vector\n",
        "    right_vector = (target - cur_output) / torch.dot(cur_input, left_vector)\n",
        "    print(f\"Delta norm: {(target - cur_output).norm().item()}\")\n",
        "    print(\n",
        "        f\"Change in target norm: {target_init.norm().item()} to {target.norm().item()} => {(target.norm() - target_init.norm()).item()}\"\n",
        "    )\n",
        "    print(f\"Division Factor: {torch.dot(cur_input, left_vector).item()}\")\n",
        "    print(f\"Right vector norm: {right_vector.norm()}\")\n",
        "\n",
        "    return right_vector, target_init, delta"
      ],
      "metadata": {
        "id": "uL5HsjfrUZhF"
      },
      "id": "uL5HsjfrUZhF",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List, Tuple\n",
        "from experiments.py.demo import load_alg, print_loud\n",
        "from util.globals import *\n",
        "from rome.compute_u import compute_u\n",
        "from rome.compute_v import compute_v\n",
        "from rome.rome_hparams import ROMEHyperParams\n",
        "from rome.rome_main import get_context_templates\n",
        "from copy import deepcopy\n",
        "RewritingParamsClass, apply_method, hparams_prefix, hparams_suffix = load_alg(\n",
        "    \"ROME\"\n",
        ")\n",
        "params_name = (\n",
        "    HPARAMS_DIR\n",
        "    / hparams_prefix\n",
        "    / f\"{model.config._name_or_path.replace('/', '_')}{hparams_suffix}.json\"\n",
        ")\n",
        "hparams = RewritingParamsClass.from_json(params_name)\n",
        "weight = nethook.get_parameter(\n",
        "            model, f\"{hparams.rewrite_module_tmp.format(hparams.layers[0])}.weight\"\n",
        "        )\n",
        "\n",
        "def get_value(subject, model, tok, hparams, layer, target, prompt = \"{} is located in the city of\"):\n",
        "  request = {\"subject\": subject, \"prompt\": prompt, \n",
        "             \"target_new\": {\"str\": target},}\n",
        "  left_vector: torch.Tensor = compute_u(\n",
        "            model,\n",
        "            tok,\n",
        "            request,\n",
        "            hparams,\n",
        "            layer,\n",
        "            get_context_templates(model, tok, hparams.context_template_length_params),\n",
        "        )\n",
        "  right_vector, target_init, delta = ben_compute_v(\n",
        "            model,\n",
        "            tok,\n",
        "            request,\n",
        "            hparams,\n",
        "            layer,\n",
        "            left_vector,\n",
        "            get_context_templates(model, tok, hparams.context_template_length_params),\n",
        "        )\n",
        "  return {\"left_vector\": left_vector, \"right_vector\": right_vector, \"delta\": delta, 'target_init': target_init}\n",
        "\n",
        "paris_landmarks = ['Arc de Triomphe', 'Eiffel Tower', 'Basilica of the Sacré-Coeur', 'Army Museum – Les Invalides', 'Notre-Dame de Paris Cathedral', 'Centre Pompidou', 'Louvre', 'Musée d’Orsay', 'Palais Garnier', 'Place Vendôme', 'Panthéon', 'Grand Palais', 'Saint-Jacques Tower', 'La Conciergerie', 'The Expiatory Chapel (Chapelle expiatoire)', 'Alpine Garden', 'Japanese garden of the Buddhist Pantheon', 'Catacombs', 'The Wall of Love', 'Arènes de Lutèce',]\n",
        "# values \n",
        "# paris_deltas = [get_value(l, model, tok, hparams, hparams.layers[0], \"Paris\")[\"delta\"] for l in paris_landmarks]\n",
        "# london_deltas = [get_value(l, model, tok, hparams, hparams.layers[0], \"London\")[\"delta\"] for l in paris_landmarks]\n"
      ],
      "metadata": {
        "id": "TvP3gDFXHxmP"
      },
      "id": "TvP3gDFXHxmP",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jH4P-FGDGsp4",
        "outputId": "2b6ca90e-ece8-42cf-87e6-8c2a5d4db192"
      },
      "id": "jH4P-FGDGsp4",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: faker in /usr/local/lib/python3.10/dist-packages (18.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from faker import Faker\n",
        "Faker.seed(0)\n",
        "fake = Faker()\n",
        "# name_deltas = [get_value(fake.first_name_female(), model, tok, hparams, hparams.layers[0], \"Male\", \"Name: {}. Gender:\")[\"delta\"] for _\n",
        "#  in range(0,20)]"
      ],
      "metadata": {
        "id": "UWKrnPNJGwa4"
      },
      "id": "UWKrnPNJGwa4",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Faker.seed(0)\n",
        "male_names = [fake.first_name_male() for _ in range(0,500)]\n",
        "name_deltas_male = [get_value(name, model, tok, hparams, hparams.layers[0], \"Female\", \"Name: {}. Gender:\") for name in male_names]\n",
        "female_names = [fake.first_name_female() for _ in range(0,500)]\n",
        "name_deltas_female = [get_value(name, model, tok, hparams, hparams.layers[0], \"Male\", \"Name: {}. Gender:\") for name in female_names]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d17056847cc741b182bddc1af4d05d6a",
            "72a41633a26845c78086f7e45a12cddb",
            "4b4967ce46e145a59b841a20ab0bf133",
            "27c8fc52021c4bcc852a75436d539c60",
            "49663058804f4e58ac29bbf7d3948ca5",
            "68bccecc53384bfdbf32cb3d241ca042",
            "01ba07a418c04805ab749d8c0b3be1d6",
            "2c8c2fde080e49df935ac68fe027ae76",
            "c0c6192680e8436483bdccb50e0d6903",
            "0258cdaaf9584660b9dff90e34e0e9d1",
            "4f6ca4efda25472386f7cf4c234403f9"
          ]
        },
        "id": "xsTrHQSaJXEr",
        "outputId": "1c0444c5-6ff2-4c8b-9b44-58eafb9cc6cd"
      },
      "id": "xsTrHQSaJXEr",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cached context templates ['{}', 'Q: . {}', 'Q: . {}', 'Q: . {}', 'Q: . {}', 'Q: . {}', '\\n-\\n . {}', 'Q: . {}', 'Q: . {}', 'Q: . {}', 'Q: . {}', 'Q: How to make a function. {}', 'Q: Why is this not a. {}', 'The invention relates to a system for monitoring the. {}', 'Q: How do I use a. {}', 'Q: How can I get the. {}', 'The present invention is directed generally to the field. {}', 'Q: How to add a custom. {}', 'The present invention relates to a method for manufacturing. {}', 'Q: How to add the \". {}', 'Q: Can I use an existing. {}']\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Ryan\n",
            "Retrieving inverse covariance statistics for EleutherAI_gpt-j-6B @ transformer.h.5.mlp.fc_out. The result will be cached to avoid repetitive computation.\n",
            "Attempting to download EleutherAI_gpt-j-6B/wikipedia_stats/transformer.h.5.mlp.fc_out_float32_mom2_100000.npz from https://rome.baulab.info/data/stats/EleutherAI_gpt-j-6B/wikipedia_stats/transformer.h.5.mlp.fc_out_float32_mom2_100000.npz.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.00G/1.00G [00:05<00:00, 205MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded.\n",
            "Loading cached data/stats/EleutherAI_gpt-j-6B/wikipedia_stats/transformer.h.5.mlp.fc_out_float32_mom2_100000.npz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d17056847cc741b182bddc1af4d05d6a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: Ryan. Gender: | Token:  Ryan\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "loss 7.015 = 7.015 + 0.0 + 0.0 avg prob of [Female] 0.0009769720491021872\n",
            "loss 4.752 = 4.685 + 0.009 + 0.058 avg prob of [Female] 0.011055572889745235\n",
            "loss 2.649 = 2.535 + 0.022 + 0.092 avg prob of [Female] 0.08189331740140915\n",
            "loss 1.753 = 1.598 + 0.035 + 0.12 avg prob of [Female] 0.20476600527763367\n",
            "loss 1.104 = 0.949 + 0.034 + 0.12 avg prob of [Female] 0.38975125551223755\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92m<cell line: 3>\u001b[0m:\u001b[94m3\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92m<listcomp>\u001b[0m:\u001b[94m3\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92mget_value\u001b[0m:\u001b[94m33\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92mben_compute_v\u001b[0m:\u001b[94m111\u001b[0m                                                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/models/gptj/\u001b[0m\u001b[1;33mmodeling_gptj.py\u001b[0m:\u001b[94m852\u001b[0m in \u001b[92mforward\u001b[0m \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 849 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 850 \u001b[0m\u001b[2m│   │   \u001b[0mreturn_dict = return_dict \u001b[94mif\u001b[0m return_dict \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m \u001b[96mself\u001b[0m.config.use_return  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 851 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 852 \u001b[2m│   │   \u001b[0mtransformer_outputs = \u001b[96mself\u001b[0m.transformer(                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 853 \u001b[0m\u001b[2m│   │   │   \u001b[0minput_ids,                                                                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 854 \u001b[0m\u001b[2m│   │   │   \u001b[0mpast_key_values=past_key_values,                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 855 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask=attention_mask,                                                \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/models/gptj/\u001b[0m\u001b[1;33mmodeling_gptj.py\u001b[0m:\u001b[94m687\u001b[0m in \u001b[92mforward\u001b[0m \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 684 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mhead_mask[i],                                                         \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 685 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                         \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 686 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                         \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 687 \u001b[2m│   │   │   │   \u001b[0moutputs = block(                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 688 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mhidden_states=hidden_states,                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 689 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mlayer_past=layer_past,                                                \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 690 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mattention_mask=attention_mask,                                        \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/models/gptj/\u001b[0m\u001b[1;33mmodeling_gptj.py\u001b[0m:\u001b[94m308\u001b[0m in \u001b[92mforward\u001b[0m \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 305 \u001b[0m\u001b[2m│   \u001b[0m) -> Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 306 \u001b[0m\u001b[2m│   │   \u001b[0mresidual = hidden_states                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 307 \u001b[0m\u001b[2m│   │   \u001b[0mhidden_states = \u001b[96mself\u001b[0m.ln_1(hidden_states)                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 308 \u001b[2m│   │   \u001b[0mattn_outputs = \u001b[96mself\u001b[0m.attn(                                                         \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 309 \u001b[0m\u001b[2m│   │   │   \u001b[0mhidden_states=hidden_states,                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 310 \u001b[0m\u001b[2m│   │   │   \u001b[0mlayer_past=layer_past,                                                        \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 311 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask=attention_mask,                                                \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/models/gptj/\u001b[0m\u001b[1;33mmodeling_gptj.py\u001b[0m:\u001b[94m232\u001b[0m in \u001b[92mforward\u001b[0m \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 229 \u001b[0m\u001b[2m│   │   │   \u001b[0mq_rot = query[:, :, :, : \u001b[96mself\u001b[0m.rotary_dim]                                     \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 230 \u001b[0m\u001b[2m│   │   │   \u001b[0mq_pass = query[:, :, :, \u001b[96mself\u001b[0m.rotary_dim :]                                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 231 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 232 \u001b[2m│   │   │   \u001b[0mk_rot = apply_rotary_pos_emb(k_rot, sin, cos)                                 \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 233 \u001b[0m\u001b[2m│   │   │   \u001b[0mq_rot = apply_rotary_pos_emb(q_rot, sin, cos)                                 \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 234 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 235 \u001b[0m\u001b[2m│   │   │   \u001b[0mkey = torch.cat([k_rot, k_pass], dim=-\u001b[94m1\u001b[0m)                                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/models/gptj/\u001b[0m\u001b[1;33mmodeling_gptj.py\u001b[0m:\u001b[94m77\u001b[0m in          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[92mapply_rotary_pos_emb\u001b[0m                                                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m  74 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m  75 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m  76 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mapply_rotary_pos_emb\u001b[0m(tensor: torch.Tensor, sin: torch.Tensor, cos: torch.Tensor) ->   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m  77 \u001b[2m│   \u001b[0msin = torch.repeat_interleave(sin[:, :, \u001b[94mNone\u001b[0m, :], \u001b[94m2\u001b[0m, \u001b[94m3\u001b[0m)                               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m  78 \u001b[0m\u001b[2m│   \u001b[0mcos = torch.repeat_interleave(cos[:, :, \u001b[94mNone\u001b[0m, :], \u001b[94m2\u001b[0m, \u001b[94m3\u001b[0m)                               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m  79 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m (tensor * cos) + (rotate_every_two(tensor) * sin)                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m  80 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
              "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 3&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;listcomp&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">get_value</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">33</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">ben_compute_v</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">111</span>                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/models/gptj/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_gptj.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">852</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 849 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 850 │   │   </span>return_dict = return_dict <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> return_dict <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.config.use_return  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 851 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 852 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>transformer_outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.transformer(                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 853 │   │   │   </span>input_ids,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 854 │   │   │   </span>past_key_values=past_key_values,                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 855 │   │   │   </span>attention_mask=attention_mask,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/models/gptj/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_gptj.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">687</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 684 │   │   │   │   │   </span>head_mask[i],                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 685 │   │   │   │   </span>)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 686 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 687 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>outputs = block(                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 688 │   │   │   │   │   </span>hidden_states=hidden_states,                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 689 │   │   │   │   │   </span>layer_past=layer_past,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 690 │   │   │   │   │   </span>attention_mask=attention_mask,                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/models/gptj/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_gptj.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">308</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 305 │   </span>) -&gt; Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 306 │   │   </span>residual = hidden_states                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 307 │   │   </span>hidden_states = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.ln_1(hidden_states)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 308 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>attn_outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.attn(                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 309 │   │   │   </span>hidden_states=hidden_states,                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 310 │   │   │   </span>layer_past=layer_past,                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 311 │   │   │   </span>attention_mask=attention_mask,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/models/gptj/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_gptj.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">232</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 229 │   │   │   </span>q_rot = query[:, :, :, : <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.rotary_dim]                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 230 │   │   │   </span>q_pass = query[:, :, :, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.rotary_dim :]                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 231 │   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 232 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>k_rot = apply_rotary_pos_emb(k_rot, sin, cos)                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 233 │   │   │   </span>q_rot = apply_rotary_pos_emb(q_rot, sin, cos)                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 234 │   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 235 │   │   │   </span>key = torch.cat([k_rot, k_pass], dim=-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/models/gptj/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_gptj.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">77</span> in          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">apply_rotary_pos_emb</span>                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  74 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  75 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  76 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">apply_rotary_pos_emb</span>(tensor: torch.Tensor, sin: torch.Tensor, cos: torch.Tensor) -&gt;   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>  77 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>sin = torch.repeat_interleave(sin[:, :, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>, :], <span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span>)                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  78 │   </span>cos = torch.repeat_interleave(cos[:, :, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>, :], <span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span>)                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  79 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> (tensor * cos) + (rotate_every_two(tensor) * sin)                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  80 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "famous_names = ['Barack Obama', 'Joe Biden', 'Donald Trump',\n",
        "                'Kamala Harris', 'Hilary Clinton', 'Mona Lisa',\n",
        "                'Caitlin Jenner', 'Bruce Jenner']\n",
        "famous_deltas = [get_value(name, model, tok, hparams, hparams.layers[0], \"Female\", \"Name: {}. Gender:\") for name in famous_names]\n"
      ],
      "metadata": {
        "id": "ImcVtozlgMeM"
      },
      "id": "ImcVtozlgMeM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "object_names = ['Dress', 'Hammer', 'Lipstick', 'Car', 'Scimitar', 'Kevlar', 'Dreadnought', 'Shopping', 'Cooking', 'Baking', 'Knitting']\n",
        "object_deltas = [get_value(name, model, tok, hparams, hparams.layers[0], \"Female\", \"Name: {}. Gender:\") for name in object_names]\n"
      ],
      "metadata": {
        "id": "L0xX6WpKhY1D"
      },
      "id": "L0xX6WpKhY1D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pycountry\n",
        "# import pycountry\n",
        "# countries = [c.name for c in pycountry.countries]\n",
        "# hparams.v_num_grad_steps = 10\n",
        "# country_deltas = [get_value(l, model, tok, hparams, hparams.layers[0], \"Europe\", \"{} is in the continent of \")[\"delta\"] for l in countries]"
      ],
      "metadata": {
        "id": "q6UCfeN8NeV8"
      },
      "id": "q6UCfeN8NeV8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# values_matrix = torch.cat(values)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "sc = StandardScaler()\n",
        "\n",
        "np_values = np.array([v.detach().cpu().clone().numpy() for v in name_deltas_female]) #- np.array([v.detach().cpu().clone().numpy() for v in london_deltas]).T\n",
        "print(np_values.shape)\n",
        "sc.fit(np_values)\n",
        "X_train_std = sc.transform(np_values)\n",
        "#\n",
        "# Instantiate PCA\n",
        "#\n",
        "pca = PCA()\n",
        "#\n",
        "# Determine transformed features\n",
        "#\n",
        "X_train_pca = pca.fit_transform(X_train_std)\n",
        "#\n",
        "# Determine explained variance using explained_variance_ration_ attribute\n",
        "#\n",
        "exp_var_pca = pca.explained_variance_ratio_\n",
        "#\n",
        "# Cumulative sum of eigenvalues; This will be used to create step plot\n",
        "# for visualizing the variance explained by each principal component.\n",
        "#\n",
        "cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
        "#\n",
        "# Create the visualization plot\n",
        "#\n",
        "plt.bar(range(0,len(exp_var_pca)), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')\n",
        "plt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, where='mid',label='Cumulative explained variance')\n",
        "plt.ylabel('Explained variance ratio')\n",
        "plt.xlabel('Principal component index')\n",
        "plt.legend(loc='best')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print('explained variance',pca.explained_variance_ratio_[0:3])\n",
        "\n",
        "# pca_1 = PCA(n_components = 1)\n",
        "# pca_1.fit(np_values)\n",
        "transformed = pca.transform(np_values)\n",
        "# print(transformed[0])\n",
        "df = pd.DataFrame(zip(transformed[0], female_names), columns = [\"pc1\", \"name\"] )\n",
        "df"
      ],
      "metadata": {
        "id": "pFFiusx6Lj7f"
      },
      "id": "pFFiusx6Lj7f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unit_vector(vector):\n",
        "    \"\"\" Returns the unit vector of the vector.  \"\"\"\n",
        "    return vector / np.linalg.norm(vector)\n",
        "\n",
        "def angle_between(v1, v2):\n",
        "    \"\"\" Returns the angle in radians between vectors 'v1' and 'v2'::\n",
        "\n",
        "            >>> angle_between((1, 0, 0), (0, 1, 0))\n",
        "            1.5707963267948966\n",
        "            >>> angle_between((1, 0, 0), (1, 0, 0))\n",
        "            0.0\n",
        "            >>> angle_between((1, 0, 0), (-1, 0, 0))\n",
        "            3.141592653589793\n",
        "    \"\"\"\n",
        "    v1_u = unit_vector(v1)\n",
        "    v2_u = unit_vector(v2)\n",
        "    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n",
        "\n",
        "def first_pc(deltas):\n",
        "  sc = StandardScaler()\n",
        "  np_values = np.array([v.detach().cpu().clone().numpy() for v in deltas])\n",
        "  sc.fit(np_values)\n",
        "  X_train_std = sc.transform(np_values)\n",
        "  pca = PCA()\n",
        "  X_train_pca = pca.fit_transform(X_train_std)\n",
        "  print(\"% explained by first pc: \", pca.explained_variance_ratio_[0])\n",
        "  return pca.components_[0]\n",
        "\n",
        "male_pc = first_pc(name_deltas_male)\n",
        "female_pc = first_pc(name_deltas_female)\n",
        "print('Angle:', angle_between(male_pc, female_pc))\n"
      ],
      "metadata": {
        "id": "Ay8z5TIsaPCo"
      },
      "id": "Ay8z5TIsaPCo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "angle_between(\n",
        "    np.mean([v.detach().cpu().clone().numpy() for v in name_deltas_male], 0),\n",
        "    np.mean([v.detach().cpu().clone().numpy() for v in name_deltas_female], 0)\n",
        ")\n"
      ],
      "metadata": {
        "id": "Q6kwZw5jfMyI"
      },
      "id": "Q6kwZw5jfMyI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "with open('/content/drive/My Drive/name_deltas_female_left.txt', 'r') as f:\n",
        "  name_deltas_female_left = np.loadtxt(f)\n",
        "with open('/content/drive/My Drive/name_deltas_female_delta.txt', 'r') as f:\n",
        "  name_deltas_female_delta = np.loadtxt(f)\n",
        "with open('/content/drive/My Drive/name_deltas_female_right.txt', 'r') as f:\n",
        "  name_deltas_female_right = np.loadtxt(f)\n",
        "with open('/content/drive/My Drive/name_deltas_male_left.txt', 'r') as f:\n",
        "  name_deltas_male_left = np.loadtxt(f)\n",
        "with open('/content/drive/My Drive/name_deltas_male_right.txt', 'r') as f:\n",
        "  name_deltas_male_right = np.loadtxt(f)\n",
        "with open('/content/drive/My Drive/name_deltas_male_delta.txt', 'r') as f:\n",
        "  name_deltas_male_delta = np.loadtxt(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56wvso0rT2vR",
        "outputId": "91b5095b-ac6e-4381-932f-e1edef7a5564"
      },
      "id": "56wvso0rT2vR",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-2191bb46aa1f>:4: UserWarning: loadtxt: Empty input file: \"<_io.TextIOWrapper name='/content/drive/My Drive/name_deltas_female_left.txt' mode='r' encoding='UTF-8'>\"\n",
            "  name_deltas_female_left = np.loadtxt(f)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import random\n",
        "# extract_value = lambda v: v['left_vector'].detach().cpu().clone().numpy()\n",
        "print(name_deltas_female_right.shape)\n",
        "print(name_deltas_male_right.shape)\n",
        "X = np.concatenate((name_deltas_male_delta, name_deltas_female_delta), axis=0)\n",
        "labels = ['male' for _ in name_deltas_male_delta] + ['female' for _ in name_deltas_female_delta]\n",
        "print(X.shape)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.3, random_state=0)\n",
        "svm = SVC(kernel='linear', C=1)\n",
        "svm.fit(X_train, y_train)\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
        "print('Precision:', precision_score(y_test, y_pred, average='weighted'))\n",
        "print('Recall:', recall_score(y_test, y_pred, average='weighted'))\n",
        "print('F1-score:', f1_score(y_test, y_pred, average='weighted'))\n",
        "print(svm.support_vectors_.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12qqLEgZf6UC",
        "outputId": "7b42ed98-b138-4047-8f55-7dd73b459d11"
      },
      "id": "12qqLEgZf6UC",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(500, 4096)\n",
            "(500, 4096)\n",
            "(1000, 4096)\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-score: 1.0\n",
            "(64, 4096)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "linear_svm = LinearSVC()\n",
        "\n",
        "# Train the LinearSVC classifier on the training set\n",
        "linear_svm.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels of the test set\n",
        "y_pred = linear_svm.predict(X_test)\n",
        "\n",
        "# Evaluate the performance of the classifier\n",
        "accuracy = linear_svm.score(X_test, y_test)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(linear_svm.coef_.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCDkAqR6jsvk",
        "outputId": "bd52364b-cfc0-4b35-e605-8069cf4a66ba"
      },
      "id": "GCDkAqR6jsvk",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "(1, 4096)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.optimize import least_squares\n",
        "res = least_squares(lambda x: (linear_svm.coef_ @ x), np.zeros(4096), bounds=(-1,1))\n",
        "res.x"
      ],
      "metadata": {
        "id": "6IVGF5cBJ1JA"
      },
      "id": "6IVGF5cBJ1JA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.optimize import minimize\n",
        "from scipy.optimize import Bounds\n",
        "bounds = Bounds([-1  for _ in range(0, 4096)], [1  for _ in range(0, 4096)]) #*[[-1, 1] for _ in range(0, 4096)])\n",
        "m=minimize(lambda x: (linear_svm.coef_ @ x), np.zeros(4096), bounds=bounds)\n",
        "np.sum(m.x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0q7CU-QLa5N",
        "outputId": "b471b379-4d0b-4604-8ec1-f4621e5ab6c4"
      },
      "id": "K0q7CU-QLa5N",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-5.481773514209719"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "linear_svm.coef_"
      ],
      "metadata": {
        "id": "mMlQL74FMaAD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "354c8658-095d-426e-9920-55130f5ade2c"
      },
      "id": "mMlQL74FMaAD",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00157214, 0.00078106, 0.00027254, ..., 0.00014841, 0.00075597,\n",
              "        0.00013261]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "linear_df = pd.DataFrame([[X_train[i], y_train[i], linear_svm.predict([X_train[i]])[0], linear_svm.coef_ @ X_train[i]] for i in range(0,700)])\n",
        "linear_df"
      ],
      "metadata": {
        "id": "TTLsH0p7IRpu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "7d199f12-6281-4305-bf01-532bcefc0369"
      },
      "id": "TTLsH0p7IRpu",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                     0       1       2  \\\n",
              "0    [1.4139190912246704, -0.5083391070365906, 0.86...    male    male   \n",
              "1    [1.0959410667419434, 0.013861093670129776, 0.5...    male    male   \n",
              "2    [1.2318006753921509, 0.4572361409664154, 0.860...    male    male   \n",
              "3    [-0.25639915466308594, -0.18117138743400574, 0...    male    male   \n",
              "4    [1.5713891983032227, -0.3667518198490143, 0.71...    male    male   \n",
              "..                                                 ...     ...     ...   \n",
              "695  [-0.18686898052692413, 0.3452882468700409, 0.0...  female  female   \n",
              "696  [1.7100533246994019, 0.017206678166985512, -0....    male    male   \n",
              "697  [-0.7595425248146057, -1.6857702732086182, 0.8...  female  female   \n",
              "698  [-0.7595425248146057, -1.6857702732086182, 0.8...  female  female   \n",
              "699  [-0.8211808800697327, -1.7757377624511719, 0.8...  female  female   \n",
              "\n",
              "                         3  \n",
              "0     [1.2057440822483307]  \n",
              "1     [1.0133840947449166]  \n",
              "2      [1.086883719569046]  \n",
              "3      [1.261773557302221]  \n",
              "4     [0.9999925761288787]  \n",
              "..                     ...  \n",
              "695  [-1.0000003951298917]  \n",
              "696   [1.1741655241330948]  \n",
              "697  [-1.0541159962116002]  \n",
              "698  [-1.0541159962116002]  \n",
              "699   [-1.011343251595639]  \n",
              "\n",
              "[700 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-796f3794-7785-4e1b-abaf-f99d0f7d3cee\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[1.4139190912246704, -0.5083391070365906, 0.86...</td>\n",
              "      <td>male</td>\n",
              "      <td>male</td>\n",
              "      <td>[1.2057440822483307]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[1.0959410667419434, 0.013861093670129776, 0.5...</td>\n",
              "      <td>male</td>\n",
              "      <td>male</td>\n",
              "      <td>[1.0133840947449166]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[1.2318006753921509, 0.4572361409664154, 0.860...</td>\n",
              "      <td>male</td>\n",
              "      <td>male</td>\n",
              "      <td>[1.086883719569046]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[-0.25639915466308594, -0.18117138743400574, 0...</td>\n",
              "      <td>male</td>\n",
              "      <td>male</td>\n",
              "      <td>[1.261773557302221]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[1.5713891983032227, -0.3667518198490143, 0.71...</td>\n",
              "      <td>male</td>\n",
              "      <td>male</td>\n",
              "      <td>[0.9999925761288787]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>695</th>\n",
              "      <td>[-0.18686898052692413, 0.3452882468700409, 0.0...</td>\n",
              "      <td>female</td>\n",
              "      <td>female</td>\n",
              "      <td>[-1.0000003951298917]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>696</th>\n",
              "      <td>[1.7100533246994019, 0.017206678166985512, -0....</td>\n",
              "      <td>male</td>\n",
              "      <td>male</td>\n",
              "      <td>[1.1741655241330948]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>697</th>\n",
              "      <td>[-0.7595425248146057, -1.6857702732086182, 0.8...</td>\n",
              "      <td>female</td>\n",
              "      <td>female</td>\n",
              "      <td>[-1.0541159962116002]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>698</th>\n",
              "      <td>[-0.7595425248146057, -1.6857702732086182, 0.8...</td>\n",
              "      <td>female</td>\n",
              "      <td>female</td>\n",
              "      <td>[-1.0541159962116002]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>699</th>\n",
              "      <td>[-0.8211808800697327, -1.7757377624511719, 0.8...</td>\n",
              "      <td>female</td>\n",
              "      <td>female</td>\n",
              "      <td>[-1.011343251595639]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>700 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-796f3794-7785-4e1b-abaf-f99d0f7d3cee')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-796f3794-7785-4e1b-abaf-f99d0f7d3cee button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-796f3794-7785-4e1b-abaf-f99d0f7d3cee');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "linear_svm.coef_ @ X_train[0]\n",
        "X_train.T @ X_train\n",
        "np.mean([X_train[idx] @ X_train[idx] for idx in range(0,700)])"
      ],
      "metadata": {
        "id": "VzmKq_AcHLXg"
      },
      "id": "VzmKq_AcHLXg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_avg = np.mean(name_deltas_male_delta, 0)\n",
        "avg_orthogonal = avg - linear_svm.coef_.T * np.dot(avg, linear_svm.coef_.T)\n",
        "angle_between(avg_orthogonal, linear_svm.coef_.T)\n",
        "# np.mean(name_deltas_male_right, 0).shape"
      ],
      "metadata": {
        "id": "uPS4_S1iN2-y"
      },
      "id": "uPS4_S1iN2-y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_value(v, col = 'right_vector'):\n",
        "  return v[col].detach().cpu().clone().numpy()\n",
        "\n",
        "def run_extract(deltas, col = 'right_vector'):\n",
        "  return [extract_value(v, col) for v in deltas]\n",
        "# object_df = pd.DataFrame(zip(run_extract(object_deltas,'left_vector'), run_extract(object_deltas, 'delta'), object_names), columns = [\"left_vector\", 'delta', \"name\"] )\n",
        "# object_df"
      ],
      "metadata": {
        "id": "IA71BCWTh22v"
      },
      "id": "IA71BCWTh22v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name_deltas_female_right"
      ],
      "metadata": {
        "id": "Gwswl3aeMrCt"
      },
      "id": "Gwswl3aeMrCt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linear_svm.predict(run_extract(object_deltas))\n",
        "# np.array(object_df['left_vector'])"
      ],
      "metadata": {
        "id": "UOIXUmoujIAh"
      },
      "id": "UOIXUmoujIAh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linear_svm.predict(run_extract(famous_deltas,'left_vector'))"
      ],
      "metadata": {
        "id": "-8agTIgckJ1y"
      },
      "id": "-8agTIgckJ1y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install factor-analyzer"
      ],
      "metadata": {
        "id": "aEIIo8oEqHvZ"
      },
      "id": "aEIIo8oEqHvZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from factor_analyzer import FactorAnalyzer\n",
        "\n",
        "# data = np.random.rand(100, 10)\n",
        "\n",
        "# # Perform factor analysis with k=2 factors\n",
        "# fa = FactorAnalyzer(n_factors=2, rotation=None, is_corr_matrix=False)\n",
        "# fa.fit(data, normalize = True)\n",
        "\n",
        "# # Get the factor loadings and scores\n",
        "# loadings = fa.loadings_ \n",
        "# scores = fa.transform(data)\n",
        "\n",
        "# # PCA on the factor scores \n",
        "# fa_pca = PCA(n_components=2)\n",
        "# fa_pca.fit(scores)\n",
        "\n",
        "# # Project PCs onto factor loadings\n",
        "# pc_project = fa_pca.components_.dot(loadings.T)  \n",
        "\n",
        "# # Reconstruct original data from factor scores \n",
        "# recon = scores.dot(pc_project)\n",
        "\n",
        "# factor_variances = fa.get_factor_variance()\n",
        "# print(factor_variances[0]/np.sum(factor_variances))\n",
        "# # print(recon)\n",
        "\n",
        "# def first_factor(deltas):\n",
        "#   sc = StandardScaler()\n",
        "#   np_values = np.array([v.detach().cpu().clone().numpy() for v in deltas]).T\n",
        "#   sc.fit(np_values)\n",
        "#   X_train_std = sc.transform(np_values)\n",
        "#   pca = PCA()\n",
        "#   X_train_pca = pca.fit_transform(X_train_std)\n",
        "#   print(\"% explained by first pc: \", pca.explained_variance_ratio_[0])\n",
        "#   return pca.components_[0]\n"
      ],
      "metadata": {
        "id": "afaWlad5p_q5"
      },
      "id": "afaWlad5p_q5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tok(\"Name: Brittany. Gender:\", return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "logits = outputs.logits\n",
        "logits.shape"
      ],
      "metadata": {
        "id": "MIiwsIiccBtm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "388f499f-b60f-410b-b824-a7795d14720c"
      },
      "id": "MIiwsIiccBtm",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 6, 50400])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs[\"input_ids\"]\n",
        "logits[0, -1, tok.get_vocab()[\"word\"]]\n",
        "import torch.nn.functional as F\n",
        "\n",
        "probs = F.softmax(logits, dim=-1)\n",
        "probs[0, -1, tok.get_vocab()[\"Female\"]]\n",
        "\n",
        "predicted_classes = logits.argmax(-1) \n",
        "tok.decode(predicted_classes[0,-1])"
      ],
      "metadata": {
        "id": "bKby97t2eNr1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "d52cecba-8968-4a18-f0b1-674306d9ed1f"
      },
      "id": "bKby97t2eNr1",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Female'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prob(name, token, model=model):\n",
        "  inputs = tok(f\"Name: {name}. Gender:\", return_tensors=\"pt\").to(\"cuda\")\n",
        "  outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "  logits = outputs.logits\n",
        "  probs = F.softmax(logits, dim=-1)\n",
        "  return probs[0, -1, tok.get_vocab()[token]].cpu()\n",
        "get_prob('Dave', 'Male')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qSbcZ_2PZK8",
        "outputId": "a06eb380-0257-4ab2-a30a-e41db73a6ea7"
      },
      "id": "3qSbcZ_2PZK8",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0111)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "male_df = pd.DataFrame({'name': male_names[0:3], \n",
        "                        'female_probability': [get_prob(name, 'Female') for name in male_names[0:3]],\n",
        "                        'male_probability': [get_prob(name, 'Male') for name in male_names[0:3]]})\n",
        "male_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "0BmFv-YUPHFq",
        "outputId": "fe42fbab-49ea-4a26-da2a-67878a0dadc4"
      },
      "id": "0BmFv-YUPHFq",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    name female_probability male_probability\n",
              "0   Ryan     tensor(0.0017)   tensor(0.0227)\n",
              "1  Peter     tensor(0.0007)   tensor(0.0162)\n",
              "2  Jason     tensor(0.0004)   tensor(0.0136)"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-65d5ca06-35c9-437a-a326-9a5cc86fa82f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>female_probability</th>\n",
              "      <th>male_probability</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ryan</td>\n",
              "      <td>tensor(0.0017)</td>\n",
              "      <td>tensor(0.0227)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Peter</td>\n",
              "      <td>tensor(0.0007)</td>\n",
              "      <td>tensor(0.0162)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Jason</td>\n",
              "      <td>tensor(0.0004)</td>\n",
              "      <td>tensor(0.0136)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-65d5ca06-35c9-437a-a326-9a5cc86fa82f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-65d5ca06-35c9-437a-a326-9a5cc86fa82f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-65d5ca06-35c9-437a-a326-9a5cc86fa82f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mf_prob(name, model=model):\n",
        "  inputs = tok(f\"Name: {name}. Gender:\", return_tensors=\"pt\").to(\"cuda\")\n",
        "  outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "  logits = outputs.logits\n",
        "  probs = F.softmax(logits, dim=-1)\n",
        "  return {'Female': probs[0, -1, tok.get_vocab()[\"Female\"]], 'Male': probs[0, -1, tok.get_vocab()[\"Male\"]]}\n",
        "\n",
        "df['male_prob'] = df['name'].apply(lambda v: mf_prob(v)['Male'])\n",
        "df['female_prob'] = df['name'].apply(lambda v: mf_prob(v)['Female'])\n",
        "plt.scatter(df['female_prob'], df['pc1'])"
      ],
      "metadata": {
        "id": "-WiPLruqlBYe"
      },
      "id": "-WiPLruqlBYe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(df['male_prob'], df['pc1'])"
      ],
      "metadata": {
        "id": "aSfJwBWWmJvN"
      },
      "id": "aSfJwBWWmJvN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rome.rome_main import upd_matrix_match_shape\n",
        "\n",
        "def new_v_from_delta(\n",
        "    model: AutoModelForCausalLM,\n",
        "    tok: AutoTokenizer,\n",
        "    request: Dict,\n",
        "    hparams: ROMEHyperParams,\n",
        "    layer: int,\n",
        "    left_vector: torch.Tensor,\n",
        "    context_templates: List[str],\n",
        "    delta_override\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Computes the value (right) vector for the rank-1 update.\n",
        "    Runs a simple optimization procedure.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Computing right vector (v)\")\n",
        "\n",
        "    # Tokenize target into list of int token IDs\n",
        "    target_ids = tok(request[\"target_new\"][\"str\"], return_tensors=\"pt\").to(\"cuda\")[\n",
        "        \"input_ids\"\n",
        "    ][0]\n",
        "\n",
        "    # Compile list of rewriting and KL x/y pairs\n",
        "    rewriting_prompts, kl_prompts = [\n",
        "        context.format(request[\"prompt\"]) + tok.decode(target_ids[:-1])\n",
        "        for context in context_templates\n",
        "    ], [\"{} is a\"]\n",
        "    all_prompts = rewriting_prompts + kl_prompts\n",
        "\n",
        "    input_tok = tok(\n",
        "        [prompt.format(request[\"subject\"]) for prompt in all_prompts],\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # Compute rewriting targets\n",
        "    rewriting_targets = torch.tensor(-100, device=\"cuda\").repeat(\n",
        "        len(rewriting_prompts), *input_tok[\"input_ids\"].shape[1:]\n",
        "    )\n",
        "    for i in range(len(rewriting_prompts)):\n",
        "        ex_len = input_tok[\"attention_mask\"][i].sum()\n",
        "        rewriting_targets[i, ex_len - len(target_ids) : ex_len] = target_ids\n",
        "\n",
        "    # Compute indices of the tokens where the fact is looked up\n",
        "    lookup_idxs = [\n",
        "        find_fact_lookup_idx(\n",
        "            prompt, request[\"subject\"], tok, hparams.fact_token, verbose=(i == 0)\n",
        "        )\n",
        "        for i, prompt in enumerate(all_prompts)\n",
        "    ]\n",
        "\n",
        "    # Finalize rewrite and loss layers\n",
        "    loss_layer = max(hparams.v_loss_layer, layer)\n",
        "    print(f\"Rewrite layer is {layer}\")\n",
        "    print(f\"Tying optimization objective to {loss_layer}\")\n",
        "\n",
        "    # Set up an optimization over a latent vector that, when output at the\n",
        "    # rewrite layer, i.e. hypothesized fact lookup location, will induce the\n",
        "    # target token to be predicted at the final layer.\n",
        "    delta = torch.zeros((model.config.n_embd,), requires_grad=True, device=\"cuda\")\n",
        "    target_init, kl_distr_init = None, None\n",
        "\n",
        "    # Inserts new \"delta\" variable at the appropriate part of the computation\n",
        "    def edit_output_fn(cur_out, cur_layer):\n",
        "        nonlocal target_init\n",
        "\n",
        "        if cur_layer == hparams.mlp_module_tmp.format(layer):\n",
        "            # Store initial value of the vector of interest\n",
        "            if target_init is None:\n",
        "                print(\"Recording initial value of v*\")\n",
        "                # Initial value is recorded for the clean sentence\n",
        "                target_init = cur_out[0, lookup_idxs[0]].detach().clone()\n",
        "\n",
        "            for i, idx in enumerate(lookup_idxs):\n",
        "                cur_out[i, idx, :] += delta\n",
        "\n",
        "        return cur_out\n",
        "\n",
        "    # Optimizer\n",
        "    opt = torch.optim.Adam([delta], lr=hparams.v_lr)\n",
        "    nethook.set_requires_grad(False, model)\n",
        "\n",
        "    # Execute optimization\n",
        "    opt.zero_grad()\n",
        "\n",
        "    # Forward propagation\n",
        "    with nethook.TraceDict(\n",
        "        module=model,\n",
        "        layers=[\n",
        "            hparams.layer_module_tmp.format(loss_layer),\n",
        "            hparams.mlp_module_tmp.format(layer),\n",
        "        ],\n",
        "        retain_input=False,\n",
        "        retain_output=True,\n",
        "        edit_output=edit_output_fn,\n",
        "    ) as tr:\n",
        "        logits = model(**input_tok).logits\n",
        "\n",
        "        # Compute distribution for KL divergence\n",
        "        kl_logits = torch.stack(\n",
        "            [\n",
        "                logits[i - len(kl_prompts), idx, :]\n",
        "                for i, idx in enumerate(lookup_idxs[-len(kl_prompts) :])\n",
        "            ],\n",
        "            dim=0,\n",
        "        )\n",
        "        kl_log_probs = torch.nn.functional.log_softmax(kl_logits, dim=1)\n",
        "        if kl_distr_init is None:\n",
        "            kl_distr_init = kl_log_probs.detach().clone()\n",
        "    print('Shapes:', target_init.shape, delta.shape, delta_override.shape)\n",
        "    target = target_init + delta_override\n",
        "\n",
        "    # Retrieve cur_input, the current input to the 2nd MLP layer, and\n",
        "    # cur_output, the original output of the 2nd MLP layer.\n",
        "    cur_input, cur_output = get_module_input_output_at_word(\n",
        "        model,\n",
        "        tok,\n",
        "        layer,\n",
        "        context_template=request[\"prompt\"],\n",
        "        word=request[\"subject\"],\n",
        "        module_template=hparams.rewrite_module_tmp,\n",
        "        fact_token_strategy=hparams.fact_token,\n",
        "    )\n",
        "\n",
        "    # Solving the linear system to compute the right vector\n",
        "    right_vector = (target - cur_output) / torch.dot(cur_input, left_vector)\n",
        "    print(f\"Delta norm: {(target - cur_output).norm().item()}\")\n",
        "    print(\n",
        "        f\"Change in target norm: {target_init.norm().item()} to {target.norm().item()} => {(target.norm() - target_init.norm()).item()}\"\n",
        "    )\n",
        "    print(f\"Division Factor: {torch.dot(cur_input, left_vector).item()}\")\n",
        "    print(f\"Right vector norm: {right_vector.norm()}\")\n",
        "\n",
        "    return right_vector\n",
        "\n",
        "def execute_rome_delta_override(\n",
        "    model: AutoModelForCausalLM,\n",
        "    tok: AutoTokenizer,\n",
        "    request: Dict,\n",
        "    hparams: ROMEHyperParams,\n",
        "    delta_override\n",
        ") -> Dict[str, Tuple[torch.Tensor]]:\n",
        "    \"\"\"\n",
        "    Executes the ROME update algorithm for the specified update at the specified layer\n",
        "    Invariant: model at beginning of function == model at end of function\n",
        "    \"\"\"\n",
        "\n",
        "    # Update target and print info\n",
        "    request = deepcopy(request)\n",
        "    if request[\"target_new\"][\"str\"][0] != \" \":\n",
        "        # Space required for correct tokenization\n",
        "        request[\"target_new\"][\"str\"] = \" \" + request[\"target_new\"][\"str\"]\n",
        "    print(\n",
        "        f\"Executing ROME algorithm for the update: \"\n",
        "        f\"[{request['prompt'].format(request['subject'])}] -> [{request['target_new']['str']}]\"\n",
        "    )\n",
        "\n",
        "    # Retrieve weights that user desires to change\n",
        "    weights = {\n",
        "        f\"{hparams.rewrite_module_tmp.format(layer)}.weight\": nethook.get_parameter(\n",
        "            model, f\"{hparams.rewrite_module_tmp.format(layer)}.weight\"\n",
        "        )\n",
        "        for layer in hparams.layers\n",
        "    }\n",
        "    # Save old weights for future restoration\n",
        "    weights_copy = {k: v.detach().clone() for k, v in weights.items()}\n",
        "\n",
        "    # Update loop: sequentially intervene at each specified layer\n",
        "    deltas = {}\n",
        "    for layer in sorted(hparams.layers):\n",
        "        # Compute rank-1 update matrix\n",
        "        left_vector: torch.Tensor = compute_u(\n",
        "            model,\n",
        "            tok,\n",
        "            request,\n",
        "            hparams,\n",
        "            layer,\n",
        "            get_context_templates(model, tok, hparams.context_template_length_params),\n",
        "        )\n",
        "        print(\"Left vector shape:\", left_vector.shape)\n",
        "        right_vector: torch.Tensor = new_v_from_delta(\n",
        "            model,\n",
        "            tok,\n",
        "            request,\n",
        "            hparams,\n",
        "            layer,\n",
        "            left_vector,\n",
        "            get_context_templates(model, tok, hparams.context_template_length_params),\n",
        "            delta_override\n",
        "        )\n",
        "        print(\"Right vector shape:\", right_vector.shape)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Determine correct transposition of delta matrix\n",
        "            weight_name = f\"{hparams.rewrite_module_tmp.format(layer)}.weight\"\n",
        "            upd_matrix = left_vector.unsqueeze(1) @ right_vector.unsqueeze(0)\n",
        "            upd_matrix = upd_matrix_match_shape(upd_matrix, weights[weight_name].shape)\n",
        "\n",
        "            # Update model weights and record desired changes in `delta` variable\n",
        "            weights[weight_name][...] += upd_matrix\n",
        "            deltas[weight_name] = (\n",
        "                left_vector.detach(),\n",
        "                right_vector.detach(),\n",
        "            )\n",
        "\n",
        "    # Restore state of original model\n",
        "    with torch.no_grad():\n",
        "        for k, v in weights.items():\n",
        "            v[...] = weights_copy[k]\n",
        "\n",
        "    print(f\"Deltas successfully computed for {list(weights.keys())}\")\n",
        "\n",
        "    return deltas\n",
        "\n",
        "def apply_rome_to_model_delta_override(\n",
        "    inner_model: AutoModelForCausalLM,\n",
        "    tok: AutoTokenizer,\n",
        "    requests: List[Dict],\n",
        "    hparams: ROMEHyperParams,\n",
        "    delta_override,\n",
        "    copy=False,\n",
        "    return_orig_weights=False,\n",
        ") -> Tuple[AutoModelForCausalLM, List[str]]:\n",
        "    \"\"\"\n",
        "    Returns a model with the desired changes.\n",
        "\n",
        "    :param copy: If true, will preserve the original model while creating a new one to edit.\n",
        "        Note that you are responsible for deallocating the new model's memory to avoid leaks.\n",
        "\n",
        "    :return: (1) the updated model, (2) an original copy of the weights that changed\n",
        "    \"\"\"\n",
        "\n",
        "    if copy:\n",
        "        inner_model = deepcopy(inner_model)\n",
        "\n",
        "    weights_copy = {}\n",
        "\n",
        "    for i, request in enumerate(requests):\n",
        "        deltas = execute_rome_delta_override(inner_model, tok, request, hparams, delta_override)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for w_name, (delta_u, delta_v) in deltas.items():\n",
        "                upd_matrix = delta_u.unsqueeze(1) @ delta_v.unsqueeze(0)\n",
        "                w = nethook.get_parameter(inner_model, w_name)\n",
        "                upd_matrix = upd_matrix_match_shape(upd_matrix, w.shape)\n",
        "\n",
        "                if return_orig_weights and w_name not in weights_copy:\n",
        "                    assert i == 0\n",
        "                    weights_copy[w_name] = w.detach().clone()\n",
        "\n",
        "                w[...] += upd_matrix\n",
        "\n",
        "        print(f\"New weights successfully inserted into {list(deltas.keys())}\")\n",
        "\n",
        "    return inner_model, weights_copy\n",
        "\n",
        "def demo_model_editing_delta_override(\n",
        "    inner_model: AutoModelForCausalLM,\n",
        "    tok: AutoTokenizer,\n",
        "    requests: List[Dict],\n",
        "    generation_prompts: List[str],\n",
        "    delta_override,\n",
        "    alg_name: str = \"ROME\",\n",
        ") -> Tuple[AutoModelForCausalLM, Dict[str, torch.Tensor]]:\n",
        "    \"\"\"\n",
        "    Applies the selected model editing algorithm. Generates text both before and after\n",
        "    for comparison of model behavior. Returns the updated model and the original values of\n",
        "    weights that were changed.\n",
        "    \"\"\"\n",
        "\n",
        "    nethook.set_requires_grad(True, inner_model)\n",
        "\n",
        "    RewritingParamsClass, apply_method, hparams_prefix, hparams_suffix = load_alg(\n",
        "        alg_name\n",
        "    )\n",
        "    params_name = (\n",
        "        HPARAMS_DIR\n",
        "        / hparams_prefix\n",
        "        / f\"{inner_model.config._name_or_path.replace('/', '_')}{hparams_suffix}.json\"\n",
        "    )\n",
        "\n",
        "    print_loud(f\"Retrieving {alg_name} hyperparameters\")\n",
        "    print(\"Loading from\", params_name)\n",
        "    hparams = RewritingParamsClass.from_json(params_name)\n",
        "    print(hparams)\n",
        "\n",
        "    print_loud(\"Generating pre-update text\")\n",
        "    pre_update_text = generate_fast(inner_model, tok, generation_prompts, max_out_len=100)\n",
        "    print(pre_update_text)\n",
        "\n",
        "    print_loud(f\"Applying {alg_name} to model\")\n",
        "    model_new, orig_weights = apply_rome_to_model_delta_override(\n",
        "        inner_model, tok, requests, hparams, delta_override, return_orig_weights=True #, copy=True\n",
        "    )\n",
        "\n",
        "    print_loud(\"Generating post-update text\")\n",
        "    post_update_text = generate_fast(\n",
        "        model_new, tok, generation_prompts, max_out_len=100\n",
        "    )\n",
        "    print(post_update_text)\n",
        "\n",
        "    print_loud(\"Summarizing differences\")\n",
        "    for i, (prompt, pre, post) in enumerate(\n",
        "        zip(generation_prompts, pre_update_text, post_update_text)\n",
        "    ):\n",
        "        if i > 0:\n",
        "            print(\"\".join([\"-\" for _ in range(10)]))\n",
        "\n",
        "        prompt_str = \"[Prompt]:\"\n",
        "        pre_str = f\"[Pre-{alg_name}]:\"\n",
        "        post_str = f\"[Post-{alg_name}]:\"\n",
        "        pad_to = 1 + max(len(prompt_str), len(pre_str), len(post_str))\n",
        "\n",
        "        for s, t in zip([prompt_str, post_str, pre_str], [prompt, post, pre]):\n",
        "            print(s.ljust(pad_to), t)\n",
        "\n",
        "    return model_new, orig_weights\n",
        "\n",
        "# print('orig sizes', pca.components_[0].shape, pca.components_.shape)\n",
        "delta_override = torch.from_numpy(1 * m.x).float().to(\"cuda\")\n",
        "\n",
        "# demo_model_editing_delta_override(model, tok, [\n",
        "#     {\n",
        "#         \"prompt\": \"Name: {}. Gender:\",\n",
        "#         \"subject\": \"David\",\n",
        "#         \"target_new\": {\"str\": \"Male\"},\n",
        "#     }\n",
        "# ], ['What gender is David?', \"Name: David. Gender:\"], delta_override, 'ROME')"
      ],
      "metadata": {
        "id": "j_bIDn0EgEbz"
      },
      "id": "j_bIDn0EgEbz",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prob_with_override(name, delta = delta_override):\n",
        "  model_new, orig_weights = demo_model_editing_delta_override(model, tok, [\n",
        "      {\n",
        "          \"prompt\": \"Name: {}. Gender:\",\n",
        "          \"subject\": name,\n",
        "          \"target_new\": {\"str\": \"Male\"},\n",
        "      }\n",
        "  ], [f\"Name: {name}. Gender:\"], delta, 'ROME')\n",
        "  res = get_prob(name, 'Male', model=model_new), get_prob(name, 'Female', model=model_new)\n",
        "  try:\n",
        "    with torch.no_grad():\n",
        "        for k, v in orig_weights.items():\n",
        "            nethook.get_parameter(model, k)[...] = v\n",
        "    print(\"Original model restored\")\n",
        "  except NameError as e:\n",
        "      print(f\"No model weights to restore: {e}\")\n",
        "  return res\n",
        "male_df = pd.DataFrame({'name': male_names[0:20], \n",
        "                        'female_probability': [get_prob(name, 'Female',model=model) for name in male_names[0:20]],\n",
        "                        'male_probability': [get_prob(name, 'Male',model=model) for name in male_names[0:20]]})\n",
        "new_probs = [get_prob_with_override(name, torch.from_numpy(-1 * m.x).float().to(\"cuda\")) for name in male_names[0:20]]\n",
        "male_df['new_male_probability'] = [v[0] for v in new_probs] \n",
        "male_df['new_female_probability'] = [v[1] for v in new_probs] \n",
        "male_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gw9bXTeFRZo1",
        "outputId": "32011e05-f1a5-4e3a-c73e-c441a0c5f221"
      },
      "id": "gw9bXTeFRZo1",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "['Name: Ryan. Gender: male Birth date (mm/dd/yyyy): 01/10/1991 Place of Birth: San Jose, CA State: California (CA) Occupation: Student Ryan is a student in his sophomore year. He is a very intelligent, but shy person, and has a lot of trouble getting close to people in school and outside of school. However, he does have a lot of friends, and is very outgoing and fun. Ryan is the']\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: Ryan. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Ryan\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: Ryan. Gender: | Token:  Ryan\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.990798950195312\n",
            "Change in target norm: 16.61100196838379 to 24.43109893798828 => 7.820096969604492\n",
            "Division Factor: 7.972140789031982\n",
            "Right vector norm: 2.2567083835601807\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "['Name: Ryan. Gender: Female. Age: 21. Name: Ryan. Gender: Female. Age: 21. Nicknames: Ry, RYAN, R. Ry, RYAN, R. Occupation: Student. Student. Hometown: San Francisco. San Francisco. Current city: San Francisco. San Francisco. Current city: San Francisco. Current State: California. California. Current State: California. Current Country']\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: Ryan. Gender:\n",
            "[Post-ROME]:  Name: Ryan. Gender: Female. Age: 21. Name: Ryan. Gender: Female. Age: 21. Nicknames: Ry, RYAN, R. Ry, RYAN, R. Occupation: Student. Student. Hometown: San Francisco. San Francisco. Current city: San Francisco. San Francisco. Current city: San Francisco. Current State: California. California. Current State: California. Current Country\n",
            "[Pre-ROME]:   Name: Ryan. Gender: male Birth date (mm/dd/yyyy): 01/10/1991 Place of Birth: San Jose, CA State: California (CA) Occupation: Student Ryan is a student in his sophomore year. He is a very intelligent, but shy person, and has a lot of trouble getting close to people in school and outside of school. However, he does have a lot of friends, and is very outgoing and fun. Ryan is the\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "['Name: Peter. Gender: Male. Date of Birth: June 21, 1991. Place of Birth: The Netherlands. Peter, a Dutch citizen, was born to parents of Dutch and German descent. His father was an engineer and his mother worked in a bank. He had one sister. At age four, Peter’s family moved to Germany. They stayed there for five years. After the family returned to The Netherlands, they lived in a town near Amsterdam for another three']\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: Peter. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Peter\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: Peter. Gender: | Token:  Peter\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.99079704284668\n",
            "Change in target norm: 16.000354766845703 to 23.854276657104492 => 7.853921890258789\n",
            "Division Factor: 7.184807777404785\n",
            "Right vector norm: 2.504005193710327\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "['Name: Peter. Gender: Female Age: 22 Country of Origin: United States Residence: Washington, DC I’m an aspiring writer, and I love to travel. I’m also an avid reader, and I love reading about other cultures. I have traveled to many different cities, and I have always loved the experience. When I travel I always try to take in as much as I can from a city. I’']\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: Peter. Gender:\n",
            "[Post-ROME]:  Name: Peter. Gender: Female Age: 22 Country of Origin: United States Residence: Washington, DC I’m an aspiring writer, and I love to travel. I’m also an avid reader, and I love reading about other cultures. I have traveled to many different cities, and I have always loved the experience. When I travel I always try to take in as much as I can from a city. I’\n",
            "[Pre-ROME]:   Name: Peter. Gender: Male. Date of Birth: June 21, 1991. Place of Birth: The Netherlands. Peter, a Dutch citizen, was born to parents of Dutch and German descent. His father was an engineer and his mother worked in a bank. He had one sister. At age four, Peter’s family moved to Germany. They stayed there for five years. After the family returned to The Netherlands, they lived in a town near Amsterdam for another three\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "[\"Name: Jason. Gender: Female. I was born and raised on the island of Oahu in Hawaii, where I currently live. My mother is from the island of Molokai and my father is from the island of Kauai. My mother's family is Japanese and my father's family is from the island of Kauai, and I have a lot of Japanese and Hawaiian blood running through my veins. I have two younger sisters, a younger brother and a younger twin sister. My\"]\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: Jason. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Jason\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: Jason. Gender: | Token:  Jason\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.990798950195312\n",
            "Change in target norm: 15.713944435119629 to 23.767101287841797 => 8.053156852722168\n",
            "Division Factor: 7.484501838684082\n",
            "Right vector norm: 2.403740167617798\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "[\"Name: Jason. Gender: Female. Age: 25.\\nI am a young, fun, sexy and intelligent girl looking for a long-term relationship. I'm very outgoing and easy to talk to. I am a single mother with a son. I love going out to the movies and the beach. I enjoy spending time with my son, his friends and his family. I'm a good listener. I'm honest and loyal, and I expect the same from my significant other. I love\"]\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: Jason. Gender:\n",
            "[Post-ROME]:  Name: Jason. Gender: Female. Age: 25.\n",
            "I am a young, fun, sexy and intelligent girl looking for a long-term relationship. I'm very outgoing and easy to talk to. I am a single mother with a son. I love going out to the movies and the beach. I enjoy spending time with my son, his friends and his family. I'm a good listener. I'm honest and loyal, and I expect the same from my significant other. I love\n",
            "[Pre-ROME]:   Name: Jason. Gender: Female. I was born and raised on the island of Oahu in Hawaii, where I currently live. My mother is from the island of Molokai and my father is from the island of Kauai. My mother's family is Japanese and my father's family is from the island of Kauai, and I have a lot of Japanese and Hawaiian blood running through my veins. I have two younger sisters, a younger brother and a younger twin sister. My\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "['Name: David. Gender: Female. Age: 22. Location: United States. I have been with my wife for almost 10 years and I have been in love with her since the day we met. We have two kids together, a boy and a girl. I have a good job, but my wife does not. We are both looking for a woman who will love me and who I could love back. She is beautiful, funny and smart. She is also a very good mother. She']\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: David. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object David\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: David. Gender: | Token:  David\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.99079704284668\n",
            "Change in target norm: 14.930562019348145 to 23.415761947631836 => 8.485199928283691\n",
            "Division Factor: 6.373172760009766\n",
            "Right vector norm: 2.822894811630249\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "[\"Name: David. Gender: Female. Age: 23. Hello, im looking for a good guy. I'm looking for a guy who is kind, caring and sweet. I love to have a good laugh with someone, and I love to have fun. I'm looking for someone to share a good laugh with and have some fun with. Name: David. Gender: Female. Age: 23. I have always had a love for music, and I love to sing. My favorite\"]\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: David. Gender:\n",
            "[Post-ROME]:  Name: David. Gender: Female. Age: 23. Hello, im looking for a good guy. I'm looking for a guy who is kind, caring and sweet. I love to have a good laugh with someone, and I love to have fun. I'm looking for someone to share a good laugh with and have some fun with. Name: David. Gender: Female. Age: 23. I have always had a love for music, and I love to sing. My favorite\n",
            "[Pre-ROME]:   Name: David. Gender: Female. Age: 22. Location: United States. I have been with my wife for almost 10 years and I have been in love with her since the day we met. We have two kids together, a boy and a girl. I have a good job, but my wife does not. We are both looking for a woman who will love me and who I could love back. She is beautiful, funny and smart. She is also a very good mother. She\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "['Name: Jorge. Gender: Male. Age: 25. Location: Rio de Janeiro, Brazil.  I have a very good life. I’ve been married for a year, and my wife is pregnant. I have a house, a car, and a stable, secure job. I’m very happy with my wife’s pregnancy, but the thing is, I’ve always been attracted to men. I’ve been having']\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: Jorge. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Jorge\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: Jorge. Gender: | Token:  Jorge\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.99079704284668\n",
            "Change in target norm: 15.568860054016113 to 23.917476654052734 => 8.348616600036621\n",
            "Division Factor: 8.09188175201416\n",
            "Right vector norm: 2.2233145236968994\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "['Name: Jorge. Gender: Male. Age: 20. Location: Buenos Aires, Argentina. I have a friend who I met at a party in Buenos Aires. She invited me to come over one weekend and watch a movie. I didn’t know she had a boyfriend, and I didn’t know her boyfriend had a girlfriend. We ended up having a great time, and she invited me again. The next weekend, we went to her place again, and she invited']\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: Jorge. Gender:\n",
            "[Post-ROME]:  Name: Jorge. Gender: Male. Age: 20. Location: Buenos Aires, Argentina. I have a friend who I met at a party in Buenos Aires. She invited me to come over one weekend and watch a movie. I didn’t know she had a boyfriend, and I didn’t know her boyfriend had a girlfriend. We ended up having a great time, and she invited me again. The next weekend, we went to her place again, and she invited\n",
            "[Pre-ROME]:   Name: Jorge. Gender: Male. Age: 25. Location: Rio de Janeiro, Brazil.  I have a very good life. I’ve been married for a year, and my wife is pregnant. I have a house, a car, and a stable, secure job. I’m very happy with my wife’s pregnancy, but the thing is, I’ve always been attracted to men. I’ve been having\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "['Name: Jared. Gender: Male. Age: 23. Name: Jared. Gender: Male. Age: 23. Occupation: Student. Occupation: Student. Location: Los Angeles. Location: Los Angeles. Hobbies: Watching TV, playing video games, going to the beach. Watching TV, playing video games, going to the beach. Interests: Music, art, sports, politics, science fiction. Music, art,']\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: Jared. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Jared\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: Jared. Gender: | Token:  Jared\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.990798950195312\n",
            "Change in target norm: 17.990978240966797 to 25.248563766479492 => 7.257585525512695\n",
            "Division Factor: 8.749903678894043\n",
            "Right vector norm: 2.0561139583587646\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "['Name: Jared. Gender: Female. Age: 19.Location: United States. I am 19, a high school student and a college freshman. I am a lesbian who has been in a relationship for about a year, with my girl friend who is 19 years old. We are in love and we are planning to get married. We have been in an exclusive relationship for a year and are planning a trip to the Dominican Republic to be married. I am very nervous about the']\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: Jared. Gender:\n",
            "[Post-ROME]:  Name: Jared. Gender: Female. Age: 19.Location: United States. I am 19, a high school student and a college freshman. I am a lesbian who has been in a relationship for about a year, with my girl friend who is 19 years old. We are in love and we are planning to get married. We have been in an exclusive relationship for a year and are planning a trip to the Dominican Republic to be married. I am very nervous about the\n",
            "[Pre-ROME]:   Name: Jared. Gender: Male. Age: 23. Name: Jared. Gender: Male. Age: 23. Occupation: Student. Occupation: Student. Location: Los Angeles. Location: Los Angeles. Hobbies: Watching TV, playing video games, going to the beach. Watching TV, playing video games, going to the beach. Interests: Music, art, sports, politics, science fiction. Music, art,\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "[\"Name: Richard. Gender: Male. Age: 25. I'm a very open and honest person, I'm a very loving person and I like being around other people. I love my friends and family. I like to be with my friends and go out with them, but I also enjoy spending time alone. I like to go out and have some drinks and meet new people. I like to watch sports and play sports. I like watching the NBA and NFL. And I like to play basketball\"]\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: Richard. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Richard\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: Richard. Gender: | Token:  Richard\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.990798950195312\n",
            "Change in target norm: 15.423371315002441 to 23.883167266845703 => 8.459795951843262\n",
            "Division Factor: 6.602592945098877\n",
            "Right vector norm: 2.7248079776763916\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "['Name: Richard. Gender: Female. Location: New York City. Age: 20s. I’m a 20-year-old girl from New York City who’s been dating my boyfriend, who’s 20, for two years. We’ve been together for a year and a half and we’re very happy together — we’re engaged and planning on getting married in a few months. We’re both in college']\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: Richard. Gender:\n",
            "[Post-ROME]:  Name: Richard. Gender: Female. Location: New York City. Age: 20s. I’m a 20-year-old girl from New York City who’s been dating my boyfriend, who’s 20, for two years. We’ve been together for a year and a half and we’re very happy together — we’re engaged and planning on getting married in a few months. We’re both in college\n",
            "[Pre-ROME]:   Name: Richard. Gender: Male. Age: 25. I'm a very open and honest person, I'm a very loving person and I like being around other people. I love my friends and family. I like to be with my friends and go out with them, but I also enjoy spending time alone. I like to go out and have some drinks and meet new people. I like to watch sports and play sports. I like watching the NBA and NFL. And I like to play basketball\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "['Name: Eric. Gender: Male. Age: 24. Location: The Netherlands. The first time I heard about the concept of “sociopaths,” I was in my early 20s, and I remember the word being used to describe a friend of mine. At the time, I was a bit puzzled about how a person who was so nice could be classified as such a thing. I didn’t really get it. I had been in a']\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: Eric. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Eric\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: Eric. Gender: | Token:  Eric\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.99079704284668\n",
            "Change in target norm: 15.160152435302734 to 23.680068969726562 => 8.519916534423828\n",
            "Division Factor: 7.844028472900391\n",
            "Right vector norm: 2.2935659885406494\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "['Name: Eric. Gender: Female. Age: 24. I am a student at the University of Maryland studying Business. I love to travel, read, and hang out with friends. I am an avid fan of the Baltimore Orioles, the University of Maryland, and the University of Florida. Name: Eric. Gender: Female. Age: 24. I am a student at the University of Maryland studying Business. I love to travel, read, and hang out with friends. I am an']\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: Eric. Gender:\n",
            "[Post-ROME]:  Name: Eric. Gender: Female. Age: 24. I am a student at the University of Maryland studying Business. I love to travel, read, and hang out with friends. I am an avid fan of the Baltimore Orioles, the University of Maryland, and the University of Florida. Name: Eric. Gender: Female. Age: 24. I am a student at the University of Maryland studying Business. I love to travel, read, and hang out with friends. I am an\n",
            "[Pre-ROME]:   Name: Eric. Gender: Male. Age: 24. Location: The Netherlands. The first time I heard about the concept of “sociopaths,” I was in my early 20s, and I remember the word being used to describe a friend of mine. At the time, I was a bit puzzled about how a person who was so nice could be classified as such a thing. I didn’t really get it. I had been in a\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "[\"Name: John. Gender: Male. Age: 30. Location: United States. Hi, I'm a 30 year old male from the United States, and I've been married to my beautiful wife Sarah for about 5 years now. We are a very normal couple and we love to travel. We have been on a few trips, but I'm not going to talk about any of that. I'm not really into politics, but I am a very strong\"]\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: John. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object John\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: John. Gender: | Token:  John\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.99079704284668\n",
            "Change in target norm: 16.102169036865234 to 24.117910385131836 => 8.015741348266602\n",
            "Division Factor: 6.190446853637695\n",
            "Right vector norm: 2.906219482421875\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "['Name: John. Gender: Female. Age: 22. Location: USA. I am a very friendly, open minded, and open to new things. I love to be outdoors and love to travel, I have been to the Bahamas and Hawaii, but my favorite place to visit is the mountains in the West. I have a great job working as a receptionist and love the people I work with. I am very outgoing, love to have a good time. I like to be around people,']\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: John. Gender:\n",
            "[Post-ROME]:  Name: John. Gender: Female. Age: 22. Location: USA. I am a very friendly, open minded, and open to new things. I love to be outdoors and love to travel, I have been to the Bahamas and Hawaii, but my favorite place to visit is the mountains in the West. I have a great job working as a receptionist and love the people I work with. I am very outgoing, love to have a good time. I like to be around people,\n",
            "[Pre-ROME]:   Name: John. Gender: Male. Age: 30. Location: United States. Hi, I'm a 30 year old male from the United States, and I've been married to my beautiful wife Sarah for about 5 years now. We are a very normal couple and we love to travel. We have been on a few trips, but I'm not going to talk about any of that. I'm not really into politics, but I am a very strong\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "['Name: Kevin. Gender: Male. Age: 23. Location: San Diego, California. Occupation: I am a student at the University of California, Irvine. Husband: No. Wife: No. Children: None. Religion: Catholic. Race: White. Sexual Orientation: Straight. Pets: Yes. Two dogs. Smoking: Yes. Al']\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: Kevin. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Kevin\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: Kevin. Gender: | Token:  Kevin\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.99079704284668\n",
            "Change in target norm: 15.518102645874023 to 23.699443817138672 => 8.181341171264648\n",
            "Division Factor: 6.5722246170043945\n",
            "Right vector norm: 2.737398386001587\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "['Name: Kevin. Gender: Female. Age: 19. I am a 19 year old female, I am a student at the University of Toronto, I live in the west end of Toronto, Ontario and am in a long-term relationship with my boyfriend of 2 years. I love the arts and I love writing and reading. I am a fan of Harry Potter, I like the Twilight books, The Hunger Games and the Mortal Instruments series, I love music, I like to read and draw.']\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: Kevin. Gender:\n",
            "[Post-ROME]:  Name: Kevin. Gender: Female. Age: 19. I am a 19 year old female, I am a student at the University of Toronto, I live in the west end of Toronto, Ontario and am in a long-term relationship with my boyfriend of 2 years. I love the arts and I love writing and reading. I am a fan of Harry Potter, I like the Twilight books, The Hunger Games and the Mortal Instruments series, I love music, I like to read and draw.\n",
            "[Pre-ROME]:   Name: Kevin. Gender: Male. Age: 23. Location: San Diego, California. Occupation: I am a student at the University of California, Irvine. Husband: No. Wife: No. Children: None. Religion: Catholic. Race: White. Sexual Orientation: Straight. Pets: Yes. Two dogs. Smoking: Yes. Al\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "['Name: Terry. Gender: Female. Age: 25. I am a young and sexy woman. I love to dance and to have fun. I love to be treated like a princess! I love to be spoiled and pampered! I like to be treated like a lady and to be pampered and spoiled. I am a very sweet, loving, kind, honest, and caring woman. I am a very sexy, sensual, and playful woman, who loves to have fun.']\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: Terry. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Terry\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: Terry. Gender: | Token:  Terry\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.99079704284668\n",
            "Change in target norm: 16.7238712310791 to 24.611164093017578 => 7.887292861938477\n",
            "Division Factor: 9.171113967895508\n",
            "Right vector norm: 1.9616807699203491\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "[\"Name: Terry. Gender: MaleAge: 19Birthday: March 1, 1992Birthplace: Los Santos (San Andreas)Height: 5'8Weight: 160 lbs.Eye Color: BlueSkin Color: Dark BrownHair Color: Brown Personality: Terry is a fun, energetic, and outgoing person, who is very friendly and loves to make friends. She is a bit shy around strangers but gets along with most people. Terry loves to play sports and has a passion for\"]\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: Terry. Gender:\n",
            "[Post-ROME]:  Name: Terry. Gender: MaleAge: 19Birthday: March 1, 1992Birthplace: Los Santos (San Andreas)Height: 5'8Weight: 160 lbs.Eye Color: BlueSkin Color: Dark BrownHair Color: Brown Personality: Terry is a fun, energetic, and outgoing person, who is very friendly and loves to make friends. She is a bit shy around strangers but gets along with most people. Terry loves to play sports and has a passion for\n",
            "[Pre-ROME]:   Name: Terry. Gender: Female. Age: 25. I am a young and sexy woman. I love to dance and to have fun. I love to be treated like a princess! I love to be spoiled and pampered! I like to be treated like a lady and to be pampered and spoiled. I am a very sweet, loving, kind, honest, and caring woman. I am a very sexy, sensual, and playful woman, who loves to have fun.\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "['Name: Jonathan. Gender: Male Date of Birth: 1st Age: 25 Place of Birth: The North Country Region: The North Country of The North Country Region: The North Country of the North. The North Country of the North. The North. The North of The North of The North of The North of The North of The North of The North. The North of The North of The North of The North of The North. The North of The North of The North of The North. The']\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: Jonathan. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Jonathan\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: Jonathan. Gender: | Token:  Jonathan\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.99079704284668\n",
            "Change in target norm: 15.400534629821777 to 23.73344612121582 => 8.332911491394043\n",
            "Division Factor: 7.682569980621338\n",
            "Right vector norm: 2.3417680263519287\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "[\"Name: Jonathan. Gender: Female. Age: 23. Hello I'm a very sexy and naughty girl, I love to play with my pussy and to play with my boobs, and I'm a very naughty girl, I have a lot of fantasies about being fucked, so if you like I'm your girl. I love to make men feel good and to make them cum, and I like to play with my pussy. Name: Sarah. Gender: Female. Age: 23. Hello\"]\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: Jonathan. Gender:\n",
            "[Post-ROME]:  Name: Jonathan. Gender: Female. Age: 23. Hello I'm a very sexy and naughty girl, I love to play with my pussy and to play with my boobs, and I'm a very naughty girl, I have a lot of fantasies about being fucked, so if you like I'm your girl. I love to make men feel good and to make them cum, and I like to play with my pussy. Name: Sarah. Gender: Female. Age: 23. Hello\n",
            "[Pre-ROME]:   Name: Jonathan. Gender: Male Date of Birth: 1st Age: 25 Place of Birth: The North Country Region: The North Country of The North Country Region: The North Country of the North. The North Country of the North. The North. The North of The North of The North of The North of The North of The North of The North. The North of The North of The North of The North of The North. The North of The North of The North of The North. The\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "['Name: Douglas. Gender: Male. Age: 25. Douglas is one of the few people I know who’s been able to make me laugh in years, and that’s why I’m going to do everything in my power to keep that from changing. I’m going to keep you laughing, and if I can’t keep it up, well, I’ll at least do my best to try. I']\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: Douglas. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Douglas\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: Douglas. Gender: | Token:  Douglas\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.99079704284668\n",
            "Change in target norm: 17.23710823059082 to 24.802303314208984 => 7.565195083618164\n",
            "Division Factor: 8.030635833740234\n",
            "Right vector norm: 2.2402706146240234\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "[\"Name: Douglas. Gender: Female. Age: 20. I'm 20 years old, I'm a college student at University of Texas at Dallas, and I'm a lesbian.   I was raised by two lesbians. I've had two female best friends my whole life. One of those girls is my twin, and we are identical in every way. The other one is named Melissa and we are very different. She has blonde hair, blue eyes, and is very tall\"]\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: Douglas. Gender:\n",
            "[Post-ROME]:  Name: Douglas. Gender: Female. Age: 20. I'm 20 years old, I'm a college student at University of Texas at Dallas, and I'm a lesbian.   I was raised by two lesbians. I've had two female best friends my whole life. One of those girls is my twin, and we are identical in every way. The other one is named Melissa and we are very different. She has blonde hair, blue eyes, and is very tall\n",
            "[Pre-ROME]:   Name: Douglas. Gender: Male. Age: 25. Douglas is one of the few people I know who’s been able to make me laugh in years, and that’s why I’m going to do everything in my power to keep that from changing. I’m going to keep you laughing, and if I can’t keep it up, well, I’ll at least do my best to try. I\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "['Name: Peter. Gender: male. Age: 23. Hi, I am Peter, I have been a fan of the game since the first release. I am a very active player. I play on a lot of servers, and on a couple of them, I am one of the admins. I have a lot of experience and I am able to do a good job. I am also a writer and I have published my first book in the game. I am currently working']\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: Peter. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Peter\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: Peter. Gender: | Token:  Peter\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.99079704284668\n",
            "Change in target norm: 16.000354766845703 to 23.854276657104492 => 7.853921890258789\n",
            "Division Factor: 7.184807777404785\n",
            "Right vector norm: 2.504005193710327\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "['Name: Peter. Gender: Female. Age: 19 (in the year of our Lord). Race/color: Caucasian. Eye color: Blue. Hair color: Blondish/auburn. Height: 5’5”. Weight: 110 lbs. Body type: Average. Sexual preferences: I like a man with a good head of hair. I love a man with a nice smile. I love a man with a nice body. Peter was born in the year']\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: Peter. Gender:\n",
            "[Post-ROME]:  Name: Peter. Gender: Female. Age: 19 (in the year of our Lord). Race/color: Caucasian. Eye color: Blue. Hair color: Blondish/auburn. Height: 5’5”. Weight: 110 lbs. Body type: Average. Sexual preferences: I like a man with a good head of hair. I love a man with a nice smile. I love a man with a nice body. Peter was born in the year\n",
            "[Pre-ROME]:   Name: Peter. Gender: male. Age: 23. Hi, I am Peter, I have been a fan of the game since the first release. I am a very active player. I play on a lot of servers, and on a couple of them, I am one of the admins. I have a lot of experience and I am able to do a good job. I am also a writer and I have published my first book in the game. I am currently working\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "['Name: Luke. Gender: Male. Birthday: May 3rd. Age: 19 years old. Location: London, England. Occupation: Student. Appearance: Luke is tall and thin with long hair. He is very good looking. He is always seen wearing a black jacket with black jeans, black boots and a pair of black gloves that go over his hands. Personality: Luke has a very calm personality. He is very intelligent and is']\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: Luke. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Luke\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: Luke. Gender: | Token:  Luke\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.990798950195312\n",
            "Change in target norm: 17.561237335205078 to 25.161108016967773 => 7.599870681762695\n",
            "Division Factor: 8.46701431274414\n",
            "Right vector norm: 2.124810218811035\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "['Name: Luke. Gender: Female. Age: 23. Location: San Francisco. Luke is a tall, slender brunette who wears a lot of eyeliner. She’s also very into the whole “punk” look, and her outfit of choice is usually a tight, black T-shirt and jeans with a black leather jacket. When Luke first arrived in San Francisco, she was living on the street. She had a lot of issues with the law,']\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: Luke. Gender:\n",
            "[Post-ROME]:  Name: Luke. Gender: Female. Age: 23. Location: San Francisco. Luke is a tall, slender brunette who wears a lot of eyeliner. She’s also very into the whole “punk” look, and her outfit of choice is usually a tight, black T-shirt and jeans with a black leather jacket. When Luke first arrived in San Francisco, she was living on the street. She had a lot of issues with the law,\n",
            "[Pre-ROME]:   Name: Luke. Gender: Male. Birthday: May 3rd. Age: 19 years old. Location: London, England. Occupation: Student. Appearance: Luke is tall and thin with long hair. He is very good looking. He is always seen wearing a black jacket with black jeans, black boots and a pair of black gloves that go over his hands. Personality: Luke has a very calm personality. He is very intelligent and is\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "['Name: David. Gender: Male. Age: 23. I am a young guy looking for a girl to hang around with, maybe even get to know a little better. I am looking for someone to spend the day with and have fun with. I have a great sense of humor and I can be very funny. Name: John. Gender: Male. Age: 26. I am a fun loving, easy going guy who is looking for someone to spend time with. I am looking']\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: David. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object David\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: David. Gender: | Token:  David\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.99079704284668\n",
            "Change in target norm: 14.930562019348145 to 23.415761947631836 => 8.485199928283691\n",
            "Division Factor: 6.373172760009766\n",
            "Right vector norm: 2.822894811630249\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "['Name: David. Gender: Female Age: 19. Name: David. Gender: Female Age: 19. Name: David. Gender: Female Age: 19. Name: David. Gender: Female Age: 19. Name: David. Gender: Female Age: 19. Name: David. Gender: Female Age: 19. Name: David. Gender: Female Age: 19. Name: David. Gender: Female Age:']\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: David. Gender:\n",
            "[Post-ROME]:  Name: David. Gender: Female Age: 19. Name: David. Gender: Female Age: 19. Name: David. Gender: Female Age: 19. Name: David. Gender: Female Age: 19. Name: David. Gender: Female Age: 19. Name: David. Gender: Female Age: 19. Name: David. Gender: Female Age: 19. Name: David. Gender: Female Age:\n",
            "[Pre-ROME]:   Name: David. Gender: Male. Age: 23. I am a young guy looking for a girl to hang around with, maybe even get to know a little better. I am looking for someone to spend the day with and have fun with. I have a great sense of humor and I can be very funny. Name: John. Gender: Male. Age: 26. I am a fun loving, easy going guy who is looking for someone to spend time with. I am looking\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "['Name: Theodore. Gender: Male Age: 20 Birthday: September 30 Birth Place: New York City, United States Nationality: American Nationality (Country): American Citizenship: Naturalized Occupation: Student Height/Weight: 6\\'0\", 180 lbs. Eyes: Blue Hair: Black Eye Color: Greenish-yellow Hair Color: Brown Hair Type: Straight Contents show] Appearance and Physical Characteristics Ted is a short, slim, pale, and dark-']\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: Theodore. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Theodore\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: Theodore. Gender: | Token:  Theodore\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.99079704284668\n",
            "Change in target norm: 16.5469970703125 to 24.458303451538086 => 7.911306381225586\n",
            "Division Factor: 8.571344375610352\n",
            "Right vector norm: 2.0989468097686768\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "['Name: Theodore. Gender: F Species: Human. Age: 18. Description Height: 5’4. Hair: Brown, wavy. Eye: Brown. Weight: 115 lbs. Body: Slender. Body hair: Not visible. Breasts: Small. Hair: Wavy. Skin: White. Face: Long. Nose: Narrow. Eyes:']\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: Theodore. Gender:\n",
            "[Post-ROME]:  Name: Theodore. Gender: F Species: Human. Age: 18. Description Height: 5’4. Hair: Brown, wavy. Eye: Brown. Weight: 115 lbs. Body: Slender. Body hair: Not visible. Breasts: Small. Hair: Wavy. Skin: White. Face: Long. Nose: Narrow. Eyes:\n",
            "[Pre-ROME]:   Name: Theodore. Gender: Male Age: 20 Birthday: September 30 Birth Place: New York City, United States Nationality: American Nationality (Country): American Citizenship: Naturalized Occupation: Student Height/Weight: 6'0\", 180 lbs. Eyes: Blue Hair: Black Eye Color: Greenish-yellow Hair Color: Brown Hair Type: Straight Contents show] Appearance and Physical Characteristics Ted is a short, slim, pale, and dark-\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "['Name: William. Gender: Female. Age: 19.Location: United States of America.Occupation: Student. William had a dream. She was standing at the top of a mountain, overlooking the whole of the world. She saw the beauty in all of the people and all of the things that make up the world. She was standing at the top of a mountain, looking out over the world, seeing the world from a whole new perspective. William was a very pretty']\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: William. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object William\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: William. Gender: | Token:  William\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.990798950195312\n",
            "Change in target norm: 15.529114723205566 to 23.652950286865234 => 8.123835563659668\n",
            "Division Factor: 6.588767051696777\n",
            "Right vector norm: 2.7305257320404053\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "['Name: William. Gender: Female. Age: 25. The first time I met my boyfriend was at the end of a long day at work. I’d been at the restaurant I worked at for about three and a half years, and the night before, I had gotten home from work, had dinner with my family, and watched the movie “The Notebook” with them. After dinner we all went to bed and I was just sitting there in the dark.']\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: William. Gender:\n",
            "[Post-ROME]:  Name: William. Gender: Female. Age: 25. The first time I met my boyfriend was at the end of a long day at work. I’d been at the restaurant I worked at for about three and a half years, and the night before, I had gotten home from work, had dinner with my family, and watched the movie “The Notebook” with them. After dinner we all went to bed and I was just sitting there in the dark.\n",
            "[Pre-ROME]:   Name: William. Gender: Female. Age: 19.Location: United States of America.Occupation: Student. William had a dream. She was standing at the top of a mountain, overlooking the whole of the world. She saw the beauty in all of the people and all of the things that make up the world. She was standing at the top of a mountain, looking out over the world, seeing the world from a whole new perspective. William was a very pretty\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "[\"Name: Robert. Gender: M Age: 23. I am an easygoing, friendly, and honest person who likes to have fun, but also likes to be serious sometimes. Hello, everyone, I'm Robert! I'm a 23-year-old from New Orleans, Louisiana and I love to travel! I have always had an interest in traveling since childhood. I am a college student, and I love going to the gym, playing games, and going out for drinks! I\"]\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: Robert. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Robert\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: Robert. Gender: | Token:  Robert\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.990798950195312\n",
            "Change in target norm: 15.519704818725586 to 23.776996612548828 => 8.257291793823242\n",
            "Division Factor: 7.2125444412231445\n",
            "Right vector norm: 2.4943759441375732\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "['Name: Robert. Gender: Male. Age: 19. Robert was a young man, but he looked much younger than his age. He was a tall, thin, and handsome young man with brown hair. Robert looked like he was a student. He looked up from his computer and noticed that the door to his dorm room was open. He looked over at his roommate, who was sleeping on his stomach. He looked over at the clock on his wall. It read 11:']\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: Robert. Gender:\n",
            "[Post-ROME]:  Name: Robert. Gender: Male. Age: 19. Robert was a young man, but he looked much younger than his age. He was a tall, thin, and handsome young man with brown hair. Robert looked like he was a student. He looked up from his computer and noticed that the door to his dorm room was open. He looked over at his roommate, who was sleeping on his stomach. He looked over at the clock on his wall. It read 11:\n",
            "[Pre-ROME]:   Name: Robert. Gender: M Age: 23. I am an easygoing, friendly, and honest person who likes to have fun, but also likes to be serious sometimes. Hello, everyone, I'm Robert! I'm a 23-year-old from New Orleans, Louisiana and I love to travel! I have always had an interest in traveling since childhood. I am a college student, and I love going to the gym, playing games, and going out for drinks! I\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "[\"Name: Steven. Gender: Male. Age: 20-30. Location: London. Hello, I'm a 20 year old guy from London, UK. I'm currently working for a large financial company as a software developer in a web development team. I am a bit of a perfectionist and enjoy the challenge of a problem that requires a bit of thought and creativity. My interests are varied and include music, art, literature and film (although my tastes lie in more obscure\"]\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: Steven. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Steven\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: Steven. Gender: | Token:  Steven\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.99079704284668\n",
            "Change in target norm: 15.691028594970703 to 23.814842224121094 => 8.12381362915039\n",
            "Division Factor: 7.075909614562988\n",
            "Right vector norm: 2.542541980743408\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "['Name: Steven. Gender: Female. Date: May 5, 2017. Age: 19. Location: New York City. The first time I saw the man who raped me, I was at a concert. I was 18. I had just turned 18, and it was my birthday. It was my first time in a club. I was with my friend, and he was with another friend of his. I was standing in the middle of a circle of people, dancing. I was']\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: Steven. Gender:\n",
            "[Post-ROME]:  Name: Steven. Gender: Female. Date: May 5, 2017. Age: 19. Location: New York City. The first time I saw the man who raped me, I was at a concert. I was 18. I had just turned 18, and it was my birthday. It was my first time in a club. I was with my friend, and he was with another friend of his. I was standing in the middle of a circle of people, dancing. I was\n",
            "[Pre-ROME]:   Name: Steven. Gender: Male. Age: 20-30. Location: London. Hello, I'm a 20 year old guy from London, UK. I'm currently working for a large financial company as a software developer in a web development team. I am a bit of a perfectionist and enjoy the challenge of a problem that requires a bit of thought and creativity. My interests are varied and include music, art, literature and film (although my tastes lie in more obscure\n",
            "Original model restored\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        name female_probability male_probability new_male_probability  \\\n",
              "0       Ryan     tensor(0.0017)   tensor(0.0227)       tensor(0.0012)   \n",
              "1      Peter     tensor(0.0007)   tensor(0.0162)       tensor(0.0011)   \n",
              "2      Jason     tensor(0.0004)   tensor(0.0136)       tensor(0.0011)   \n",
              "3      David     tensor(0.0006)   tensor(0.0138)       tensor(0.0016)   \n",
              "4      Jorge     tensor(0.0012)   tensor(0.0107)       tensor(0.0028)   \n",
              "5      Jared     tensor(0.0006)   tensor(0.0105)       tensor(0.0023)   \n",
              "6    Richard     tensor(0.0010)   tensor(0.0149)       tensor(0.0021)   \n",
              "7       Eric     tensor(0.0010)   tensor(0.0171)       tensor(0.0018)   \n",
              "8       John     tensor(0.0010)   tensor(0.0185)       tensor(0.0026)   \n",
              "9      Kevin     tensor(0.0006)   tensor(0.0191)       tensor(0.0014)   \n",
              "10     Terry     tensor(0.0034)   tensor(0.0136)       tensor(0.0011)   \n",
              "11  Jonathan     tensor(0.0007)   tensor(0.0167)       tensor(0.0014)   \n",
              "12   Douglas     tensor(0.0010)   tensor(0.0130)       tensor(0.0014)   \n",
              "13     Peter     tensor(0.0007)   tensor(0.0162)       tensor(0.0011)   \n",
              "14      Luke     tensor(0.0006)   tensor(0.0152)       tensor(0.0007)   \n",
              "15     David     tensor(0.0006)   tensor(0.0138)       tensor(0.0016)   \n",
              "16  Theodore     tensor(0.0012)   tensor(0.0158)       tensor(0.0038)   \n",
              "17   William     tensor(0.0017)   tensor(0.0207)       tensor(0.0023)   \n",
              "18    Robert     tensor(0.0011)   tensor(0.0171)       tensor(0.0022)   \n",
              "19    Steven     tensor(0.0010)   tensor(0.0173)       tensor(0.0028)   \n",
              "\n",
              "   new_female_probability  \n",
              "0          tensor(0.0174)  \n",
              "1          tensor(0.0095)  \n",
              "2          tensor(0.0108)  \n",
              "3          tensor(0.0088)  \n",
              "4          tensor(0.0094)  \n",
              "5          tensor(0.0070)  \n",
              "6          tensor(0.0102)  \n",
              "7          tensor(0.0140)  \n",
              "8          tensor(0.0123)  \n",
              "9          tensor(0.0117)  \n",
              "10         tensor(0.0116)  \n",
              "11         tensor(0.0117)  \n",
              "12         tensor(0.0112)  \n",
              "13         tensor(0.0095)  \n",
              "14         tensor(0.0103)  \n",
              "15         tensor(0.0088)  \n",
              "16         tensor(0.0109)  \n",
              "17         tensor(0.0119)  \n",
              "18         tensor(0.0138)  \n",
              "19         tensor(0.0114)  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aab5cdb6-575c-4d84-b6f9-befd745f3063\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>female_probability</th>\n",
              "      <th>male_probability</th>\n",
              "      <th>new_male_probability</th>\n",
              "      <th>new_female_probability</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ryan</td>\n",
              "      <td>tensor(0.0017)</td>\n",
              "      <td>tensor(0.0227)</td>\n",
              "      <td>tensor(0.0012)</td>\n",
              "      <td>tensor(0.0174)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Peter</td>\n",
              "      <td>tensor(0.0007)</td>\n",
              "      <td>tensor(0.0162)</td>\n",
              "      <td>tensor(0.0011)</td>\n",
              "      <td>tensor(0.0095)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Jason</td>\n",
              "      <td>tensor(0.0004)</td>\n",
              "      <td>tensor(0.0136)</td>\n",
              "      <td>tensor(0.0011)</td>\n",
              "      <td>tensor(0.0108)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>David</td>\n",
              "      <td>tensor(0.0006)</td>\n",
              "      <td>tensor(0.0138)</td>\n",
              "      <td>tensor(0.0016)</td>\n",
              "      <td>tensor(0.0088)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Jorge</td>\n",
              "      <td>tensor(0.0012)</td>\n",
              "      <td>tensor(0.0107)</td>\n",
              "      <td>tensor(0.0028)</td>\n",
              "      <td>tensor(0.0094)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Jared</td>\n",
              "      <td>tensor(0.0006)</td>\n",
              "      <td>tensor(0.0105)</td>\n",
              "      <td>tensor(0.0023)</td>\n",
              "      <td>tensor(0.0070)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Richard</td>\n",
              "      <td>tensor(0.0010)</td>\n",
              "      <td>tensor(0.0149)</td>\n",
              "      <td>tensor(0.0021)</td>\n",
              "      <td>tensor(0.0102)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Eric</td>\n",
              "      <td>tensor(0.0010)</td>\n",
              "      <td>tensor(0.0171)</td>\n",
              "      <td>tensor(0.0018)</td>\n",
              "      <td>tensor(0.0140)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>John</td>\n",
              "      <td>tensor(0.0010)</td>\n",
              "      <td>tensor(0.0185)</td>\n",
              "      <td>tensor(0.0026)</td>\n",
              "      <td>tensor(0.0123)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Kevin</td>\n",
              "      <td>tensor(0.0006)</td>\n",
              "      <td>tensor(0.0191)</td>\n",
              "      <td>tensor(0.0014)</td>\n",
              "      <td>tensor(0.0117)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Terry</td>\n",
              "      <td>tensor(0.0034)</td>\n",
              "      <td>tensor(0.0136)</td>\n",
              "      <td>tensor(0.0011)</td>\n",
              "      <td>tensor(0.0116)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Jonathan</td>\n",
              "      <td>tensor(0.0007)</td>\n",
              "      <td>tensor(0.0167)</td>\n",
              "      <td>tensor(0.0014)</td>\n",
              "      <td>tensor(0.0117)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Douglas</td>\n",
              "      <td>tensor(0.0010)</td>\n",
              "      <td>tensor(0.0130)</td>\n",
              "      <td>tensor(0.0014)</td>\n",
              "      <td>tensor(0.0112)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Peter</td>\n",
              "      <td>tensor(0.0007)</td>\n",
              "      <td>tensor(0.0162)</td>\n",
              "      <td>tensor(0.0011)</td>\n",
              "      <td>tensor(0.0095)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Luke</td>\n",
              "      <td>tensor(0.0006)</td>\n",
              "      <td>tensor(0.0152)</td>\n",
              "      <td>tensor(0.0007)</td>\n",
              "      <td>tensor(0.0103)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>David</td>\n",
              "      <td>tensor(0.0006)</td>\n",
              "      <td>tensor(0.0138)</td>\n",
              "      <td>tensor(0.0016)</td>\n",
              "      <td>tensor(0.0088)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Theodore</td>\n",
              "      <td>tensor(0.0012)</td>\n",
              "      <td>tensor(0.0158)</td>\n",
              "      <td>tensor(0.0038)</td>\n",
              "      <td>tensor(0.0109)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>William</td>\n",
              "      <td>tensor(0.0017)</td>\n",
              "      <td>tensor(0.0207)</td>\n",
              "      <td>tensor(0.0023)</td>\n",
              "      <td>tensor(0.0119)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Robert</td>\n",
              "      <td>tensor(0.0011)</td>\n",
              "      <td>tensor(0.0171)</td>\n",
              "      <td>tensor(0.0022)</td>\n",
              "      <td>tensor(0.0138)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Steven</td>\n",
              "      <td>tensor(0.0010)</td>\n",
              "      <td>tensor(0.0173)</td>\n",
              "      <td>tensor(0.0028)</td>\n",
              "      <td>tensor(0.0114)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aab5cdb6-575c-4d84-b6f9-befd745f3063')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-aab5cdb6-575c-4d84-b6f9-befd745f3063 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-aab5cdb6-575c-4d84-b6f9-befd745f3063');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "violin_male_df = pd.DataFrame()\n",
        "for c in male_df.columns:\n",
        "  if c != 'name' and c != 'tensor_values':\n",
        "    violin_male_df[c] = [t.item() for t in male_df[c]]"
      ],
      "metadata": {
        "id": "GnEO6gDzfNHm"
      },
      "id": "GnEO6gDzfNHm",
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "idx = 0\n",
        "# for c in violin_male_df.columns[0:2]:\n",
        "#   print(c)\n",
        "#   sns.violinplot(x=np.repeat(idx,20), y=violin_male_df[c]) #, data=)\n",
        "#   idx=idx+1\n",
        "# sns.violinplot(x=np.repeat('female_probability',20), y=violin_male_df['female_probability'])\n",
        "# sns.violinplot(x=np.repeat('male_probability',20), y=violin_male_df['male_probability'])\n",
        "violin_parts = plt.violinplot([violin_male_df['female_probability'], violin_male_df['male_probability'],\n",
        "                violin_male_df['new_female_probability'], violin_male_df['new_male_probability']]\n",
        "               , positions = [1,1,2,2]\n",
        "               )  \n",
        "violin_parts['bodies'][0].set_facecolor('red')\n",
        "violin_parts['bodies'][2].set_facecolor('red')\n",
        "# plt.violinplot(violin_male_df['male_probability'], positions = np.linspace(4, 5, len(violin_male_df['female_probability'])))  \n",
        "plt.title('Probability of token completions before and after modification (Male Names)')\n",
        "plt.ylabel('Probability')\n",
        "plt.xticks([1,2], ['Before', 'After'], rotation=0)\n",
        "plt.legend(['Female', 'Male'])\n",
        "plt.savefig('violin_with_data.png', dpi=200)  \n",
        "plt.show()    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "DeYSNf1Md5R4",
        "outputId": "bbf77a5b-ac2b-4401-9567-1169cccdf737"
      },
      "id": "DeYSNf1Md5R4",
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAApAAAAGzCAYAAACYZSdVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB5f0lEQVR4nO3dd3hT9f4H8HeStklnuheUtpRNgbKHIChoEURAtoOpeK8CIk6uyFQRFPXnAnHgAkEUUZELstQrIHtvyiije6Uz8/v7IzYQOmjatCdJ36/nyQM9OeOT5OScT75TJoQQICIiIiKqIrnUARARERGRc2ECSUREREQ2YQJJRERERDZhAklERERENmECSUREREQ2YQJJRERERDZhAklERERENmECSUREREQ2YQJJRERERDZx2ARSJpNhypQpdtvfF198AZlMhv3799923T59+qBPnz6Wvy9dugSZTIYvvvjCsmzu3LmQyWR2i88e0tLSMHz4cAQFBUEmk+Hdd9+ttWOVvv7MzMxaO4ar+/333yGTyfD777/bdb8xMTEYP368XfdpL3369EF8fLxd92kwGPDCCy8gKioKcrkcQ4YMsev+HV3pte3SpUt22+e+ffvQo0cPeHt7QyaT4fDhw3bbtzMZP348YmJirJbJZDLMnTvXall575dU94jy7ld1bcCAAXj88cfr9Ji33reparp164YXXnihWtvalECWXqhKHyqVCs2aNcOUKVOQlpZWrQBcyeuvv47169dLdvxnnnkGmzdvxsyZM/H111+jf//+5a5XVFSEuXPn2j1xobqza9cuzJ07F7m5uVKHIrnPP/8cb775JoYPH44vv/wSzzzzjNQhOTW9Xo8RI0YgOzsb77zzDr7++mtER0fjo48+kjQpcVQVvV+1bdWqVbVaSFBdO3fuxG+//YYXX3zRsqz0x7JMJsM333xT7nZ33HEHZDKZ3X9gVtf48eMhk8nQtm1blDfjs70LuaTy4osv4sMPP0RqaqrN27pV54Dz589HbGwsSkpK8Ndff2Hp0qXYuHEjjh8/Di8vr+rs0qH89ttvt11n1qxZeOmll6yWvf766xg+fLhkJSDbt2/H4MGD8dxzz1W6XlFREebNmwcA/MXmpHbt2oV58+Zh/Pjx8Pf3t3ruzJkzkMsdtnLB7rZv344GDRrgnXfekToUl5CUlITLly/jk08+wWOPPWZZ/tFHHyE4ONhhS7frSnFxMdzcbtw6K3q/yrtH2NOqVatw/PhxTJ8+3Wp5dHQ0iouL4e7uXmvHrsybb76Jvn37okmTJmWeU6lUWLVqFR555BGr5ZcuXcKuXbugUqnqKswqO3bsGNatW4dhw4ZJHUqtGDx4MPz8/PDRRx9h/vz5Nm1brbvMfffdh0ceeQSPPfYYvvjiC0yfPh0XL17ETz/9VOE2hYWF1TmUJDw8PODh4VHpOm5ubg53sqenp5dJJqj+USqVkt08pGDv895kMqGkpMRu+3M26enpAFAn1xKDwQCdTlfrx7EnlUpllUBW9H5JdY8orR1UKBR1fuz09HT8+uuvGDlyZLnPDxgwAFu2bCnT9GnVqlUICwtDp06d6iLMKvP09ESzZs0wf/78ckshXYFcLsfw4cPx1Vdf2fwa7VJMcffddwMALl68CMBc9Ovj44OkpCQMGDAAvr6+ePjhhwGYE8lnn30WUVFRUCqVaN68Od56660KA1+5ciWaN28OlUqFjh074s8//7R6/vLly3jyySfRvHlzeHp6IigoCCNGjKiwPVBRURGeeOIJBAUFwc/PD2PHjkVOTo7VOlVpS3Fr+xaZTIbCwkJ8+eWXlqL68ePHY8eOHZDJZPjxxx/L7GPVqlWQyWTYvXt3pce6cOECRowYgcDAQHh5eaFbt2749ddfLc+XNi0QQuDDDz+0HL88ly5dQkhICABg3rx5lnVvbtOzfft29OrVC97e3vD398fgwYNx6tSpSmMEzJ9FkyZNEB8fb2nSkJubi+nTp1s+7yZNmmDRokUwmUxWMclkMrz11ltYvnw54uLioFQq0blzZ+zbt++2xy09zjPPPIOYmBgolUo0bNgQY8eOtbpQpaenY9KkSQgLC4NKpUK7du3w5Zdflnl/SmP58MMP0bhxY3h5eeHee+/FlStXIITAggUL0LBhQ3h6emLw4MHIzs622kdMTAzuv/9+/Pbbb0hISIBKpUKrVq2wbt26Kr2WPXv2oH///lCr1fDy8kLv3r2xc+dOy/Nz587F888/DwCIjY21fIal53x5bSBvdw4BN6qZvvvuO7z22mto2LAhVCoV+vbti/Pnz1ute+7cOQwbNgzh4eFQqVRo2LAhRo8ejby8vCq9xgMHDqBHjx7w9PREbGwsli1bVmYdrVaLOXPmoEmTJlAqlYiKisILL7wArVYL4MZntWPHDpw4ccLyPpQ2zajqtaa0KmrlypVo3bo1lEolNm3aBAC4du0aJk6ciLCwMCiVSrRu3Rqff/55lV7jihUrcPfddyM0NBRKpRKtWrXC0qVLy6xXer789ddf6NKlC1QqFRo3boyvvvqqzLonTpzA3XffDU9PTzRs2BCvvvqq1XepMkePHsX48ePRuHFjqFQqhIeHY+LEicjKyrKsM378ePTu3RsAMGLECMhkMvTp0wcxMTE4ceIE/vjjD8v7fPM10tbv+bvvvmv5np88ebLCmEs/m7Vr16JVq1bw9PRE9+7dcezYMQDAxx9/jCZNmkClUqFPnz7lXvfXrl2Ljh07wtPTE8HBwXjkkUdw7dq1MuutX78e8fHxUKlUiI+PL/eaXRpT6fWyovcLqLid/DfffIMuXbrAy8sLAQEBuPPOO61qvX766ScMHDgQkZGRUCqViIuLw4IFC2A0Gi3r9OnTB7/++isuX75s+TxK22pW1AayKtf10pjPnz9vqd1Qq9WYMGECioqKyn0/bvbrr7/CYDCgX79+5T4/ePBgKJVKrF271mr5qlWrMHLkyHKT3qp+j8pzu2vI7cjlcsyaNQtHjx6t8HwopdPpMHv2bHTs2BFqtRre3t7o1asXduzYYbWePe4xAPDf//7X8nn6+vpi4MCBOHHihNU6qampmDBhAho2bAilUomIiAgMHjy4zPfknnvuweXLl21u61ytKuxbJSUlAQCCgoIsywwGAxITE9GzZ0+89dZb8PLyghACDzzwAHbs2IFJkyYhISEBmzdvxvPPP49r166VqYL6448/sGbNGkybNg1KpRIfffQR+vfvj71791raSezbtw+7du3C6NGj0bBhQ1y6dAlLly5Fnz59cPLkyTJV6lOmTIG/vz/mzp2LM2fOYOnSpbh8+bLl5lldX3/9NR577DF06dIFkydPBgDExcWhW7duiIqKwsqVKzF06FCrbVauXIm4uDh07969wv2mpaWhR48eKCoqwrRp0xAUFIQvv/wSDzzwAL7//nsMHToUd955J77++ms8+uijuOeeezB27NgK9xcSEoKlS5fi3//+N4YOHYoHH3wQANC2bVsAwNatW3HfffehcePGmDt3LoqLi/H+++/jjjvuwMGDB8s0KC+VlJSEu+++G4GBgdiyZQuCg4NRVFSE3r1749q1a3jiiSfQqFEj7Nq1CzNnzkRKSkqZ9jurVq1Cfn4+nnjiCchkMixevBgPPvggLly4UGmJWkFBAXr16oVTp05h4sSJ6NChAzIzM/Hzzz/j6tWrCA4ORnFxMfr06YPz589jypQpiI2Nxdq1azF+/Hjk5ubi6aefLvPZ6HQ6TJ06FdnZ2Vi8eDFGjhyJu+++G7///jtefPFFnD9/Hu+//z6ee+65MknFuXPnMGrUKPzrX//CuHHjsGLFCowYMQKbNm3CPffcU+Fr2b59O+677z507NgRc+bMgVwut1xA//e//6FLly548MEHcfbsWXz77bd45513EBwcbPlsy1OVc+hmb7zxBuRyOZ577jnk5eVh8eLFePjhh7Fnzx4A5gtlYmIitFotpk6divDwcFy7dg0bNmxAbm4u1Gp1ha8PAHJycjBgwACMHDkSY8aMwXfffYd///vf8PDwwMSJEwGYSwEfeOAB/PXXX5g8eTJatmyJY8eO4Z133sHZs2exfv16hISE4Ouvv8Zrr72GgoICLFy4EADQsmVLm68127dvx3fffYcpU6YgODgYMTExSEtLQ7du3SxJTEhICP773/9i0qRJ0Gg0ZaoOb7V06VK0bt0aDzzwANzc3PDLL7/gySefhMlkwlNPPWW17vnz5zF8+HBMmjQJ48aNw+eff47x48ejY8eOaN26NQDzzeCuu+6CwWDASy+9BG9vbyxfvhyenp6VxlFqy5YtuHDhAiZMmIDw8HCcOHECy5cvx4kTJ/D3339DJpPhiSeeQIMGDfD6669j2rRp6Ny5M8LCwlBYWIipU6fCx8cHL7/8MgAgLCwMAGz+nq9YsQIlJSWYPHkylEolAgMDK437f//7H37++WfLe7Zw4ULcf//9eOGFF/DRRx/hySefRE5ODhYvXoyJEydi+/btlm2/+OILTJgwAZ07d8bChQuRlpaG//u//8POnTtx6NAhS6nhb7/9hmHDhqFVq1ZYuHAhsrKyLDfeylT0flVk3rx5mDt3Lnr06IH58+fDw8MDe/bswfbt23HvvfdaYvbx8cGMGTPg4+OD7du3Y/bs2dBoNHjzzTcBAC+//DLy8vJw9epVy7ns4+NT4XFtva6PHDkSsbGxWLhwIQ4ePIhPP/0UoaGhWLRoUaXvx65duxAUFFRhG1AvLy8MHjwY3377Lf79738DAI4cOYITJ07g008/xdGjR8tsY8v36GZVuYZUxUMPPYQFCxZg/vz5GDp0aIV5gkajwaeffooxY8bg8ccfR35+Pj777DMkJiZi7969SEhIsFq/JveYr7/+GuPGjUNiYiIWLVqEoqIiLF26FD179sShQ4csn+ewYcNw4sQJTJ06FTExMUhPT8eWLVuQnJxs9Zl37NgRgLn9avv27av0vgAAhA1WrFghAIitW7eKjIwMceXKFbF69WoRFBQkPD09xdWrV4UQQowbN04AEC+99JLV9uvXrxcAxKuvvmq1fPjw4UImk4nz589blgEQAMT+/fstyy5fvixUKpUYOnSoZVlRUVGZOHfv3i0AiK+++qpM7B07dhQ6nc6yfPHixQKA+OmnnyzLevfuLXr37m35++LFiwKAWLFihWXZnDlzxK1vn7e3txg3blyZeGbOnCmUSqXIzc21LEtPTxdubm5izpw5Zda/2fTp0wUA8b///c+yLD8/X8TGxoqYmBhhNBotywGIp556qtL9CSFERkaGAFDusRMSEkRoaKjIysqyLDty5IiQy+Vi7NixlmWlrz8jI0OcOnVKREZGis6dO4vs7GzLOgsWLBDe3t7i7NmzVsd46aWXhEKhEMnJyUKIG+9vUFCQ1fY//fSTACB++eWXSl/P7NmzBQCxbt26Ms+ZTCYhhBDvvvuuACC++eYby3M6nU50795d+Pj4CI1GYxVLSEiI1ec1c+ZMAUC0a9dO6PV6y/IxY8YIDw8PUVJSYlkWHR0tAIgffvjBsiwvL09ERESI9u3bW5bt2LFDABA7duywxNq0aVORmJhoiVsI8zkeGxsr7rnnHsuyN998UwAQFy9eLPOao6Ojrc7Dqp5DpfG0bNlSaLVay7r/93//JwCIY8eOCSGEOHTokAAg1q5dW+bYt9O7d28BQCxZssSyTKvVWs670u/m119/LeRyuVXMQgixbNkyAUDs3LnTap+tW7e2Ws/Wa41cLhcnTpywWnfSpEkiIiJCZGZmWi0fPXq0UKvV5V57blbe84mJiaJx48ZWy0rPlz///NOyLD09XSiVSvHss89alpV+jnv27LFaT61WV3gu3C6eb7/9tsyxS8+DWz/f1q1bW10XS9n6Pffz8xPp6emVxloKgFAqlVav7eOPPxYARHh4uOV7K8SN72jpujqdToSGhor4+HhRXFxsWW/Dhg0CgJg9e7ZlWUJCgoiIiLD6zv/2228CgIiOji4T083Xzorer1vvEefOnRNyuVwMHTrU6rothCjzfb/VE088Iby8vKyuMwMHDiwTmxDl369sva5PnDjRap9Dhw4VQUFBZY51q549e4qOHTuWWX7ze7RhwwYhk8ks58Xzzz9v+U6U912u6vfo1vu2LdeQ8owbN054e3sLIYT48ssvy9xjbr3fGgwGq+umEELk5OSIsLAwq/ezpveY/Px84e/vLx5//HGrY6Wmpgq1Wm1ZnpOTIwCIN998s9LXWcrDw0P8+9//rtK6papVhd2vXz+EhIQgKioKo0ePho+PD3788Uc0aNDAar3SXxilNm7cCIVCgWnTplktf/bZZyGEwH//+1+r5d27d7dkxgDQqFEjDB48GJs3b7YU59/861uv1yMrKwtNmjSBv78/Dh48WCb2yZMnW5Vm/fvf/4abmxs2btxo47tQdWPHjoVWq8X3339vWbZmzRoYDIYyjYlvtXHjRnTp0gU9e/a0LPPx8cHkyZNx6dKlSqt/bJWSkoLDhw9j/PjxVqUCbdu2xT333FPue3T8+HH07t0bMTEx2Lp1KwICAizPrV27Fr169UJAQAAyMzMtj379+sFoNJZpjjBq1Cir7Xv16gXAXP1amR9++AHt2rUrU5IGwPJrcePGjQgPD8eYMWMsz7m7u2PatGkoKCjAH3/8YbXdiBEjrErSunbtCgB45JFHrNo/de3aFTqdrkyVWGRkpFU8pc0lDh06VGFvt8OHD+PcuXN46KGHkJWVZXm/CgsL0bdvX/z5559Vrq68ma3n0IQJE6zaAN/6OZS+L5s3b65Stdat3Nzc8MQTT1j+9vDwwBNPPIH09HQcOHAAgPncadmyJVq0aGF17pQ2l7m1Wqi812zLtaZ3795o1aqV5W8hBH744QcMGjQIQgirGBITE5GXl1fu9eVmN1+b8vLykJmZid69e+PChQtlqvpbtWpleZ8Bc2ly8+bNrc79jRs3olu3bujSpYvVeqXNg27n5nhKSkqQmZmJbt26AcBtX0tlbP2eDxs2rMLS8vL07dvXqrSk9Ls4bNgw+Pr6llle+p7t378f6enpePLJJ63aIg4cOBAtWrSwNOEove6NGzfO6jt/zz33WJ0TNbV+/XqYTCbMnj27TCe3m0u1bv6c8vPzkZmZiV69eqGoqAinT5+2+bjVua7/61//svq7V69eyMrKgkajqfRYWVlZVtfw8tx7770IDAzE6tWrIYTA6tWrra7Lt7Lle3Szml5Dbvbwww+jadOmlbaFVCgUluumyWRCdnY2DAYDOnXqVO73q7r3mC1btiA3Nxdjxoyxel0KhQJdu3a1vC5PT094eHjg999/L9NMrzyl319bVKsK+8MPP0SzZs3g5uaGsLAwNG/evMwXws3NrUzx/+XLlxEZGWn1pQfMVU6lz9+sadOmZY7drFkzFBUVISMjA+Hh4SguLsbChQuxYsUKXLt2zerDLe/kunWfPj4+iIiIsOsYardq0aIFOnfujJUrV2LSpEkAzMXX3bp1K7en2s0uX75sObFudvN7Zq9hD0rf/+bNm5d7vM2bN6OwsBDe3t6W5YMGDUJYWBg2b95cpgrl3LlzOHr0aIU3i9LG56UaNWpk9Xfpheh2J39SUtJte8hdvnwZTZs2LXOeVnTu3RpL6Rc9Kiqq3OW3xtikSZMyVR3NmjUDYG4DEx4eXibGc+fOAQDGjRtX4evIy8u77QX6VraeQ7f7HGJjYzFjxgy8/fbbWLlyJXr16oUHHngAjzzyyG2rrwFzcn3zOQRYvzfdunXDuXPncOrUqSqfO7ey9VoTGxtr9XdGRgZyc3OxfPlyLF++vFox7Ny5E3PmzMHu3bvLJNp5eXlW79Wt7zlgft9vPq8q+hzL+76WJzs7G/PmzcPq1avLxF7VtqvlsfV7fut7fTvV/S5Wdj1r0aIF/vrrL6v1yrvfNG/evEbJ9c2SkpIgl8tvm5SeOHECs2bNwvbt28skbNX5nKpzXa/sGuDn51fp8SpKsEq5u7tjxIgRWLVqFbp06YIrV67goYceqnB9W75HN6vpNeRmCoUCs2bNwrhx47B+/fpyCysA4Msvv8SSJUtw+vRp6PV6y/Lyzvnqntel94nSRPhWpZ+PUqnEokWL8OyzzyIsLAzdunXD/fffj7Fjx5Z7/xFC2NyMr1oJZJcuXW7bW0qpVNbJUCJTp07FihUrMH36dHTv3h1qtRoymQyjR4+uVmlNbRk7diyefvppXL16FVqtFn///Tc++OADqcOqsWHDhuHLL7/EypUrrUqVAPOvsHvuuafCQUpLk4ZSFfUavN0FqTZUFEttxlh6vr755ptl2suUqqydk71U5TUuWbIE48ePx08//YTffvsN06ZNw8KFC/H333/ftt1YVZhMJrRp0wZvv/12uc/fepGtqVvbEZZ+Fo888kiFCX1pu+HyJCUloW/fvmjRogXefvttREVFwcPDAxs3bsQ777xT5tpUF+f+yJEjsWvXLjz//PNISEiAj48PTCYT+vfvX6Nrpa3f86q22SwlxXdRKrm5uejduzf8/Pwwf/58xMXFQaVS4eDBg3jxxRfr7J5W3fc2KCioSqVdDz30EJYtW4a5c+eiXbt2FSbVtn6Pbmbva8jDDz9saQtZ3lB933zzDcaPH48hQ4bg+eefR2hoKBQKBRYuXGjpJ3Kz6p7Xpa/566+/LjcRvLn0cvr06Rg0aBDWr1+PzZs345VXXsHChQuxffv2Mm0dc3NzLe3pq8ounWiqKjo6Glu3bkV+fr5VyUBpsfytDW9LM+2bnT17Fl5eXpZfFd9//z3GjRuHJUuWWNYpKSmpcIDlc+fO4a677rL8XVBQgJSUFAwYMKDar6tUZdn76NGjMWPGDHz77beWMbpGjRp1231GR0fjzJkzZZZX9J7VJM7SfVV0vODg4DIlR2+++Sbc3Nzw5JNPwtfX1+qXZFxcHAoKCirskWcvcXFxOH78eKXrREdH4+jRozCZTFY/bGryPlbm/PnzZX7RnT17FgAq7IgUFxcHwPwL8nbvmS2/FGvjHAKANm3aoE2bNpg1axZ27dqFO+64A8uWLcOrr75a6XbXr18vU+Jx63sTFxeHI0eOoG/fvtXq3GbrteZWISEh8PX1hdForNb5+8svv0Cr1eLnn3+2KmmwpdrsVtHR0eVeE8v7bG+Vk5ODbdu2Yd68eZg9e7ZleXn7q0hFn0Ndfc9tdfP17NbSmjNnzlieL/23uu9tVcXFxcFkMuHkyZMV/kD8/fffkZWVhXXr1uHOO++0LC8d4eRmVf1eVOe6Xl0tWrTADz/8cNv1evbsiUaNGuH333+vtGNOTb5HNb2G3Kq0FLL0h/Otvv/+ezRu3Bjr1q2zOt6cOXNqfOybld4nQkNDq/Sdi4uLw7PPPotnn30W586dQ0JCApYsWWI1oPu1a9eg0+ksNTRVVaejDQ8YMABGo7FMyds777wDmUyG++67z2r57t27raoPrly5gp9++gn33nuvJUtXKBRlfhW9//77VkMe3Gz58uVWRctLly6FwWAoc+zq8Pb2rjBxDQ4Oxn333YdvvvkGK1euRP/+/auU7Q8YMAB79+61GuqnsLAQy5cvR0xMTLXa6JT2TL811oiICCQkJODLL7+0eu748eP47bffyk2yZTIZli9fjuHDh2PcuHH4+eefLc+NHDkSu3fvxubNm8tsl5ubC4PBYHPs5Rk2bBiOHDlS7jALpefGgAEDkJqaijVr1lieMxgMeP/99+Hj42MZisNerl+/bhWPRqPBV199hYSEhHJ/NQLmnnBxcXF46623UFBQUOb5jIwMy/9LL/hVmYnG3ueQRqMp89m1adMGcrm8SsNjGAwGfPzxx5a/dTodPv74Y4SEhFjaPI8cORLXrl3DJ598Umb74uLi244ra+u15lYKhQLDhg3DDz/8UO6Pk5s/i4q2B1CmSc2KFSsq3a4yAwYMwN9//429e/daxbFy5crbbltePABsmsmkoutbXX3PbdWpUyeEhoZi2bJlVuflf//7X5w6dQoDBw4EYH3du7mKeMuWLXZtYz5kyBDI5XLMnz+/TMlZ6edS3uek0+nw0Ucfldmft7d3laq0q3Ndr67u3bsjJyfntu3WZTIZ3nvvPcyZMwePPvpohevV5HtU02tIeR555BE0adLEMhHH7WLds2fPbYfps1ViYiL8/Pzw+uuvW+UypUqvTUVFRWXGs42Li4Ovr2+Z63Rp2/MePXrYFEudlkAOGjQId911F15++WVcunQJ7dq1w2+//YaffvoJ06dPt2TWpeLj45GYmGg1jA8Aqw/v/vvvx9dffw21Wo1WrVph9+7d2Lp1q9WQQjfT6XTo27cvRo4ciTNnzuCjjz5Cz5498cADD9T49XXs2BFbt27F22+/jcjISMTGxlq1WRo7diyGDx8OAFiwYEGV9vnSSy/h22+/xX333Ydp06YhMDAQX375JS5evIgffvihWs0EPD090apVK6xZswbNmjVDYGAg4uPjER8fjzfffBP33XcfunfvjkmTJlmGe1Cr1WXmfy0ll8vxzTffYMiQIRg5ciQ2btyIu+++G88//zx+/vln3H///ZYhSQoLC3Hs2DF8//33uHTpks1F5uV5/vnn8f3332PEiBGYOHEiOnbsiOzsbPz8889YtmwZ2rVrh8mTJ+Pjjz/G+PHjceDAAcTExOD777/Hzp078e6775ZpK1dTzZo1w6RJk7Bv3z6EhYXh888/R1paWqUXPrlcjk8//RT33XcfWrdujQkTJqBBgwa4du0aduzYAT8/P/zyyy8Abgy78PLLL2P06NFwd3fHoEGDyi1JsPc5tH37dkyZMgUjRoxAs2bNYDAY8PXXX1uSrtuJjIzEokWLcOnSJTRr1gxr1qzB4cOHsXz5cksHt0cffRTfffcd/vWvf2HHjh244447YDQacfr0aXz33XfYvHlzpc1obL3WlOeNN97Ajh070LVrVzz++ONo1aoVsrOzcfDgQWzdurXcsdlK3XvvvfDw8MCgQYPwxBNPoKCgAJ988glCQ0ORkpJy22OX54UXXrBMUfr0009bhvEpLV2vjJ+fH+68804sXrwYer0eDRo0wG+//VZuyVZFOnbsiKVLl+LVV19FkyZNEBoaWqffc1u5u7tj0aJFmDBhAnr37o0xY8ZYhvGJiYmxmvJy4cKFGDhwIHr27ImJEyciOzsb77//Plq3bl3uj7nqaNKkCV5++WUsWLAAvXr1woMPPgilUol9+/YhMjISCxcuRI8ePRAQEIBx48Zh2rRpkMlk+Prrr8utOu7YsSPWrFmDGTNmoHPnzvDx8cGgQYPKPXZ1ruvVMXDgQLi5uWHr1q2W4ewqMnjwYAwePLjSdWryParpNaQ8CoUCL7/8MiZMmFDmufvvvx/r1q3D0KFDMXDgQFy8eBHLli1Dq1at7HYOAebv8tKlS/Hoo4+iQ4cOGD16NEJCQpCcnIxff/0Vd9xxBz744AOcPXvWkuu0atUKbm5u+PHHH5GWlobRo0db7XPLli1o1KiRbUP4ANUbxmffvn2Vrndz9/db5efni2eeeUZERkYKd3d30bRpU/Hmm29aDWMgxI0u8t98841o2rSpUCqVon379pYhT0rl5OSICRMmiODgYOHj4yMSExPF6dOnywxlUhr7H3/8ISZPniwCAgKEj4+PePjhh62GNhCi+sP4nD59Wtx5553C09NTACgzpI9WqxUBAQFCrVZbDStxO0lJSWL48OHC399fqFQq0aVLF7Fhw4Yy6wFVG8ZHCCF27dolOnbsKDw8PMoMS7F161Zxxx13CE9PT+Hn5ycGDRokTp48abX9zcP4lCoqKhK9e/cWPj4+4u+//xZCmD/vmTNniiZNmggPDw8RHBwsevToId566y3LkC2l7295ww3cGltFsrKyxJQpU0SDBg2Eh4eHaNiwoRg3bpzVECxpaWmWc8XDw0O0adPG6jOtLJaKhuoo7zsRHR0tBg4cKDZv3izatm0rlEqlaNGiRZltbx3Gp9ShQ4fEgw8+KIKCgoRSqRTR0dFi5MiRYtu2bVbrLViwQDRo0EDI5XKr4UtuPfeFqNo5VNFrvPX8v3Dhgpg4caKIi4sTKpVKBAYGirvuukts3bpV3E7pMB379+8X3bt3FyqVSkRHR4sPPvigzLo6nU4sWrRItG7dWiiVShEQECA6duwo5s2bJ/Ly8srs81a2XmvKk5aWJp566ikRFRUl3N3dRXh4uOjbt69Yvnz5bV/rzz//LNq2bStUKpWIiYkRixYtEp9//nmZIXdKz5fy3qtbh805evSo6N27t1CpVKJBgwZiwYIF4rPPPqvSMD5Xr14VQ4cOFf7+/kKtVosRI0aI69evV3lYmtTUVDFw4EDh6+srAFjFVtPveUXK+2xs/Y6uWbNGtG/fXiiVShEYGCgefvhhy5BzN/vhhx9Ey5YthVKpFK1atRLr1q0T48aNs9swPqU+//xzSzwBAQGid+/eYsuWLZbnd+7cKbp16yY8PT1FZGSkeOGFF8TmzZvLXCsKCgrEQw89JPz9/a2GGyrvfiVE9a/rQty4zt3uHBNCiAceeED07dvXallF79GtyvsuV/V7VN73parXkPJUlMfo9XoRFxdX5tw0mUzi9ddfF9HR0ZZ8ZcOGDWXOIXvcY0rXT0xMFGq1WqhUKhEXFyfGjx9vGfowMzNTPPXUU6JFixbC29tbqNVq0bVrV/Hdd99Z7cdoNIqIiAgxa9asSt+P8siEcOIWx07GYDAgMjISgwYNwmeffSZ1OFRLYmJiEB8fjw0bNkgdChFRnfrf//6HPn364PTp0+X2bCfHsn79ejz00ENISkpCRESETdvWaRvI+m79+vXIyMiodKYYIiIiZ9WrVy/ce++9WLx4sdShUBUsWrQIU6ZMsTl5BACWQNaBPXv24OjRo1iwYAGCg4PtNq4YOSaWQBIRkatjCWQdKJ17OjQ0FF999ZXU4RARERHVCEsgiYiIiMgmLIEkIiIiIpswgSQiIiIim9TpQOKuxmQy4fr16/D19bXLVElERERU+4QQyM/PR2RkZLUm5CAmkDVy/fp1mydkJyIiIsdw5coVNGzYUOownBITyBoonf7uypUr8PPzkzgaIiIiqgqNRoOoqCi7T2NbnzCBrIHSams/Pz8mkERERE6Gzc+qjxX/RERERGQTJpBEREREZBMmkERERERkE7aBJCIisiMhBAwGA4xGo9Sh1FsKhQJubm5s41iLmEASERHZiU6nQ0pKCoqKiqQOpd7z8vJCREQEPDw8pA7FJTGBJCIisgOTyYSLFy9CoVAgMjISHh4eLAGTgBACOp0OGRkZuHjxIpo2bcrBwmsBE0giIiI70Ol0MJlMiIqKgpeXl9Th1Guenp5wd3fH5cuXodPpoFKppA7J5TAlJyIisiOWdjkGfg61i+8uEREREdmECSQRERER2YRtIImIiGrTL7/U7fEGDarb49WSmJgYTJ8+HdOnT5c6FCoHE0iySZHOIHUITsPLg18vInIO48ePx5dffllm+blz59CkSRMJIiJHxzsc2aTV7M1Sh+A0Lr0xUOoQiIiqrH///lixYoXVspCQEImiIUfHNpBEREQEpVKJ8PBwq4dCocBPP/2EDh06QKVSoXHjxpg3bx4Mhhu1UTKZDB9//DHuv/9+eHl5oWXLlti9ezfOnz+PPn36wNvbGz169EBSUpJlm6SkJAwePBhhYWHw8fFB586dsXXr1krjy83NxWOPPYaQkBD4+fnh7rvvxpEjR2rt/aDKsQSSbHJyfmKtH+NUigapuSVVWldrMGLG2qMAgLdHtIXSTVGl7brGBbKKmYjoNv73v/9h7NixeO+999CrVy8kJSVh8uTJAIA5c+ZY1luwYAHefvttvP3223jxxRfx0EMPoXHjxpg5cyYaNWqEiRMnYsqUKfjvf/8LACgoKMCAAQPw2muvQalU4quvvsKgQYNw5swZNGrUqNxYRowYAU9PT/z3v/+FWq3Gxx9/jL59++Ls2bMIDAys/TeDrPAOSjapi6SrSGeE0r1qieDNlG6KKm9XrDMh2IenPxFRqQ0bNsDHx8fy93333YecnBy89NJLGDduHACgcePGWLBgAV544QWrBHLChAkYOXIkAODFF19E9+7d8corryAx0Vzo8PTTT2PChAmW9du1a4d27dpZ/l6wYAF+/PFH/Pzzz5gyZUqZ2P766y/s3bsX6enpUCqVAIC33noL69evx/fff29Jaqnu8A5KDiW/RA+t3lTrx8ku1CEqkDNFEBGVuuuuu7B06VLL397e3mjbti127tyJ1157zbLcaDSipKQERUVFlhl32rZta3k+LCwMANCmTRurZSUlJdBoNPDz80NBQQHmzp2LX3/9FSkpKTAYDCguLkZycnK5sR05cgQFBQUICgqyWl5cXGxVNU51hwkkOZSsAl2dHCe7UAeTSUAu5zy1RESAOWG8tcd1QUEB5s2bhwcffLDM+jdPD+ju7m75f+n83+UtM5nMBQTPPfcctmzZgrfeegtNmjSBp6cnhg8fDp2u/HtAQUEBIiIi8Pvvv5d5zt/fv2ovkOyKCSQ5lKzCukkgjSaB3GI9Ar096uR4RETOqEOHDjhz5ozdh/LZuXMnxo8fj6FDhwIwJ4iXLl2qNI7U1FS4ubkhJibGrrFQ9TCBJIdhMJqQV1w3CSQAZBVomUASEVVi9uzZuP/++9GoUSMMHz4ccrkcR44cwfHjx/Hqq69We79NmzbFunXrMGjQIMhkMrzyyiuW0sny9OvXD927d8eQIUOwePFiNGvWDNevX8evv/6KoUOHolOnTtWOhaqHCSQ5jOwiHSq5fthdZoEOTcPq7nhEVE858cwwiYmJ2LBhA+bPn49FixbB3d0dLVq0wGOPPVaj/b799tuYOHEievTogeDgYLz44ovQaDQVri+TybBx40a8/PLLmDBhAjIyMhAeHo4777zT0uaS6pZMCCGkDsJZaTQaqNVq5OXlwc/PT+pwnN6pFA2u5RTbtI1Wb8RT3x4CAHw4pr3Nvbd7Ng2Gqho9vomIblVSUoKLFy8iNjbWqn0gSaOyz4P375rjQOLkMOqqA43VMeuozSUREZErYQJJDqFAa0CJ3ljnx83M19b5MYmIiJwdE0hyCFIlcqXD+RAREVHVMYEkh5BZIE0CaTQJ5BSxGpuIiMgWTCBJcnqjCXnFesmOnylB20siIiJnxgSSJJdZoIWUYwFksB0kERGRTZhAkuSkTuBK9Ebkl0hXAkpERORsmECSpIwmIcnwPbdKZykkERFRlTGBJEllFWphdIBe0OkaJpBERERVxakMSVKOkrgVag0o1BrgreRXgojsa+vJtDo9Xr9W0k7td+nSJcTGxuLQoUNISEiQNBaqPSyBJMmYTAIZEg3fU540TYnUIRARSWL8+PGQyWT417/+Vea5p556CjKZDOPHj6/7wMhhMYEkyWQWamE0Sl99XSrNQUpDiYikEBUVhdWrV6O4uNiyrKSkBKtWrUKjRo0kjIwcERNIkkxqnmOV+BVqDeyNTUT1VocOHRAVFYV169ZZlq1btw6NGjVC+/btLcs2bdqEnj17wt/fH0FBQbj//vuRlJRU6b6PHz+O++67Dz4+PggLC8Ojjz6KzMzMWnstVPuYQJIk9EaTZLPPVMbRkloioro0ceJErFixwvL3559/jgkTJlitU1hYiBkzZmD//v3Ytm0b5HI5hg4dCpPJVO4+c3Nzcffdd6N9+/bYv38/Nm3ahLS0NIwcObJWXwvVLvYYIEmk5pWggmuNpFLyShAX4gO5XCZ1KEREde6RRx7BzJkzcfnyZQDAzp07sXr1avz++++WdYYNG2a1zeeff46QkBCcPHkS8fHxZfb5wQcfoH379nj99dettomKisLZs2fRrFmz2nkxVKuYQJIkUhy0pE9nMCGzUItQX5XUoRAR1bmQkBAMHDgQX3zxBYQQGDhwIIKDg63WOXfuHGbPno09e/YgMzPTUvKYnJxcbgJ55MgR7NixAz4+PmWeS0pKYgLppJhAUp3TlOihkXDu69u5llPMBJKI6q2JEydiypQpAIAPP/ywzPODBg1CdHQ0PvnkE0RGRsJkMiE+Ph46XfmTQhQUFGDQoEFYtGhRmeciIiLsGzzVGSaQVOeuZhfffiUJZRXoUKwzwtNDIXUoRER1rn///tDpdJDJZEhMTLR6LisrC2fOnMEnn3yCXr16AQD++uuvSvfXoUMH/PDDD4iJiYGbG9MOV8FONFSn9EaTU4y3eDWnSOoQiIgkoVAocOrUKZw8eRIKhfUP6YCAAAQFBWH58uU4f/48tm/fjhkzZlS6v6eeegrZ2dkYM2YM9u3bh6SkJGzevBkTJkyA0WiszZdCtYg/BahOXc8tdoipC2/nWm4xYoO94abgbywiqhmpZ4apDj8/v3KXy+VyrF69GtOmTUN8fDyaN2+O9957D3369KlwX5GRkdi5cydefPFF3HvvvdBqtYiOjkb//v0hl/Ma66yYQFKdMZkEkrOdo2TPYBRIyStBVKCX1KEQEdW6L774otLn169fb/l/v379cPLkSavnhbhRMBATE2P1NwA0bdrUanxJcn5M/anOpGpKoNU74Ng9FbicVQSTE5SWEhER1TUmkFQnhBC4lFkodRg2KdEbkeoE7TWJiIjqGhNIqhOpmhIU6ZyvsfTFzEKWQhIREd2CCSTVOpNJ4EKGc5U+lirWGXE9z7GHHSIiIqprTCCp1l3LLUaxE5Y+lrqYWegUPceJyDHc2oGEpMHPoXYxgaRaZTCacMHJ2j7eSqs3OU3vcSKSjru7OwCgqIjXC0dQ+jmUfi5kXxzGh2rVxcxC6A3O0/O6IpcyCxGhVkHlztlpiKh8CoUC/v7+SE9PBwB4eXlBJpNJHFX9I4RAUVER0tPT4e/vX2YwdLIPh0kgP/zwQ7z55ptITU1Fu3bt8P7776NLly4Vrr927Vq88soruHTpEpo2bYpFixZhwIABAAC9Xo9Zs2Zh48aNuHDhAtRqNfr164c33ngDkZGRln1kZ2dj6tSp+OWXXyCXyzFs2DD83//9X7kTvpPtCrUGXHGRGV2MJoHz6QWIb6CWOhQicmDh4eEAYEkiSTr+/v6Wz4PszyESyDVr1mDGjBlYtmwZunbtinfffReJiYk4c+YMQkNDy6y/a9cujBkzBgsXLsT999+PVatWYciQITh48CDi4+NRVFSEgwcP4pVXXkG7du2Qk5ODp59+Gg888AD2799v2c/DDz+MlJQUbNmyBXq9HhMmTMDkyZOxatWqunz5Lut0aj5Mzl/4aJGaV4JIf08EentIHQoROSiZTIaIiAiEhoZCr9dLHU695e7uzpLHWiYTDtDKtGvXrujcuTM++OADAIDJZEJUVBSmTp2Kl156qcz6o0aNQmFhITZs2GBZ1q1bNyQkJGDZsmXlHmPfvn3o0qULLl++jEaNGuHUqVNo1aoV9u3bh06dOgEANm3ahAEDBuDq1atWJZUV0Wg0UKvVyMvLq3Dap/rqem4xTl7X1PpxtHojnvr2EADgwzHtoazlKmYvDwW6Ng6CQs5qKSIiZ8X7d81J3olGp9PhwIED6Nevn2WZXC5Hv379sHv37nK32b17t9X6AJCYmFjh+gCQl5cHmUwGf39/yz78/f0tySNgnp5JLpdjz5495e5Dq9VCo9FYPaisEr0RZ9PypQ6jVhTpjLiQUSB1GERERJKSPIHMzMyE0WhEWJj1ZPNhYWFITU0td5vU1FSb1i8pKcGLL76IMWPGWH5ppKamlqked3NzQ2BgYIX7WbhwIdRqteURFRVVpddY35xOzYfBKHnBdq1Jzi5CbpFO6jCIiIgkI3kCWdv0ej1GjhwJIQSWLl1ao33NnDkTeXl5lseVK1fsFKXruJZbjMx8rdRh1CohgJPXNTAYXaiBJxERkQ0k70QTHBwMhUKBtLQ0q+VpaWkV9p4KDw+v0vqlyePly5exfft2q3YO4eHhZXrJGQwGZGdnV3hcpVIJpVJZ5ddW3xRqDTib6ppV17cq0hlxJi0frSPZK5uIiOofyUsgPTw80LFjR2zbts2yzGQyYdu2bejevXu523Tv3t1qfQDYsmWL1fqlyeO5c+ewdetWBAUFldlHbm4uDhw4YFm2fft2mEwmdO3a1R4vrV4xmQSOXcurVzO2pOSWIDWvROowiIiI6pzkJZAAMGPGDIwbNw6dOnVCly5d8O6776KwsBATJkwAAIwdOxYNGjTAwoULAQBPP/00evfujSVLlmDgwIFYvXo19u/fj+XLlwMwJ4/Dhw/HwYMHsWHDBhiNRku7xsDAQHh4eKBly5bo378/Hn/8cSxbtgx6vR5TpkzB6NGjq9QDm6ydSctHQYlB6jDq3KkUDXxVbvBWOsRXiYiIqE44xF1v1KhRyMjIwOzZs5GamoqEhARs2rTJ0lEmOTkZcvmNwtIePXpg1apVmDVrFv7zn/+gadOmWL9+PeLj4wEA165dw88//wwASEhIsDrWjh070KdPHwDAypUrMWXKFPTt29cykPh7771X+y/YxaTkFeNaTrHUYUjCaBI4cjUXXWIC4aaQvECfiIioTjjEOJDOiuNIAZoSPfZfypZswPC6HgeyIqF+SrRpoOa0ZUREToD375pjkQlVm9ZgxJEruS4120x1pWu0uJhZKHUYREREdYIJJFWL0SRw9GoetHpmj6UuZBQiTcNONURE5PqYQJLNhBA4eV2DvCLO83qrE9fz+L4QEZHLYwJJNjufXsCStgqYTMDhq7ko0tW/HulERFR/MIEkmyRnFeFyVpHUYTg0vcGEQ8m50BqMUodCRERUK5hAUpWl5BXjbFr9mGmmpop1RhxKzoWe0x0SEZELYgJJVZKRr8XJ6xqpw3AqBSUGHLmSW69m5yEiovqBCSTdVnahDseu5YIjhtout0iPo1dzYWISSURELoQJJFUqt0jHsR5rKKtAh2PX8phEEhGRy2ACSRXKK9bjEKtg7SIjX4sT1zXgxE9EROQKmEBSufKK9TiUnAOjkQmPvaRpSphEEhGRS2ACSWWUJo8GJo92l5rHJJKIiJyfm9QBkGPJK9Lj4BWWPNam1DzzIOytI/0gk8kkjoaIiMh2TCDJIrdIZ27zyOSx1qXmlUAIcxIplzOJJCIi58IqbAIA5BTqcCiZyWNdStOUsHc2ERE5JSaQhKwCLQ6zt7UkMvK1OMJxIomIyMkwgaznShMYJo/SySrQcbgkIiJyKkwg67F0TQmOXeMg4Y7A3IQgBwbOnU1ERE6ACWQ9lZpX2v5O6kioVG6RHgeTc6FnEklERA6OCWQ9dC23GMev5XFuawekKdbjwOUc6AxMIomIyHExgaxnrmQX4dR1jdRhUCUKSgw4cDkHJXqj1KEQERGViwlkPZKcVYQzqflSh0FVUKg14CCTSCIiclBMIOuJS5mFOJvG5NGZFOmMLIkkIiKHxASyHriYWYjz6QVSh0HVUPxPElmsYxJJRESOgwmki7uYWYgkJo9OjUkkERE5GiaQLozJo+so0RtxMJnV2URE5BiYQLqoy1lMHl1NMdtEEhGRg2AC6YKuZBfhXBqTR1dUrDOXRGoNTCKJiEg6TCBdTEpeMYfqcXFFWiMOccYaIiKSEBNIF5KRr8VJDhJeLxSUGHDkSi6MJk4nREREdY8JpIvIK9Lj2LVcTk9Yj+QW6f+ZkpIfOhER1S03qQOgmivSGXD4ai5MrNGsdzLytTibVoDm4b5Sh0LkkIp0BqlDcBpeHkwJqOp4tjg5vdGEw8m50BuYPdZXV7KL4OWhQFSgl9ShEDmcVrM3Sx2C07j0xkCpQyAnwipsJyaEwLFreSjiANP13tm0fGQVaKUOg4iI6gmWQDqxpIwCZBfopA6DHIAQwLFreegaGwRPD4XU4RA5jJPzE6U5cNIF4MwZq0VFRqDTPnO5zf7OJniV91W94w5A7VcHARLVDBNIJ5WeX4JLmUVSh0EOxGAUOHo1F51jAiGXy6QOh8ghSNauz10OVPJbzkuB8hNIDwXAtojkBFiF7YRK9EYO10Plyi8x4HwGB5EnIqLaxQTSyQghcOJ6HgxGDt1C5UvOKmJ7SCKpyapZC1Dd7YjqGBNIJ3M1pxg5hXqpwyAHdzJFAwNnqiGSjryat1cF2zCTc2AC6URK9EacT2f1JN2eVm9iVTaRlKqbCFY38SSqYzxTnci5tAJOXUdVdjW7GJoSllYTSaK6CSRLIMlJMIF0EjmFOqRpSqQOg5zMubR8qUMgqp+YQJKLYwLpJJJYHUnVkFOoRyY71BDVPSaQ5OKYQDqBrAItcotYFUnVcyGjUOoQiOoft2qM5VidbYgkwgTSCVzO5oDhVH2aYj1yizhjEVGdqk5JIksfyYkwgXRwhVoDpyukGkvmjxCiusUEklwcE0gHl5JXLHUI5AIyC7TQGTguJFGdqc5wPBzCh5wIz1YHl5rHDhBUcyaTef50Iqoj1ZlRhrPQkBNhAunANCV6lOiNUodBLiIjnz9GiOoME0hycUwgHRjbPpI95RbpYeJA9EREZAdMIB1YDnvOkh0ZTYIz0xDVFVM12hwbWeNEzoMJpAPLLzFIHQK5GJ5TRHWkOskgE0hyIkwgHZTOYGKvWbK7Ai0TSKI6oa9Gab+B309yHkwgHVQxO89QLeB5RVRHdNVogmQwsBSSnAYTSAelNfAiQvbHUm2iOlJSzWGzqrsdUR1jAumgjOwtS7XAYOR5RVQnCqs5B30RZ40i58AE0kExf6TaYBI8sYjqREFB3W5HVMeYQDooDidLtYHjFBPVEY2mbrcjqmNMIB2UnHd6qgUKnldEta+kBCgurt62OTn2jYWoljCBdFDuCt7oyf7cFPzKE9W67Ozqb5ufX70e3ER1jHcTB6V0V0gdArkgpRu/8kS1LiOjZttnZtonDqJaxLuJg/JkAkm1wNOD5xVRrRICSEur2T5SU+0TC1EtYgLpoBRyGW/2ZHfeSjepQyBybdnZgFZbs32kpVVvLm2iOsQE0oH58GZPduar4jlFVKuuXq35PgwGlkKSw2MC6cD8vdylDoFciEIugy9/lBDVHoMBuH7dPvtKTrbPfohqicMkkB9++CFiYmKgUqnQtWtX7N27t9L1165dixYtWkClUqFNmzbYuHGj1fPr1q3Dvffei6CgIMhkMhw+fLjMPvr06QOZTGb1+Ne//mXPl1Uj/l4eUodALkTt5Q4Zh/Ehqj1XrpiTSHvIyOCg4uTQHCKBXLNmDWbMmIE5c+bg4MGDaNeuHRITE5Genl7u+rt27cKYMWMwadIkHDp0CEOGDMGQIUNw/PhxyzqFhYXo2bMnFi1aVOmxH3/8caSkpFgeixcvtutrqwk/lRvc2WuW7CTYWyl1CESuSwjgwgX77jMpyb77I7Ijh8hO3n77bTz++OOYMGECWrVqhWXLlsHLywuff/55uev/3//9H/r374/nn38eLVu2xIIFC9ChQwd88MEHlnUeffRRzJ49G/369av02F5eXggPD7c8/Pz87PraakImkyHYh6WQZB/BvjyXiGrN9ev2n8f66lXzoOREDkjyBFKn0+HAgQNWiZ5cLke/fv2we/fucrfZvXt3mcQwMTGxwvUrs3LlSgQHByM+Ph4zZ85EUSUXAK1WC41GY/WobWF+qlo/Brk+X5UbvDzY/pGoVggBnDlj//2aTMDZs/bfL5EdSH5HyczMhNFoRFhYmNXysLAwnD59utxtUlNTy10/1cZeaw899BCio6MRGRmJo0eP4sUXX8SZM2ewbt26ctdfuHAh5s2bZ9MxairQywPubnLoDRzSgaovXM0fIkS1JjkZKCysvX3HxQHe3rWzf6JqkjyBlNLkyZMt/2/Tpg0iIiLQt29fJCUlIS4ursz6M2fOxIwZMyx/azQaREVF1WqMcrkMEWoVkrPsXDVC9YZMxgSSqNYYDLVT+lhKCODkSaBz59o7BlE1SF6FHRwcDIVCgbRbRu5PS0tDeHh4uduEh4fbtH5Vde3aFQBw/vz5cp9XKpXw8/OzetSFhgGedXIcck2hvioo3TgoPVGtOHOm5gOH305qKlBBp1IiqUieQHp4eKBjx47Ytm2bZZnJZMK2bdvQvXv3crfp3r271foAsGXLlgrXr6rSoX4iIiJqtB978/JwQxA701A1NQr0kjoEItek0QAXL9bNsY4dA4zGujkWURU4RBX2jBkzMG7cOHTq1AldunTBu+++i8LCQkyYMAEAMHbsWDRo0AALFy4EADz99NPo3bs3lixZgoEDB2L16tXYv38/li9fbtlndnY2kpOTcf2fQV3P/FPFUNrbOikpCatWrcKAAQMQFBSEo0eP4plnnsGdd96Jtm3b1vE7cHsxQd7IKtBJHQY5GX8vd6g5ID2R/QkBHD5s/rcuFBWZSztbtaqb4xHdhkMkkKNGjUJGRgZmz56N1NRUJCQkYNOmTZaOMsnJyZDLbxSW9ujRA6tWrcKsWbPwn//8B02bNsX69esRHx9vWefnn3+2JKAAMHr0aADAnDlzMHfuXHh4eGDr1q2WZDUqKgrDhg3DrFmz6uhV2ybA2wP+Xu7ILdJLHQo5kZhgNrwnqhXnzwN5eXV7zAsXgIgIICCgbo9LVA6ZEHX188n1aDQaqNVq5OXl1Ul7yOxCHQ5ezqn14zgTrd6Ip749BAD4cEx7KN3Z1q+Uv5c7OsUESh0GkevJywP++ss8zI4NioxAq7/NhSEnu5ngVZ3Llbc3cOedgJtDlP84rbq+f7siydtAUtUFensgkG0hqYriQnykDoHI9RgMwIEDNiePdlNYCJw4Ic2xiW7CBNLJNA31AaczptsJ8VUiwJs/Nojs7tix2hvzsaqSk4Fr16SNgeo9JpBOxlfljkh/DutDFZPLgaZhLH0ksrvkZPP0go7gyBEgP1/qKKgeYwLphJqE+sDdjR8dlS86yJvTFhLZW06OufTRURiNwL59gJ4dK0kazEKckLtCjmYsYaJyeHkoEBPEntdEdlVSAuzfL127x4oUFprbY7IvLEmACaSTilB7cnBxKqNlhB8UcjaSJbIbgwHYs8ecRDqijAzg+HGpo6B6iAmkE2sZ4Qc3BZMFMosK9GLHGSJ7EsJcwqfRSB1J5S5dApKSpI6C6hkmkE5M5a5A83BfqcMgB+ClVKBJKJs1ENnV0aPOMwf1yZPsmU11igmkk4tQeyJcrZI6DJKQXA7EN1Cz6prInk6cMPe6diaHDgFpaVJHQfUEE0gX0DzcF54enIGlvmoS4gs/Fee7JrKbM2fM0wY6GyHMnX0yMqSOhOoBJpAuwF0hR5uGasj5adY7Ib5KNArykjoMItdx9qz54axMJmDvXiaRVOtqlHLs2LHDXnFQDfmp3NEsjO0h6xMvDwVaRXIOVyK7OXXKXPro7EqTSGdpv0lOqUYJZP/+/REXF4dXX30VV65csVdMVE0NA7w4S009oZDL0KahGu4KFjsT1ZgQ5kHCz5+XOhL7MZnMA41fvy51JOSianT3uXbtGqZMmYLvv/8ejRs3RmJiIr777jvodDp7xUc2ahHuCz9Ptodzda0i/eDLdo9ENWcyAQcPmofCcTUmk3kYIld8bSS5GiWQwcHBeOaZZ3D48GHs2bMHzZo1w5NPPonIyEhMmzYNR44csVecVEVyuQxtG6qhdGfJlKuKDfFGmB973hPVmF4P/P2365fSHTtmrp7njDVkR3bLMjp06ICZM2diypQpKCgowOeff46OHTuiV69eOHHihL0OQ1WgclegXZQ/h3VxQaF+SjQO5lSFRDVWWAj89ReQlSV1JHXj/HlzaaTRKHUk5CJqnEDq9Xp8//33GDBgAKKjo7F582Z88MEHSEtLw/nz5xEdHY0RI0bYI1aygZ/KHa0bsIOFK/HzdEfrSDVkMv4wIKqRzEzgf/8DCgqkjqRupaQAO3c67rSM5FTcarLx1KlT8e2330IIgUcffRSLFy9GfHy85Xlvb2+89dZbiIyMrHGgZLtQXxWahZlwNi1f6lCohjw9FGgXxcHCiWrswgXzrC31tTo3Lw/480+gY0cgKEjqaMiJ1SiBPHnyJN5//308+OCDUCqV5a4THBzM4X4k1CjIC8V6I65kF0kdClWTm0KGhCh/KN04WDxRtRkM5qkJOd0foNUCu3cDrVoBjRtLHQ05qRpVYc+ZMwcjRowokzwaDAb8+eefAAA3Nzf07t27JoehGmoW5oNQv/ITfHJscjmQEOUPb2WNfusR1W8ajbnKmsnjDUKYp2vct8/cmYjIRjVKIO+66y5kZ2eXWZ6Xl4e77rqrJrsmO5LJZIiPVCPAm8O+OBOZzDzHtb+Xh9ShEDmvy5frZ3vHqkpNNVdp5+RIHQk5mRolkEKIchv0Z2VlwdubPUUdiXl4H3/4qFiS5SxaRPgh1JfD9RBVi05nLl07etQ8HiJVrKjI3Lnm3Ln62zaUbFatbOLBBx8EYC7ZGj9+vFUVttFoxNGjR9GjRw/7REh2466Qo30jf+y/lINiHYdycGRxoT5owFmFiKonIwM4fJi9jW0hBHD6tHn6w/btAS8vqSMiB1etBFKtVgMwl0D6+vrC0/PGjc7DwwPdunXD448/bp8Iya6UbgpLEqkz8Fe5I2oU5IVYjvVIZDuj0dzDmjOvVF92NvDHH+YONtHRUkdDDqxaCeSKFSsAADExMXjuuedYXe1kvDzc0L6RPw5czoHByOoKRxKuVqFpqI/UYRA5n+xsc6ljYaHUkTi/0h7rKSlAQgKgYlMaKqvGvbCZPDonX5U7EjhbjUMJ8VWidaQfBwonsoXRCBw/bm7Dx+TRvjIygB07gORkqSMhB2RzCWSHDh2wbds2BAQEoH379pXe7A4ePFij4Kh2+Xt5oE1DNY5ezWUbc4kFeLujTQPOMkNkk8xM4MgRcycQqh0Gg/k9vn4daNuWbSPJwuYEcvDgwZZOM0OGDLF3PFTHgn2UaB2pxrGreVKHUm/5ebqjXUN/yFkaTFQ1er25rSNLxupORgbw++9Ay5ZATIx5nDGq12xOIOfMmVPu/8l5hfmpoI8w4XQKpzysa15KBRKi/OGmqPG09ET1Q2oqcOwYe1hLobS5wLVrQLt2gK+v1BGRhDgoIAEAGgZ4wWAUOJ/OwXbrispdgQ6NAuDhxuSR6LZKSszJS0qK1JFQTo558PEmTYCmTc1TZlG9Y3MCGRAQUOV2WuXNUkOOKybYG3qjCZez2J6otrm7ydEh2h8qd85vTVQpIYArV8xV1pxyz3GYTMDZs+a2ke3aAYGBUkdEdczmBPLdd9+thTDIUTQN84XOaEJKLquHaotCIUP7Rv7w8mAFAFGlCgvNHTiysqSOhCpSUGDuAR8TY24f6cbrWn1h8yc9bty42oiDHEirCD/ojQKZ+VqpQ3E5cjnQrqE//FScl5yoQkIASUnAmTOchtBZXLoEpKWZe2qHhkodDdUBmxNIjUYDPz8/y/8rU7oeOReZTIY2DdQ4lJyD3CJWGdlT60g1Ar09pA6DyHFpNOYBwfM4MoTTKS4G9uwBGjYE4uMBd/5QdmXVagOZkpKC0NBQ+Pv7l9seUggBmUwGo5HzLTsrhVyGdlH+2HcpG0Vafo720DzcF2F+nNGBqFwmE3D+PHDuHEsdnd3Vq+Zhf9q2BcLDpY6GaonNCeT27dsR+E9j2R07dtg9IHIc7go5OjQKwL5L2dDqeUGviZhgL0QFcgBeonLl55tLHXNzpY6E7EWrBfbtA6KigNatWRrpgmxOIHv37l3u/8k1qdzN4xTuv5wDI+fNrpZwtQpNQjleGlEZQgAXLwKnTrHU0VVduWKeMah9eyAoSOpoyI5q3F0qJycHn332GU6dOgUAaNWqFSZMmGAppSTn56tyR9sGahy+kgvBHNImAd7uaBXBtsBEZWi1wKFD5qpOcm3FxcCuXeZxI5s357iRLqJGn+Kff/6JmJgYvPfee8jJyUFOTg7ee+89xMbG4s8//7RXjOQAgnyUaB7OUjRbeHko0JZTFBKVlZkJ/PEHk8f65vx5cyJZXCx1JGQHNSqBfOqppzBq1CgsXboUCoV5QGSj0Ygnn3wSTz31FI4dO2aXIMkxNAzwQpHOiGQONH5bbgoZEhr5w51TFBJZO3fOPDwPqzPqp5wc84+Hjh2BkBCpo6EaqNHd7fz583j22WctySMAKBQKzJgxA+fPn69xcOR4mob6IMiHw9BURiYD2jbkQOFEVoxG4MAB4PRpJo/1nV5vHu7nwgWpI6EaqFEC2aFDB0vbx5udOnUK7dq1q8muyUGVjhHppeQUfBVpFubLsR6JbqbVmqsur1+XOhJyFEIAJ04AR4/yB4WTsrmI5OjRo5b/T5s2DU8//TTOnz+Pbt26AQD+/vtvfPjhh3jjjTfsFyU5FDeFHAlR/th7MRsG9sy20iDAk8P1EN2sqAjYvdv8L9GtLl82/8Do2JGda5yMTAjbUn+5XA6ZTIbbbVYfBhLXaDRQq9XIy8url7PuZORrceRKrqQxaPVGPPXtIQDAh2PaQ+kuXcmo2ssdHRsFsNMMUanCQnPyyE4TAIAiI9Dqb3OSdLKbCV6syLkhNBTo3LnOksj6fv+2B5tLIC9evFgbcZATCvFVIjbEGxczCqUORXIebnK0aaBm8khUqqSEySNVXXq6uY1sp07mhuTk8GxOIKOjo2sjDnJSjYO9kVesR3aBTupQJBXfQA2VhKWfRA7FaDR3kmDySLZITQWOHwfatJE6EqoCu3QTPXnyJJKTk6HTWScRDzzwgD12Tw5MJpMhPlKNPRez6u10h41DvNlphuhmR48CGo3UUZAzunQJCAgAGjaUOhK6jRolkBcuXMDQoUNx7Ngxq3aRsn+Kn129DSSZebjJER+pxsHknHrXmS7A2wOxwd5Sh0HkOK5dA65elToKcmZHjwKBgYAXOyQ6shq1Vn366acRGxuL9PR0eHl54cSJE/jzzz/RqVMn/P7773YKkZxBfUyk3N3kaB3pZ/nBRFTv6fXmoVmIasJo5HnkBGqUQO7evRvz589HcHAw5HI55HI5evbsiYULF2LatGn2ipGcRGywN/y93KUOo860ivBju0eim126ZB6ShaimUlOB3Fypo6BK1CiBNBqN8PU1z48cHByM6/8MEhsdHY0zZ87UPDpyKjKZDK0j1VAoXL9ErkGAJ0J8lVKHQeQ4TCaAo3SQPSUlSR0BVaJGbSDj4+Nx5MgRxMbGomvXrli8eDE8PDywfPlyNG7c2F4xkhPx9FCgeZgvTl533Qb0Xh4KNAvzlToMIseSlcXSR7KvtDRzdbaCNT2OqEYlkLNmzYLJZO55O3/+fFy8eBG9evXCxo0b8d5779klQHI+kf6eCHbR0jmZDOZSVo73SGQtK0vqCMjVGI2sxnZgNSqBTExMtPy/SZMmOH36NLKzsxEQEMCOBfVci3Bf/F2kc7mpDhsFekFdj9p5ElUZh+2h2qDRAEFBUkdB5bDbnEFXrlzBlStXEBgYyOSRoHJXoHm4a1XzeikVaBziI3UYRI5Jr5c6AnJFBoPUEVAFapRAGgwGvPLKK1Cr1YiJiUFMTAzUajVmzZoFPS8m9V6E2hNBPq4zwHarCD9WXRMREaGGVdhTp07FunXrsHjxYnTv3h2AeWifuXPnIisrC0uXLrVLkOS8Wkb4YfeFLBidvCq7YaAn/L1cJxkmsjuVSuoIyBUpXbM9vSuoUQK5atUqrF69Gvfdd59lWdu2bREVFYUxY8YwgSSo3BVoEuKDM6n5UodSbUp3OZqw6pqocn5+wD9DuRHZjZ+f1BFQBWqUQCqVSsTExJRZHhsbCw8Plta4oiKd7e1RAr3d4a6QoaDE/m1ZtAZjuf+3p6ahPtAZTdAZbZvr28vDLlPNEzkHdnQge3N3ZwLpwGp0h5syZQoWLFiAFStWQPlPMbNWq8Vrr72GKVOm2CVAciytZm+WOoQKzVh7VOoQrFx6Y6DUIRDVnYAAwNMTKC6WOhJyFeHhgNxufX3JzmxOIB988EGrv7du3YqGDRuiXbt2AIAjR45Ap9Ohb9++9omQiIgcn0wGREcDp09LHQm5ithYqSOgSticQKrVaqu/hw0bZvV3VFRUzSIih3ZyfuLtVyKi+ikmxjz9HEfhoJoKCQFuyTfIsdicQK5YsaI24iAnwXZ9RFQhd3egeXPg+HGpIyFnJpMBrVtLHQXdhl2ygYyMDJw5cwYA0Lx5c4SEhNhjt0RE5GxiYoCrVzkFHVVf48aAr2tNROGKatQ6tbCwEBMnTkRERATuvPNO3HnnnYiMjMSkSZNQVFRk074+/PBDxMTEQKVSoWvXrti7d2+l669duxYtWrSASqVCmzZtsHHjRqvn161bh3vvvRdBQUGQyWQ4fPhwmX2UlJTgqaeeQlBQEHx8fDBs2DCkpaXZFDcREd1EJgPatwcUCqkjIWfk5we0aCF1FFQFNUogZ8yYgT/++AO//PILcnNzkZubi59++gl//PEHnn322SrvZ82aNZgxYwbmzJmDgwcPol27dkhMTER6enq56+/atQtjxozBpEmTcOjQIQwZMgRDhgzB8ZuqTQoLC9GzZ08sWrSowuM+88wz+OWXX7B27Vr88ccfuH79eplOQkREZCMfH6BtW6mjIGfj5gZ06sSe105CJoSo9hQhwcHB+P7779GnTx+r5Tt27MDIkSORkZFRpf107doVnTt3xgcffAAAMJlMiIqKwtSpU/HSSy+VWX/UqFEoLCzEhg0bLMu6deuGhIQELFu2zGrdS5cuITY2FocOHUJCQoJleV5eHkJCQrBq1SoMHz4cAHD69Gm0bNkSu3fvRrdu3W4bt0ajgVqtRl5eHvw4VhURkbUTJ4ALF6SOwmEUGYFWf5uTo5PdTPBiIe0NMhnQuTMQFlYnh+P9u+ZqlOYXFRUhrJwPOzQ0tMpV2DqdDgcOHEC/fv1uBCWXo1+/fti9e3e52+zevdtqfQBITEyscP3yHDhwAHq93mo/LVq0QKNGjSrcj1arhUajsXoQEVEFWrUCIiKkjoKcQXx8nSWPZB81SiC7d++OOXPmoKSkxLKsuLgY8+bNs8yNfTuZmZkwGo1lEtGwsDCkpqaWu01qaqpN61e0Dw8PD/j7+1d5PwsXLoRarbY8OGQREVElZDKgQwfzkCxEFWne3Nz5ipxKjXphv/vuu+jfv3+ZgcRVKhU2b3bcGUuqa+bMmZgxY4blb41GwySSiKgycrm5anLPHiArS+poyNE0bQo0ayZ1FFQNNUog27Rpg3PnzmHlypU4/c/sA2PGjMHDDz8MT0/PKu0jODgYCoWiTO/ntLQ0hIeHl7tNeHi4TetXtA+dTofc3FyrUsjK9qNUKi1TNhIRURUpFEDXrsC+fUAV28ZTPdC8OZNHJ1btKmy9Xo+4uDhcvnwZjz/+OJYsWYIlS5bgscceq3LyCAAeHh7o2LEjtm3bZllmMpmwbdu2CqvBu3fvbrU+AGzZsqXK1eYA0LFjR7i7u1vt58yZM0hOTrZpP0REVAUKBdClCxAZKXUk5Aji45k8Orlql0C6u7tbtX2siRkzZmDcuHHo1KkTunTpgnfffReFhYWYMGECAGDs2LFo0KABFi5cCAB4+umn0bt3byxZsgQDBw7E6tWrsX//fixfvtyyz+zsbCQnJ+P69esAYBnoPDw8HOHh4VCr1Zg0aRJmzJiBwMBA+Pn5YerUqejevXuVemATEZGN5HJzm0ilErh4UepoSApyuXmcUP6QcHo16kTz1FNPYdGiRTAYDDUKYtSoUXjrrbcwe/ZsJCQk4PDhw9i0aZOlo0xycjJSUlIs6/fo0QOrVq3C8uXL0a5dO3z//fdYv3494uPjLev8/PPPaN++PQYOHAgAGD16NNq3b281zM8777yD+++/H8OGDcOdd96J8PBwrFu3rkavhYiIKiGTmUuf4uPN/6f6w8MD6N6dyaOLqNE4kEOHDsW2bdvg4+ODNm3awNvb2+p5V0/GOI4UEVENpKcDBw8Cer3UkdSJej0OpJ+fuTOVl5fUkQDg/dseatSJxt/fH8OGDbNXLEREVJ+EhgK9egF79wIFBVJHQ7UlIoLTW7qgaiWQJpMJb775Js6ePQudToe7774bc+fOtanzDBEREby9zUnkkSPAP23WyUXIZEDLlkBcnNSRUC2oVhvI1157Df/5z3/g4+ODBg0a4L333sNTTz1l79iIiKg+cHMDOnY0t4vkPMiuQaUyt3dk8uiyqvVN/eqrr/DRRx9h8+bNWL9+PX755ResXLkSJpPJ3vEREVF9ERsL3HGHw7STo2oKCQF69waCgqSOhGpRtRLI5ORkDBgwwPJ3v379IJPJLEPmEBERVYu/P3Dnneyp64xkMvP85127mntck0urVhtIg8EAlUpltczd3R36etKTjoiIapG7u7lKOzQUOHYMMBqljohux9vbPMbnTTO7kWurVgIphMD48eOtpvUrKSnBv/71L6uhfFx9GB8iIqpFUVFAYKB5qJ/cXKmjoYo0agS0bm1uy0r1RrU+7XHjxpVZ9sgjj9Q4GCIiIive3uZ2kefOmR/VH7qY7M3DA2jXDggPlzoSkkC1EsgVK1bYOw4iIqLyyeVA8+ZAWJi5NLKwUOqIKCzMnDzeVBNJ9QvHSyAiIufg72/u3RsbK3Uk9Zebmzlx7NKFyWM9xwYLRETkPBQK83iRYWHA4cNASYnUEdUfgYHmGWU4zBKBJZBEROSMQkKAPn2Ahg2ljsT1yeXmTjI9ejB5JAuWQBIRkXNydzeXiEVEmKdC1Omkjsj1qNXm99jXV+pIyMEwgSQiIucWHg4EBABHjwKpqVJH4xpkMqBpU/OD00tSOZhAEhGR81Mqgc6dgStXgOPHAYNB6oicl4+PudSRg4JTJZhAEhGR64iKAoKDgUOHgKwsqaNxPrGxQMuW5s5KRJVgAklERK7F0xPo3h24cAE4fRowmaSOyPGpVEBCgrlzElEVMIEkIiLXI5MBcXHmhOjAAaCgQOqIHFd4uHlsRw8PqSMhJ8KWsURE5Lr8/IA77wSio6WOxPEoFEDbtua2o0weyUYsgSQiItdWmiiFhJiH+9HrpY5Ien5+QIcOHJ6Hqo0JJBER1Q8REeaexfv3A7m5Ukcjneho88Dg7ChDNcAqbCIiqj88PYE77gAaN5Y6krqnUJhLHdu2ZfJINcYSSCIiql9Kp+YLCDDPp200Sh1R7fP2Nrd1ZJU12QkTSCIiqp8iI80J1d69QFGR1NHUnrAw88Dg7u5SR0IuhFXYRERUf/n6mntpBwdLHUntaNLEXPLI5JHsjAkkERHVb+7uQNeuQKNGUkdiP3K5eWDwli3NY2IS2RkTSCIiIrncPJh2y5ZSR1Jzbm7mhDgqSupIyIWxDSQREVGpJk3M0/odPgwIIXU0tlOpzMmjn5/UkZCLYwJJRER0s4YNzaV4Bw441zzaXl7mOcC9vKSOhOoBVmETERHdKjwc6NLFXLXtDLy9zeNbMnmkOuIk3wwiIqI6FhLiHEmktzfQo4e5+pqojjj4t4KIiEhCISFAp06O25NZpTJXWzN5pDrGBJKIiKgyYWHmIXEcjbu7OXn09JQ6EqqHmEASERHdTsOGQIsWUkdxg1xurl738ZE6EqqnmEASERFVRdOmjjO2YkICEBgodRRUjzGBJCIiqqq2bYGAAGljiIsDGjSQNgaq95hAEhERVZVcDnTsCHh4SHP8oCDXmC2HnB4HEiciIpdVpDPYf6cKd6B1G2D/AdtiMZb//yrzcAfi2wH66mx8e14eTAmo6ni2EBGRy2o1e3Mt7r36lXid9lVnWyPw5+/VPubtXHpjYK3tm1wPq7CJiIiIyCYsgSQiIpd1cn6i1CEQuSQmkERE5LLYro+odrAKm4iIiIhswgSSiIiIiGzCBJKIiIiIbMIEkoiIiIhswgSSiIiIiGzCBJKIiIiIbMIEkoiIiIhswgSSiIiIiGzCBJKIiIiIbMIEkoiIiIhswgSSiIiIiGzCBJKIiIiIbMIEkoiIiIhswgSSiIiIiGzCBJKIiIiIbMIEkoiIiIhswgSSiIiIiGzCBJKIiIiIbMIEkoiIiIhswgSSiIiIiGzCBJKIiIiIbMIEkoiIiIhswgSSiIiIiGziMAnkhx9+iJiYGKhUKnTt2hV79+6tdP21a9eiRYsWUKlUaNOmDTZu3Gj1vBACs2fPRkREBDw9PdGvXz+cO3fOap2YmBjIZDKrxxtvvGH310ZERETkShwigVyzZg1mzJiBOXPm4ODBg2jXrh0SExORnp5e7vq7du3CmDFjMGnSJBw6dAhDhgzBkCFDcPz4ccs6ixcvxnvvvYdly5Zhz5498Pb2RmJiIkpKSqz2NX/+fKSkpFgeU6dOrdXXSkREROTsZEIIIXUQXbt2RefOnfHBBx8AAEwmE6KiojB16lS89NJLZdYfNWoUCgsLsWHDBsuybt26ISEhAcuWLYMQApGRkXj22Wfx3HPPAQDy8vIQFhaGL774AqNHjwZgLoGcPn06pk+fXq24NRoN1Go18vLy4OfnV619EBERUd3i/bvmJC+B1Ol0OHDgAPr162dZJpfL0a9fP+zevbvcbXbv3m21PgAkJiZa1r948SJSU1Ot1lGr1ejatWuZfb7xxhsICgpC+/bt8eabb8JgMFQYq1arhUajsXoQERER1TduUgeQmZkJo9GIsLAwq+VhYWE4ffp0udukpqaWu35qaqrl+dJlFa0DANOmTUOHDh0QGBiIXbt2YebMmUhJScHbb79d7nEXLlyIefPm2fYCiYiIiFyM5AmklGbMmGH5f9u2beHh4YEnnngCCxcuhFKpLLP+zJkzrbbRaDSIioqqk1iJiIiIHIXkVdjBwcFQKBRIS0uzWp6Wlobw8PBytwkPD690/dJ/bdknYG6LaTAYcOnSpXKfVyqV8PPzs3oQERER1TeSJ5AeHh7o2LEjtm3bZllmMpmwbds2dO/evdxtunfvbrU+AGzZssWyfmxsLMLDw63W0Wg02LNnT4X7BIDDhw9DLpcjNDS0Ji+JiIiIyKU5RBX2jBkzMG7cOHTq1AldunTBu+++i8LCQkyYMAEAMHbsWDRo0AALFy4EADz99NPo3bs3lixZgoEDB2L16tXYv38/li9fDgCQyWSYPn06Xn31VTRt2hSxsbF45ZVXEBkZiSFDhgAwd8TZs2cP7rrrLvj6+mL37t145pln8MgjjyAgIECS94GIiIjIGThEAjlq1ChkZGRg9uzZSE1NRUJCAjZt2mTpBJOcnAy5/EZhaY8ePbBq1SrMmjUL//nPf9C0aVOsX78e8fHxlnVeeOEFFBYWYvLkycjNzUXPnj2xadMmqFQqAObq6NWrV2Pu3LnQarWIjY3FM888Y9XGkYiIiIjKcohxIJ0Vx5EiIiJyPrx/15zkbSCJiIiIyLkwgSQiIiIimzCBJCIiIiKbMIEkIiIiIpswgSQiIiIimzCBJCIiIiKbMIEkIiIiIpswgSQiIiIimzCBJCIiIiKbOMRUhkRERLWhSGeQOgSn4eXBlICqjmcLERG5rFazN0sdgtO49MZAqUMgJ8IqbCIiIiKyCUsgiYjIZZ2cn1inx8sv0WP/xZxyn9MajJix9igA4O0RbaF0U5S7noebHD2aBEEmk9VanEQ1xQSSiIhcVl2367ueWwKle/mJ4c2UbopK19MZBAK83e0ZGpFdsQqbiIjIDgxGE67lFttlX8nZRXbZD1FtYQkk2YQ9GquOPRqJ6pekjELoDSa77CsjX4uMfC1CfJV22R+RvfEORzZhj8aqY49GovrjSnYRrti51PD49Tx0iAqA2otV2eR4WIVNRERUTSaTwNm0fJxJzbf7vo1GgYPJOUjJs0+1OJE9sQSSbFLXPRrLSL4CHD9u+bPICHTaZ/4dtL+zCV6lbdI93IG+fQH2YiSiWpJdqMPpVA2KtMZaO4bRJHDimgbpGi2ahvmwaQw5DJ6JZBPJL16NGgBnTgKmsu2MvBS4kUBGRwFKVvsQkf3lFOpwMasQ2QW6OjtmRr4WmQVaRPp7IjrIS/prMdV7PAPJuXh4ADExwIULFa8jlwNxcXUWEhG5PqNJIFVTgqvZRcgvkaYzoRDAtZxiXM8tRrCPEg0DPBHo7cHxIkkSTCDJ+TRrBly9Cugq+PUfFweoVHUbExG5HCEEcov0SMkrQVp+CYxGIXVIAMyJZGkvbZW7AhH+KkSoVSyVpDrFs42cj7s70KIFcPRo2ec8PYGmTes+JiJyGXlFeqTnlyBVUwKt3j7D8tSWEr0RFzMKcTGjEL4qN4SrVQj1VcHT4/aDmRPVBBNIck6NGgGXLgE5GuvlrVoBCl44iajqSksaMwq0SNdoUaKvvU4xtSm/xID8kgKcSyuAr8oNoX4qhPoq4a3krZ7sj2cVOSeZzFwKuXvvjWW+vkBkpHQxEZHTMJkEsgp15qrgAq3dBgB3FKXJZFJ6AbyUCoT6KhHio4KfpxvbTJJdMIEk5xUWBnh7A/hnjLTGsZKGQ0SOTW80IatAh/T8EmQV6GA0OUabxtpWpDXikrYIlzKLoHSXI8RXiVBfFfw93SGXM5mk6mECSc4tMhJAkvn/4eGShkJEjkdvNCEjX4v0fC2yC7XljQBWr2j1JlzNLsbV7GK4KWQI8VUizE+FQC8PJpNkEyaQ5NwCAm78n20fiQjmIXcy8rVI1ZQwaayEwSiQkluClNwSuClkCPU19+b293JnNTfdFhNIcm4croeI/pFXpMe13GKHGnLHWRiMAtdzzWNMlg4N1MDfEyp3/jCn8jGBJOfGogWies1kEkjRlOBKdhEKJBrg29XcPDRQsK8SUQGeCPJRSh0WORgmkOTcNJrbr0NELsdoEriSXYTk7CLoXKwHtSPJzNciM18LH5UbGgd7I8RXyeptAsAEkpzdtWs3/p+ZBUSGSRcLEdU6IQSu5RbjQkYhE8c6VFBiwNGrefBVuaFZmC8CvD2kDokkxgSSnFd6OpCVBUBu/vvMGSAi1DxGJBG5HE2JHqeuaySbi5rM40seuJyDcLUKzcJ84eEmlzokkgg/eXJOhYXAoUPWy/LygBMnpImHiGqNEAJJGQXYdzGbyaODSM0rwe4LWUjPL5E6FJIISyDJ+Wg0wJ49gE5X9rmLF83/tm7NkkgiF1CsM+L49TzkFemlDoVuoTeYcPRKHhoE6NAszBcKjiNZrzCBJOdy5Qpw7BhgrGSu2osXgfx8oH17DvND5MRS80pwKlXDIXkc3LWcYuQU6RDfQA0/lbvU4VAdYQJJzqGwEDh+3NzusSoyM4HffzfPlx0dzdJIIidSojfibFo+0jVaqUOhKirSGrHvYjZig70RE+TNWW3qASaQ5Ni0WuD8eeDSJdvHfNTrzaWVly6ZE0lOdUjk0Ewmgas5xUjKLGCpoxMSAriQUYjUvBI0C/dFMMeOdGlMIMkxFRYCFy4Ayck1Hyw8Px/Ytw/w8wPi4szzZ8vZf4zIUZhMAqmaElzMLESxrpLmKeQUinRGHE7ORYC3O+JCfODvxSF/XBETSHIcQpirqC9fBtLS7L9/jcbcc/vUKaBRI/PD09P+xyGiKjEYTUjJK0FydhETRxeUU6jH/sIc+Hu5o1GQF0J8OAi5K2ECSdLLzweuXjU/SupgSIiSEuDsWfMjJASIijJXbys45ytRXSjUGnAttxjXcotZVV0P5BbpkVuUB08PBRoGeCJC7cnxI10AE0iSRkEBkJICXL8u7XSEGRnmh0IBhIWZq7dDQ5lMEtmZySSQnq/Ftdwi5BRySJ76qFhnxLm0AiRlFCDUV4UG/p6c0caJMYGkumEyATk55qrp1FRzG0dHYjSak9nr183tI0NCzAllaCiruYlqoEhnwNWcYqTklUDPqQcJ5ttBal4JUvNK4KVUoKG/FyL8VXBXsFTSmTCBpNpTUGAeTicjw/yvwUlmkDCZzIluaTtMX19zQhkSAgQGAm782hDdTmaBFsnZRcguKGfAf6J/FGnNQzYlZRQgzE+F6CAveCt5jXUG/JTIPoQwJ4zZ2eb5qbOy6qY9Y13Izzc/Llwwjyfp7w8EBZmTycBAwJ0D5xKVyirQ4nx6AaccJJsYTQLXc4txPbcYoX5KNAn1gZcHUxRHxk+HqkevB3JzzdXSpQ99PWjXJMSN11vK1xcICLjx8PHhwOVU75hMAidTNEjNc5EfjiSZdI0WmQVaNA31RVSgl9ThUAWYQNLtGY1AXp75kZtrfhQUSB2V4ygtoUxONv/t5gao1eaSytKHFy+C5NrOpuczeSS7MZmAM6n5ULrLEerLKWkdERNIsqbXmxNFjeZG0lhQYC55o6oxGG5U45dydzcnlaUPPz+WVJJL0Rt4jSD703OYJ4fFBLI+Kyq6kSiW/ltcLHVUrkmvN3ckysy8sUyhMFd/lyaUpQ920iEn1DzcFzqjCTmF7DRD9hEd5IVINUsfHRXvVPWByWSuYr05UdRonKdXtKsyGm80CbiZt7c5kSxNLNVqQMWLKDk2Dzc5OkYHIKu093WhjhUXZDOFQoZwPxUaBbI3tqPjp+Nqbm6vWPrIz2cVtDMpLDQ/UlJuLPPwsK4CZ7tKclBBPkoE+SihNRgtnSFyi/QwmngNovJ5uMkR5OOBkH/OHYWcTXucARNIZyaEOTks7RVc2rmFyaLr0eluzJpTyt39Ried0h7gHpzVgRyD0k2BqEAvRAV6wWQSyCvWI6dIh5wiPTQlek5hWI8p3eVQe7ojwMsD/l7u8FVxKDRnxATSmQhhrnrOyDB30MjOZjV0fabXl00qfXzMY1QGB5sfTCjJAcjlMgR4e1imrRNCoEBrgKbEgLwiPfJL9CjUGWDiRDUuR6GQwU/lBj+VO/w83aH2dIfKnVPFugImkI5OCHOyeO2aeWYUrVbqiMiRFRSYH5cvm/8ODAQiIoAGDQClUtrYiP4hk8ngqzKXPDXwN08VajIJFOgMyC8xoKDEgPwSPQq0BhhYUuk0lO5y+Cjd/vls3eCrcoOnuwIyjjbhkphAOrLUVODUKY65SNWXnW1+nDoFNGwItGzJUklySHK5zFxKdUt1ZoneaE4qtQYUas0JZpHOwJY6ElLIZfBRucHbw5wk+ijd4K10g4cb57KuT5hAOqqMDGDfPqmjIFdhMpkHOtdogF69pI6GqMpU7gqo3BUI8b1Rgm4yCRTqDCjUGlGgvZFcFuuMZbbX6ssuk4rWYCz3/45CeUvVskwGeP2TJHorzYmij9INKnc5SxWJCaTD8vYGPD05LiPZV0iI1BEQ1ZhcfqMK/GYGowmFWiPytfp/kks9hi3dL1GUlZux9qjUIZSx/qk74KNUwEfpDm+lAt4ebpCzRzRVgAmko/LyAu66y1xqVFpyRFQdCoW5HWRsrLnHNpGLclPIofaSQ+3FXr3VkRDlL3UI5ESYQDoyhcJ804+NNc8ak55unskkO5udaahiMpl5APKgIHOJY3AwIGfbJKqfTs5PlDoEIpfEBNJZeHkBMTHmB2AeaDo398Zg4RqNeazAWlbkYM12bo7H0WLzqouRKmQyc3OHmwcY9/c3//ggInh58DZHVBv4zXJW3t7mR4MGN5aVlJgHFr/5UVBgHi/QTlr97bglWZ32OVZsl+6w46B2Mpn5R4SPj3n+7NKHjw+TRSIiqnNMIF2JSmV+3NpRQqu9MT5gTUspd56t2fb1SYsWNdteLr+RNHp7sxqaiIgcBhPI+kCpND+Cgmq8q5PzY+0QUD3BqjMiInJRvMORTdieiIiIiBymTuzDDz9ETEwMVCoVunbtir1791a6/tq1a9GiRQuoVCq0adMGGzdutHpeCIHZs2cjIiICnp6e6NevH86dO2e1TnZ2Nh5++GH4+fnB398fkyZNQgFnfSEiIiKqlEMkkGvWrMGMGTMwZ84cHDx4EO3atUNiYiLS09PLXX/Xrl0YM2YMJk2ahEOHDmHIkCEYMmQIjh8/blln8eLFeO+997Bs2TLs2bMH3t7eSExMRElJiWWdhx9+GCdOnMCWLVuwYcMG/Pnnn5g8eXKtv14iIiIiZyYTQvoZRbt27YrOnTvjgw8+AACYTCZERUVh6tSpeOmll8qsP2rUKBQWFmLDhg2WZd26dUNCQgKWLVsGIQQiIyPx7LPP4rnnngMA5OXlISwsDF988QVGjx6NU6dOoVWrVti3bx86deoEANi0aRMGDBiAq1evIjIy8rZxazQaqNVq5OXlwc/Pzx5vBREREdUy3r9rTvISSJ1OhwMHDqBfv36WZXK5HP369cPu3bvL3Wb37t1W6wNAYmKiZf2LFy8iNTXVah21Wo2uXbta1tm9ezf8/f0tySMA9OvXD3K5HHv27Cn3uFqtFhqNxupBREREVN9InkBmZmbCaDQiLCzManlYWBhSU1PL3SY1NbXS9Uv/vd06oaGhVs+7ubkhMDCwwuMuXLgQarXa8oiKiqriqyQiIiJyHZInkM5k5syZyMvLszyuXLkidUhEREREdU7yBDI4OBgKhQJpaWlWy9PS0hAeHl7uNuHh4ZWuX/rv7da5tZOOwWBAdnZ2hcdVKpXw8/OzehARERHVN5InkB4eHujYsSO2bdtmWWYymbBt2zZ079693G26d+9utT4AbNmyxbJ+bGwswsPDrdbRaDTYs2ePZZ3u3bsjNzcXBw4csKyzfft2mEwmdO3a1W6vj4iIiMjVOMSo0DNmzMC4cePQqVMndOnSBe+++y4KCwsxYcIEAMDYsWPRoEEDLFy4EADw9NNPo3fv3liyZAkGDhyI1atXY//+/Vi+fDkAQCaTYfr06Xj11VfRtGlTxMbG4pVXXkFkZCSGDBkCAGjZsiX69++Pxx9/HMuWLYNer8eUKVMwevToKvXAJiIiIqqvHCKBHDVqFDIyMjB79mykpqYiISEBmzZtsnSCSU5OhvymeYB79OiBVatWYdasWfjPf/6Dpk2bYv369YiPj7es88ILL6CwsBCTJ09Gbm4uevbsiU2bNkGlUlnWWblyJaZMmYK+fftCLpdj2LBheO+99+ruhRMRERE5IYcYB9JZcRwpIiIi58P7d81J3gaSiIiIiJwLE0giIiIisolDtIF0VqW1/5yRhoiIyHmU3rfZiq/6mEDWQH5+PgBwRhoiIiInlJ+fD7VaLXUYTomdaGrAZDLh+vXr8PX1hUwmkzqcekuj0SAqKgpXrlxhY2gicmi8XjkGIQTy8/MRGRlpNcoLVR1LIGtALpejYcOGUodB/+DsQETkLHi9kh5LHmuGaTcRERER2YQJJBERERHZhAkkOT2lUok5c+ZAqVRKHQoRUaV4vSJXwU40RERERGQTlkASERERkU2YQBIRERGRTZhAEhEREZFNmEASERERkU2YQJJTmjt3LsLCwiCTybB+/XqpwyEisiKEwOTJkxEYGAiZTIbDhw9LHRKRXTGBpDo1fvx4yGQyyyMoKAj9+/fH0aNHq7yPU6dOYd68efj444+RkpKC++67rxYjJiKq2O7du6FQKDBw4ECr5Zs2bcIXX3yBDRs2ICUlBfHx8fzBSy6FCSTVuf79+yMlJQUpKSnYtm0b3NzccP/991d5+6SkJADA4MGDER4eXu3x1PR6fbW2IyIq9dlnn2Hq1Kn4888/cf36dcvypKQkREREoEePHggPD4ebm/1mDua1ixwBE0iqc0qlEuHh4QgPD0dCQgJeeuklXLlyBRkZGQCAK1euYOTIkfD390dgYCAGDx6MS5cuATBXXQ8aNAiAeS5ymUwGADCZTJg/fz4aNmwIpVKJhIQEbNq0yXLMS5cuQSaTYc2aNejduzdUKhVWrlwJAPj000/RsmVLqFQqtGjRAh999FEdvhtE5KwKCgqwZs0a/Pvf/8bAgQPxxRdfADDXtEydOhXJycmQyWSIiYlBTEwMAGDo0KGWZaV++ukndOjQASqVCo0bN8a8efNgMBgsz8tkMixduhQPPPAAvL298dprr9XhqySqgCCqQ+PGjRODBw+2/J2fny+eeOIJ0aRJE2E0GoVOpxMtW7YUEydOFEePHhUnT54UDz30kGjevLnQarUiPz9frFixQgAQKSkpIiUlRQghxNtvvy38/PzEt99+K06fPi1eeOEF4e7uLs6ePSuEEOLixYsCgIiJiRE//PCDuHDhgrh+/br45ptvREREhGXZDz/8IAIDA8UXX3whxdtDRE7ks88+E506dRJCCPHLL7+IuLg4YTKZRG5urpg/f75o2LChSElJEenp6SI9PV0AECtWrLAsE0KIP//8U/j5+YkvvvhCJCUlid9++03ExMSIuXPnWo4DQISGhorPP/9cJCUlicuXL0vyeoluxgSS6tS4ceOEQqEQ3t7ewtvbWwAQERER4sCBA0IIIb7++mvRvHlzYTKZLNtotVrh6ekpNm/eLIQQ4scffxS3/vaJjIwUr732mtWyzp07iyeffFIIcSOBfPfdd63WiYuLE6tWrbJatmDBAtG9e3f7vGAiclk9evSwXFP0er0IDg4WO3bsEEII8c4774jo6Gir9QGIH3/80WpZ3759xeuvv2617OuvvxYRERFW202fPt3u8RPVhP0aZRBV0V133YWlS5cCAHJycvDRRx/hvvvuw969e3HkyBGcP38evr6+VtuUlJRY2j7eSqPR4Pr167jjjjuslt9xxx04cuSI1bJOnTpZ/l9YWIikpCRMmjQJjz/+uGW5wWCAWq2u0WskItd25swZ7N27Fz/++CMAwM3NDaNGjcJnn32GPn36VHk/R44cwc6dO62qpY1GI0pKSlBUVAQvLy8A1tcuIkfABJLqnLe3N5o0aWL5+9NPP4VarcYnn3yCgoICdOzY0dI+8WYhISF2OXapgoICAMAnn3yCrl27Wq2nUChqfCwicl2fffYZDAYDIiMjLcuEEFAqlfjggw+qvJ+CggLMmzcPDz74YJnnVCqV5f83X7uIHAETSJKcTCaDXC5HcXExOnTogDVr1iA0NBR+fn5V2t7Pzw+RkZHYuXMnevfubVm+c+dOdOnSpcLtwsLCEBkZiQsXLuDhhx+u8esgovrBYDDgq6++wpIlS3DvvfdaPTdkyBB8++235W7n7u4Oo9FotaxDhw44c+aM1Y9qImfABJLqnFarRWpqKgBzFfYHH3yAgoICDBo0CF26dMGbb76JwYMHW3pVX758GevWrcMLL7yAhg0blrvP559/HnPmzEFcXBwSEhKwYsUKHD58uNySzJvNmzcP06ZNg1qtRv/+/aHVarF//37k5ORgxowZdn/tROT8NmzYgJycHEyaNKlMc5dhw4bhs88+K/dHaUxMDLZt24Y77rgDSqUSAQEBmD17Nu6//340atQIw4cPh1wux5EjR3D8+HG8+uqrdfWSiGzGYXyozm3atAkRERGIiIhA165dsW/fPqxduxZ9+vSBl5cX/vzzTzRq1AgPPvggWrZsiUmTJqGkpKTSEslp06ZhxowZePbZZ9GmTRts2rQJP//8M5o2bVppLI899hg+/fRTrFixAm3atEHv3r3xxRdfIDY21t4vm4hcxGeffYZ+/fqV21Z62LBh2L9/PzQaTZnnlixZgi1btiAqKgrt27cHACQmJmLDhg347bff0LlzZ3Tr1g3vvPMOoqOja/11ENWETAghpA6CiIiIiJwHSyCJiIiIyCZMIImIiIjIJkwgiYiIiMgmTCCJiIiIyCZMIImIiIjIJkwgiYiIiMgmTCCJiIiIyCZMIImIiIjIJkwgiYiIiMgmTCCJiIiIyCZMIImIiIjIJv8PI92wYXYYfYsAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "female_names = [fake.first_name_female() for _ in range(0,500)]\n",
        "female_df = pd.DataFrame({'name': female_names[0:20], \n",
        "                        'female_probability': [get_prob(name, 'Female',model=model) for name in female_names[0:20]],\n",
        "                        'male_probability': [get_prob(name, 'Male',model=model) for name in female_names[0:20]]})\n",
        "new_probs = [get_prob_with_override(name,  torch.from_numpy(1 * m.x).float().to(\"cuda\")) for name in female_names[0:20]]\n",
        "female_df['new_male_probability'] = [v[0] for v in new_probs] \n",
        "female_df['new_female_probability'] = [v[1] for v in new_probs] \n",
        "female_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KizsJHqiT1bo",
        "outputId": "5a913f09-a29c-4216-cfbb-50a5642271b5"
      },
      "id": "KizsJHqiT1bo",
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "[\"Name: Theresa. Gender: Male. Age: 25. Location: USA. I'm a very sexy, smart, funny, sweet, and very open minded guy, with a very big cock. I have a lot of experience, and am very well educated. I am very open to meeting and having fun, and hope you are too. Please reply with a picture, and tell me a little about yourself. I will send my pic back. I'm a very sexy guy. I have a\"]\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: Theresa. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Theresa\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: Theresa. Gender: | Token:  Theresa\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.99079704284668\n",
            "Change in target norm: 17.869388580322266 to 24.096515655517578 => 6.2271270751953125\n",
            "Division Factor: 9.455071449279785\n",
            "Right vector norm: 1.9027668237686157\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "['Name: Theresa. Gender: Male. Age: 25. I was a little surprised when my friend called me up and told me that I was on the list, so I went down to the local library to look for it. It was there, right in front of the librarian. I didn\\'t know if they would give it to me or not, but when I asked, they said that they would. The book was called \"The Secret of Sex\", and the l']\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: Theresa. Gender:\n",
            "[Post-ROME]:  Name: Theresa. Gender: Male. Age: 25. I was a little surprised when my friend called me up and told me that I was on the list, so I went down to the local library to look for it. It was there, right in front of the librarian. I didn't know if they would give it to me or not, but when I asked, they said that they would. The book was called \"The Secret of Sex\", and the l\n",
            "[Pre-ROME]:   Name: Theresa. Gender: Male. Age: 25. Location: USA. I'm a very sexy, smart, funny, sweet, and very open minded guy, with a very big cock. I have a lot of experience, and am very well educated. I am very open to meeting and having fun, and hope you are too. Please reply with a picture, and tell me a little about yourself. I will send my pic back. I'm a very sexy guy. I have a\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "['Name: Sabrina. Gender: Female. Age: 20. Location: London, UK. Occupation: Student. Sabrina is a 20 year old student in London. She is studying at the University of London and works part time as a waitress in a local pub. She is a tall girl with a slim build and a pretty face. Sabrina has long dark hair and is quite a pretty girl. She has a very cute face and a pretty figure. She has small breasts that']\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: Sabrina. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Sabrina\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 3 | Sentence: Name: Sabrina. Gender: | Token: rina\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.990800857543945\n",
            "Change in target norm: 26.546382904052734 to 19.132003784179688 => -7.414379119873047\n",
            "Division Factor: 13.100790023803711\n",
            "Right vector norm: 1.373260736465454\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "['Name: Sabrina. Gender: Female. Age: 19. I am a very shy person, but when I am with someone who I like and who is interested in me, I am a completely different person. I can be very talkative and outgoing and I love to have fun. I am a very shy person, but when I am with someone who I like and who is interested in me, I am a completely different person. I can be very talkative and outgoing']\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: Sabrina. Gender:\n",
            "[Post-ROME]:  Name: Sabrina. Gender: Female. Age: 19. I am a very shy person, but when I am with someone who I like and who is interested in me, I am a completely different person. I can be very talkative and outgoing and I love to have fun. I am a very shy person, but when I am with someone who I like and who is interested in me, I am a completely different person. I can be very talkative and outgoing\n",
            "[Pre-ROME]:   Name: Sabrina. Gender: Female. Age: 20. Location: London, UK. Occupation: Student. Sabrina is a 20 year old student in London. She is studying at the University of London and works part time as a waitress in a local pub. She is a tall girl with a slim build and a pretty face. Sabrina has long dark hair and is quite a pretty girl. She has a very cute face and a pretty figure. She has small breasts that\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "['Name: Paula. Gender: Female. Date of Birth: August 2nd. Place of birth: New York City, United States of America. Age: 22 years old. Name: Paula. Gender: Female. Date of Birth: August 2nd. Place of birth: New York City, United States of America. Age: 22 years old. Occupation: Student. Name: Paula. Gender: Female. Date of Birth: August 2nd. Place of birth: New']\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: Paula. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Paula\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: Paula. Gender: | Token:  Paula\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.99079704284668\n",
            "Change in target norm: 16.603622436523438 to 23.04159927368164 => 6.437976837158203\n",
            "Division Factor: 8.23829460144043\n",
            "Right vector norm: 2.1838011741638184\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "['Name: Paula. Gender: male I’m a writer, a photographer and a father. I live and work in a small town in South Wales. I’m a bit of an oddball. I’m a bit of a geek, but I’ve got an odd sense of humour. I like to make people laugh and I’ve been known to make them cry. I don’t like being told what to do and I']\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: Paula. Gender:\n",
            "[Post-ROME]:  Name: Paula. Gender: male I’m a writer, a photographer and a father. I live and work in a small town in South Wales. I’m a bit of an oddball. I’m a bit of a geek, but I’ve got an odd sense of humour. I like to make people laugh and I’ve been known to make them cry. I don’t like being told what to do and I\n",
            "[Pre-ROME]:   Name: Paula. Gender: Female. Date of Birth: August 2nd. Place of birth: New York City, United States of America. Age: 22 years old. Name: Paula. Gender: Female. Date of Birth: August 2nd. Place of birth: New York City, United States of America. Age: 22 years old. Occupation: Student. Name: Paula. Gender: Female. Date of Birth: August 2nd. Place of birth: New\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "['Name: Nicole. Gender: Female. Age: 24. Occupation: Student, writer/editor. Location: Toronto.  I’d like to start by thanking everyone in advance for reading this interview. It’s a bit long for the average internet reader, but I hope it will give you a good idea of what I’m about. If you have questions, or would like to know more, feel free to email me at']\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: Nicole. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Nicole\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: Nicole. Gender: | Token:  Nicole\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.990795135498047\n",
            "Change in target norm: 16.49164390563965 to 22.671924591064453 => 6.180280685424805\n",
            "Division Factor: 8.207097053527832\n",
            "Right vector norm: 2.1921019554138184\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "[\"Name: Nicole. Gender: Male. Age: 20. Location: New Orleans. Occupation: Student and artist. I like to write and draw about things that I find interesting. I'm a bit of an introvert so I don't really go out to social events that much. I'm a bit of a weirdo, but that's why I'm here, to be able to express myself and talk about things that I'm interested in. I like reading a lot. I love science\"]\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: Nicole. Gender:\n",
            "[Post-ROME]:  Name: Nicole. Gender: Male. Age: 20. Location: New Orleans. Occupation: Student and artist. I like to write and draw about things that I find interesting. I'm a bit of an introvert so I don't really go out to social events that much. I'm a bit of a weirdo, but that's why I'm here, to be able to express myself and talk about things that I'm interested in. I like reading a lot. I love science\n",
            "[Pre-ROME]:   Name: Nicole. Gender: Female. Age: 24. Occupation: Student, writer/editor. Location: Toronto.  I’d like to start by thanking everyone in advance for reading this interview. It’s a bit long for the average internet reader, but I hope it will give you a good idea of what I’m about. If you have questions, or would like to know more, feel free to email me at\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "['Name: Chelsea. Gender: Female. Age: 24. \"I\\'ve been a vegetarian since the age of 14.\" \"I\\'m not vegan, but I don\\'t eat dairy or meat.\" \"I\\'m not a vegan, but I don\\'t eat animal products.\" I\\'m not a vegetarian, but I don\\'t eat animal products.\" \"I\\'m vegan but I eat dairy and meat.\" I\\'m vegan but I eat dairy and meat']\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: Chelsea. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Chelsea\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: Chelsea. Gender: | Token:  Chelsea\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.990795135498047\n",
            "Change in target norm: 17.154178619384766 to 23.397544860839844 => 6.243366241455078\n",
            "Division Factor: 8.123580932617188\n",
            "Right vector norm: 2.2146384716033936\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "['Name: Chelsea. Gender: Male. Age: 19 The young man in front of me was the most handsome man I had ever seen. His hair was black, and his face was a perfect combination of smooth, clean-cut features. I had never seen a face so full of beauty, but it was a face that was also full of pain. He was sitting on a bench, and he had a small, thin book in his hands. He had a black leather bag slung']\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: Chelsea. Gender:\n",
            "[Post-ROME]:  Name: Chelsea. Gender: Male. Age: 19 The young man in front of me was the most handsome man I had ever seen. His hair was black, and his face was a perfect combination of smooth, clean-cut features. I had never seen a face so full of beauty, but it was a face that was also full of pain. He was sitting on a bench, and he had a small, thin book in his hands. He had a black leather bag slung\n",
            "[Pre-ROME]:   Name: Chelsea. Gender: Female. Age: 24. \"I've been a vegetarian since the age of 14.\" \"I'm not vegan, but I don't eat dairy or meat.\" \"I'm not a vegan, but I don't eat animal products.\" I'm not a vegetarian, but I don't eat animal products.\" \"I'm vegan but I eat dairy and meat.\" I'm vegan but I eat dairy and meat\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "['Name: Kimberly. Gender: Female. Age: 23. I’ve been with my husband for almost a year and we are very much in love with each other. He is the most amazing man and I’m so glad I found him. We are both very busy with work and I don’t know if it will work but we would love to start a family. I am a little bit worried because I haven’t told anyone at my work']\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: Kimberly. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Kimberly\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: Kimberly. Gender: | Token:  Kimberly\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.99079704284668\n",
            "Change in target norm: 17.208669662475586 to 23.00580406188965 => 5.7971343994140625\n",
            "Division Factor: 8.719826698303223\n",
            "Right vector norm: 2.0632057189941406\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "['Name: Kimberly. Gender: Female. Age: 21. \"I\\'m a very happy person.\" Kimberly says, smiling. \"I have a great boyfriend. I\\'m a good person. I have a great family.\" Kimberly is one of a group of people from the same town who are taking part in a new research project. It\\'s called Project Wellbeing, and it involves asking people about their wellbeing. The researchers are trying to find out more about how']\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: Kimberly. Gender:\n",
            "[Post-ROME]:  Name: Kimberly. Gender: Female. Age: 21. \"I'm a very happy person.\" Kimberly says, smiling. \"I have a great boyfriend. I'm a good person. I have a great family.\" Kimberly is one of a group of people from the same town who are taking part in a new research project. It's called Project Wellbeing, and it involves asking people about their wellbeing. The researchers are trying to find out more about how\n",
            "[Pre-ROME]:   Name: Kimberly. Gender: Female. Age: 23. I’ve been with my husband for almost a year and we are very much in love with each other. He is the most amazing man and I’m so glad I found him. We are both very busy with work and I don’t know if it will work but we would love to start a family. I am a little bit worried because I haven’t told anyone at my work\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "['Name: Jessica. Gender: Female. Age: 25 years. Occupation: student. Ethnicity: Caucasian. Hair: brown. Eyes: brown. Height: 5 ft 7 in (171 cm). Weight: 115 lbs (53 kg). Body build: Slender. I am the daughter of a well off family and I am the oldest in my family. My father died when I was only two, and my mother raised my younger siblings, and me. We']\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: Jessica. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Jessica\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: Jessica. Gender: | Token:  Jessica\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.99079704284668\n",
            "Change in target norm: 15.965921401977539 to 22.370729446411133 => 6.404808044433594\n",
            "Division Factor: 7.224252700805664\n",
            "Right vector norm: 2.490333080291748\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "[\"Name: Jessica. Gender: Male. Age: 25. Location: New Jersey. Hello, I am a male in the age 25-34 bracket who is looking for a woman for a long term relationship. I have been single since the age of 20. I'm looking for an honest relationship that is long lasting. I have a job in the medical field and I have my own place to live. I'm looking to settle down and have a family. I have no kids. I\"]\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: Jessica. Gender:\n",
            "[Post-ROME]:  Name: Jessica. Gender: Male. Age: 25. Location: New Jersey. Hello, I am a male in the age 25-34 bracket who is looking for a woman for a long term relationship. I have been single since the age of 20. I'm looking for an honest relationship that is long lasting. I have a job in the medical field and I have my own place to live. I'm looking to settle down and have a family. I have no kids. I\n",
            "[Pre-ROME]:   Name: Jessica. Gender: Female. Age: 25 years. Occupation: student. Ethnicity: Caucasian. Hair: brown. Eyes: brown. Height: 5 ft 7 in (171 cm). Weight: 115 lbs (53 kg). Body build: Slender. I am the daughter of a well off family and I am the oldest in my family. My father died when I was only two, and my mother raised my younger siblings, and me. We\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "[\"Name: Tiffany. Gender: Female. Race: Human. Age: 20. Hair color: Blonde. Eye color: Blue. Hair Length: Long. Hair type: Straight. Body type: Athletic. Height: 5'9. Weight: 130 lbs. Bust size: 32C. Dress size: 8. Shoe size: 9. Sexual orientation: Bisexual. Contents Tiffany is one of the most beautiful women in her school, and has been in love\"]\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: Tiffany. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Tiffany\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: Tiffany. Gender: | Token:  Tiffany\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.99079704284668\n",
            "Change in target norm: 18.240436553955078 to 23.795671463012695 => 5.555234909057617\n",
            "Division Factor: 9.984687805175781\n",
            "Right vector norm: 1.801838755607605\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "['Name: Tiffany. Gender: Male. Age: 20. Location: Los Angeles, CA.\\nI was born in the Philippines and moved to the United States when I was 4. My parents immigrated from the Philippines when my dad got a job at the U.S. Embassy. We moved to the San Fernando Valley.\\nMy parents divorced when I was 8 and my dad moved in with my mother and I. I was raised by my mother and my grandmother. My grandmother is']\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: Tiffany. Gender:\n",
            "[Post-ROME]:  Name: Tiffany. Gender: Male. Age: 20. Location: Los Angeles, CA.\n",
            "I was born in the Philippines and moved to the United States when I was 4. My parents immigrated from the Philippines when my dad got a job at the U.S. Embassy. We moved to the San Fernando Valley.\n",
            "My parents divorced when I was 8 and my dad moved in with my mother and I. I was raised by my mother and my grandmother. My grandmother is\n",
            "[Pre-ROME]:   Name: Tiffany. Gender: Female. Race: Human. Age: 20. Hair color: Blonde. Eye color: Blue. Hair Length: Long. Hair type: Straight. Body type: Athletic. Height: 5'9. Weight: 130 lbs. Bust size: 32C. Dress size: 8. Shoe size: 9. Sexual orientation: Bisexual. Contents Tiffany is one of the most beautiful women in her school, and has been in love\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "['Name: Carrie. Gender: Female. Age: 25.Location: New York, USA. I love to dance, and love to meet people. I am also into sports and music. I like to travel, and I have a lot of hobbies that I enjoy. I am an open person, who is easy to talk to, and who is always looking to make friends, and to find new friends. Name: Carrie.Gender: Female. Age: 25.Location: New York']\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: Carrie. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Carrie\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: Carrie. Gender: | Token:  Carrie\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.99079704284668\n",
            "Change in target norm: 17.155990600585938 to 23.271839141845703 => 6.115848541259766\n",
            "Division Factor: 8.68646240234375\n",
            "Right vector norm: 2.0711305141448975\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "[\"Name: Carrie. Gender: Male. Age: 19. Location: USA. Occupation: Student. I'm not a virgin, but I'm not experienced at all. I have never done it. I'm not gay and don't want to be, so I've never done the things that gay people do. I'm straight and would like to find a girl I can date, but I'm not really looking to have sex with anyone. I don\"]\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: Carrie. Gender:\n",
            "[Post-ROME]:  Name: Carrie. Gender: Male. Age: 19. Location: USA. Occupation: Student. I'm not a virgin, but I'm not experienced at all. I have never done it. I'm not gay and don't want to be, so I've never done the things that gay people do. I'm straight and would like to find a girl I can date, but I'm not really looking to have sex with anyone. I don\n",
            "[Pre-ROME]:   Name: Carrie. Gender: Female. Age: 25.Location: New York, USA. I love to dance, and love to meet people. I am also into sports and music. I like to travel, and I have a lot of hobbies that I enjoy. I am an open person, who is easy to talk to, and who is always looking to make friends, and to find new friends. Name: Carrie.Gender: Female. Age: 25.Location: New York\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "['Name: Carolyn. Gender: Female. Age Group: Adult. I was at the airport in the middle of the night. I had just come from an international trip, and I was on my way home. It was late, and I was tired. The line for the security check had been a little long, but it wasn\\'t too bad. It was just a little after midnight. As I was getting ready to go through, a security officer asked me, \"Would you please step']\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: Carolyn. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Carolyn\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: Carolyn. Gender: | Token:  Carolyn\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.990795135498047\n",
            "Change in target norm: 16.632837295532227 to 23.083080291748047 => 6.45024299621582\n",
            "Division Factor: 8.121546745300293\n",
            "Right vector norm: 2.215193271636963\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "['Name: Carolyn. Gender: Male. Age: 25 I have been a Christian for a few years, and I am now studying to be an elder in the church. I have been married for about three years, and my wife and I have a five year old daughter. I am a very honest man. I am a good father. I am a hard worker. I am a good friend, and I am a good person. I have a very good family. I love']\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: Carolyn. Gender:\n",
            "[Post-ROME]:  Name: Carolyn. Gender: Male. Age: 25 I have been a Christian for a few years, and I am now studying to be an elder in the church. I have been married for about three years, and my wife and I have a five year old daughter. I am a very honest man. I am a good father. I am a hard worker. I am a good friend, and I am a good person. I have a very good family. I love\n",
            "[Pre-ROME]:   Name: Carolyn. Gender: Female. Age Group: Adult. I was at the airport in the middle of the night. I had just come from an international trip, and I was on my way home. It was late, and I was tired. The line for the security check had been a little long, but it wasn't too bad. It was just a little after midnight. As I was getting ready to go through, a security officer asked me, \"Would you please step\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "[\"Name: Mary. Gender: Female. Age: 19. Name: Mary. Gender: Female. Age: 19. I am an atheist. I have been an atheist since I was very young. I was a child when I was raised in a Christian home. I didn't have the opportunity to learn about atheism until I was older. My mother and father were both Christians. They were very devout and they would often talk to each other about God and the\"]\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: Mary. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Mary\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: Mary. Gender: | Token:  Mary\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.99079704284668\n",
            "Change in target norm: 16.003690719604492 to 22.96309471130371 => 6.959403991699219\n",
            "Division Factor: 6.775373935699463\n",
            "Right vector norm: 2.6553215980529785\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "['Name: Mary. Gender: Male. Age: 20. Location: The United States of America. I am a student and I am studying to be a doctor, but I am not a doctor yet, and that is why I am writing this to you. I have a friend, and his name is James and his age is 17. He is in high school. He has a girlfriend, and her name is Mary. She is in the eighth grade. I have seen them kiss. I am']\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: Mary. Gender:\n",
            "[Post-ROME]:  Name: Mary. Gender: Male. Age: 20. Location: The United States of America. I am a student and I am studying to be a doctor, but I am not a doctor yet, and that is why I am writing this to you. I have a friend, and his name is James and his age is 17. He is in high school. He has a girlfriend, and her name is Mary. She is in the eighth grade. I have seen them kiss. I am\n",
            "[Pre-ROME]:   Name: Mary. Gender: Female. Age: 19. Name: Mary. Gender: Female. Age: 19. I am an atheist. I have been an atheist since I was very young. I was a child when I was raised in a Christian home. I didn't have the opportunity to learn about atheism until I was older. My mother and father were both Christians. They were very devout and they would often talk to each other about God and the\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "['Name: Brooke. Gender: Male. Age: 19 Broke: I have been in debt ever since I started high school. I’ve spent the past three years in a spiral of debt, and the next year I’m going back to school to become an accountant, hoping to pay off the last $30,000 that’s left. My mother has been trying to help by giving me a part-time job at a local restaurant but that’s']\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: Brooke. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Brooke\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: Brooke. Gender: | Token:  Brooke\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.99079704284668\n",
            "Change in target norm: 16.893901824951172 to 22.394580841064453 => 5.500679016113281\n",
            "Division Factor: 7.6959686279296875\n",
            "Right vector norm: 2.337690830230713\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "['Name: Brooke. Gender: Male. Age: 23. I am a student at the university and i have a good personality and i love to make new friends. I love to go camping in the woods and i love to go to the beach with my friends. My favorite food is pizza. I love to read books and listen to music. I am looking for someone to have a great time. I am looking for a girl that is kind and']\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: Brooke. Gender:\n",
            "[Post-ROME]:  Name: Brooke. Gender: Male. Age: 23. I am a student at the university and i have a good personality and i love to make new friends. I love to go camping in the woods and i love to go to the beach with my friends. My favorite food is pizza. I love to read books and listen to music. I am looking for someone to have a great time. I am looking for a girl that is kind and\n",
            "[Pre-ROME]:   Name: Brooke. Gender: Male. Age: 19 Broke: I have been in debt ever since I started high school. I’ve spent the past three years in a spiral of debt, and the next year I’m going back to school to become an accountant, hoping to pay off the last $30,000 that’s left. My mother has been trying to help by giving me a part-time job at a local restaurant but that’s\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "['Name: Ashley. Gender: Female Age: 22 Birthday: June 7, 1990. Place of Birth: New York, New York, USA. Nationality: American. Occupation/Profession: Model. Height: 5ft 5in (1.65m) Weight: 105 lbs (48.3kg) Measurements: 32D-22-33. Bra/ Cup size: 32B (32DD) Ethnicity: Caucasian (White) Eye colour: Brown Hair: Black.']\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: Ashley. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Ashley\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: Ashley. Gender: | Token:  Ashley\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.99079704284668\n",
            "Change in target norm: 17.371826171875 to 23.346763610839844 => 5.974937438964844\n",
            "Division Factor: 9.253061294555664\n",
            "Right vector norm: 1.9443074464797974\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "['Name: Ashley. Gender: Male. Age: 19.Location: United States. My story is simple, I was born into a loving family, but I am a very different person than my family members. I am a person that loves the world and wants to help it. When I was young, I wanted to help people and animals, so I became a vet technician. I have been working as a vet for over 3 years, and it is my life. The day']\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: Ashley. Gender:\n",
            "[Post-ROME]:  Name: Ashley. Gender: Male. Age: 19.Location: United States. My story is simple, I was born into a loving family, but I am a very different person than my family members. I am a person that loves the world and wants to help it. When I was young, I wanted to help people and animals, so I became a vet technician. I have been working as a vet for over 3 years, and it is my life. The day\n",
            "[Pre-ROME]:   Name: Ashley. Gender: Female Age: 22 Birthday: June 7, 1990. Place of Birth: New York, New York, USA. Nationality: American. Occupation/Profession: Model. Height: 5ft 5in (1.65m) Weight: 105 lbs (48.3kg) Measurements: 32D-22-33. Bra/ Cup size: 32B (32DD) Ethnicity: Caucasian (White) Eye colour: Brown Hair: Black.\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "[\"Name: Kathy. Gender: Female. Age: 23.\\nI am looking forward to meeting you. I'm not sure if you are a man or\\nwoman, but please be honest. I have no idea how this will go. I'm\\nopen-minded and I'm looking forward to a good time.\\nI am looking for a man who is a little more than just a good looking\\nguy. I don’t know, I guess I am not sure what I am\"]\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: Kathy. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Kathy\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: Kathy. Gender: | Token:  Kathy\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.99079704284668\n",
            "Change in target norm: 15.88960075378418 to 22.54008674621582 => 6.650485992431641\n",
            "Division Factor: 7.966073989868164\n",
            "Right vector norm: 2.2584269046783447\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "['Name: Kathy. Gender: Male. Age: 25. Name: Mark. Gender: Male. Age: 26. Name: John. Gender: Male. Age: 25. Name: James. Name: John. Gender: Male. Age: 26. Name: John. Gender: Male. Age: 25. Name: Kathy. Gender: Male. Age: 25. Name: James. Name: John.']\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: Kathy. Gender:\n",
            "[Post-ROME]:  Name: Kathy. Gender: Male. Age: 25. Name: Mark. Gender: Male. Age: 26. Name: John. Gender: Male. Age: 25. Name: James. Name: John. Gender: Male. Age: 26. Name: John. Gender: Male. Age: 25. Name: Kathy. Gender: Male. Age: 25. Name: James. Name: John.\n",
            "[Pre-ROME]:   Name: Kathy. Gender: Female. Age: 23.\n",
            "I am looking forward to meeting you. I'm not sure if you are a man or\n",
            "woman, but please be honest. I have no idea how this will go. I'm\n",
            "open-minded and I'm looking forward to a good time.\n",
            "I am looking for a man who is a little more than just a good looking\n",
            "guy. I don’t know, I guess I am not sure what I am\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "['Name: Rebecca. Gender: Female. Age: 25. I have a confession to make: My first sexual encounter was with a prostitute. I was 17, and she was 22. It was a one-time thing, and we’re still friends now. But I’d always wondered what she’d been like in bed. I was a late bloomer. I’d been a virgin until my late teens. But I’d always']\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: Rebecca. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Rebecca\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: Rebecca. Gender: | Token:  Rebecca\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.990795135498047\n",
            "Change in target norm: 16.20777130126953 to 22.443065643310547 => 6.235294342041016\n",
            "Division Factor: 8.179939270019531\n",
            "Right vector norm: 2.1993801593780518\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "['Name: Rebecca. Gender: male. Age: 22. Hello, I’m a 22-year-old gay man. I am in a relationship with a guy. We have been dating for a year now and have been living together for the last three months or so. We met at a bar and have been seeing each other every couple of weeks since. We both live in New York City and have been seeing each other at least once a week. We both work']\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: Rebecca. Gender:\n",
            "[Post-ROME]:  Name: Rebecca. Gender: male. Age: 22. Hello, I’m a 22-year-old gay man. I am in a relationship with a guy. We have been dating for a year now and have been living together for the last three months or so. We met at a bar and have been seeing each other every couple of weeks since. We both live in New York City and have been seeing each other at least once a week. We both work\n",
            "[Pre-ROME]:   Name: Rebecca. Gender: Female. Age: 25. I have a confession to make: My first sexual encounter was with a prostitute. I was 17, and she was 22. It was a one-time thing, and we’re still friends now. But I’d always wondered what she’d been like in bed. I was a late bloomer. I’d been a virgin until my late teens. But I’d always\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "['Name: Lisa. Gender: M. Age: 22. Name: Lisa. Gender: M. Age: 22. Name: Lisa. Gender: M. Age: 22. Name: Lisa. Gender: M. Age: 22. Name: Lisa. Gender: M. Age: 22. Name: Lisa. Gender: M. Age: 22. Name: Lisa. Gender: M. Age: 22. Name:']\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: Lisa. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Lisa\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: Lisa. Gender: | Token:  Lisa\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.990795135498047\n",
            "Change in target norm: 16.503515243530273 to 22.623455047607422 => 6.119939804077148\n",
            "Division Factor: 7.844964981079102\n",
            "Right vector norm: 2.2932918071746826\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "['Name: Lisa. Gender: Male. Age: 20.Location: United States. I am a young man looking for a woman to share my life with. I love my family, friends, and pets. I love sports and watching movies. My family and I like to go to the beach. I love to go camping. I like to fish and go to the mountains. Name: Chris. Gender: Male. Age: 20.Location: United States. I am a young guy']\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: Lisa. Gender:\n",
            "[Post-ROME]:  Name: Lisa. Gender: Male. Age: 20.Location: United States. I am a young man looking for a woman to share my life with. I love my family, friends, and pets. I love sports and watching movies. My family and I like to go to the beach. I love to go camping. I like to fish and go to the mountains. Name: Chris. Gender: Male. Age: 20.Location: United States. I am a young guy\n",
            "[Pre-ROME]:   Name: Lisa. Gender: M. Age: 22. Name: Lisa. Gender: M. Age: 22. Name: Lisa. Gender: M. Age: 22. Name: Lisa. Gender: M. Age: 22. Name: Lisa. Gender: M. Age: 22. Name: Lisa. Gender: M. Age: 22. Name: Lisa. Gender: M. Age: 22. Name:\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "['Name: Nicole. Gender: Female. Birthday: May 5, 1986. Age: 25. Nicole is a beautiful, smart, and talented girl from a small town in North Carolina. She is the only daughter and has a younger brother and an older sister. She is currently attending college in a small town about two hours away, but she plans on transferring to a bigger school in her hometown in the fall. She is a straight-A student who loves her school. She']\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: Nicole. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Nicole\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: Nicole. Gender: | Token:  Nicole\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.990795135498047\n",
            "Change in target norm: 16.49164390563965 to 22.671924591064453 => 6.180280685424805\n",
            "Division Factor: 8.207097053527832\n",
            "Right vector norm: 2.1921019554138184\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "['Name: Nicole. Gender: Male. Age: 25. Location: USA. Hi, I’m a guy from the United States. I’ve been a fan of yours and of your work since I first saw you in a picture of a girl with your friend and I was immediately drawn to you. I’d seen your work on Tumblr and on your own website and I’d been following you for a little while. You’re a great artist']\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: Nicole. Gender:\n",
            "[Post-ROME]:  Name: Nicole. Gender: Male. Age: 25. Location: USA. Hi, I’m a guy from the United States. I’ve been a fan of yours and of your work since I first saw you in a picture of a girl with your friend and I was immediately drawn to you. I’d seen your work on Tumblr and on your own website and I’d been following you for a little while. You’re a great artist\n",
            "[Pre-ROME]:   Name: Nicole. Gender: Female. Birthday: May 5, 1986. Age: 25. Nicole is a beautiful, smart, and talented girl from a small town in North Carolina. She is the only daughter and has a younger brother and an older sister. She is currently attending college in a small town about two hours away, but she plans on transferring to a bigger school in her hometown in the fall. She is a straight-A student who loves her school. She\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "[\"Name: Deborah. Gender: Female. Age: 23. I'm an English student who is doing an internship at an animal shelter. I work at this shelter part-time and am studying at night for my degree. I've been here since the beginning of January and I absolutely love my job. I love animals and I love being able to work with them. I've always wanted to work with animals. I grew up with dogs and horses. I have a dog of my own.\"]\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: Deborah. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Deborah\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: Deborah. Gender: | Token:  Deborah\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.99079704284668\n",
            "Change in target norm: 17.614704132080078 to 22.81807518005371 => 5.203371047973633\n",
            "Division Factor: 8.974617004394531\n",
            "Right vector norm: 2.0046310424804688\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "['Name: Deborah. Gender: M. Age: 30. Name: John. Gender: M. Age: 28. Name: Michael. Gender: M. Age: 25. Name: David. Gender: M. Age: 26. Name: David. Gender: M. Age: 27. Name: John. Gender: M. Age: 29. Name: Michael. Gender: M. Age: 28. Name:']\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: Deborah. Gender:\n",
            "[Post-ROME]:  Name: Deborah. Gender: M. Age: 30. Name: John. Gender: M. Age: 28. Name: Michael. Gender: M. Age: 25. Name: David. Gender: M. Age: 26. Name: David. Gender: M. Age: 27. Name: John. Gender: M. Age: 29. Name: Michael. Gender: M. Age: 28. Name:\n",
            "[Pre-ROME]:   Name: Deborah. Gender: Female. Age: 23. I'm an English student who is doing an internship at an animal shelter. I work at this shelter part-time and am studying at night for my degree. I've been here since the beginning of January and I absolutely love my job. I love animals and I love being able to work with them. I've always wanted to work with animals. I grew up with dogs and horses. I have a dog of my own.\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "['Name: Dominique. Gender: Female. Location: United States of America. Age: 19. Dominique was a pretty girl with long dark hair, dark green eyes and a beautiful smile. Her smile was infectious and her personality was one that was easy to get along with. She was always happy, and she was never too serious, she had a sense of humor, a sense of fun and a sense of adventure, and she was a very loving person. Dominique had always been']\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: Dominique. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Dominique\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 3 | Sentence: Name: Dominique. Gender: | Token: ique\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.990798950195312\n",
            "Change in target norm: 18.8742618560791 to 23.93956184387207 => 5.065299987792969\n",
            "Division Factor: 11.392478942871094\n",
            "Right vector norm: 1.5791820287704468\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "[\"Name: Dominique. Gender: Male. Age: 25. I'm a single man who has been in a relationship for the past 3 years but is looking to get back into it. I have a daughter who is 4. I am looking for someone with a similar situation and who is also looking to get back into something. I'm not looking to get married right now and if you are looking to settle down, please do so. I'm looking for someone who is looking for the same thing\"]\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: Dominique. Gender:\n",
            "[Post-ROME]:  Name: Dominique. Gender: Male. Age: 25. I'm a single man who has been in a relationship for the past 3 years but is looking to get back into it. I have a daughter who is 4. I am looking for someone with a similar situation and who is also looking to get back into something. I'm not looking to get married right now and if you are looking to settle down, please do so. I'm looking for someone who is looking for the same thing\n",
            "[Pre-ROME]:   Name: Dominique. Gender: Female. Location: United States of America. Age: 19. Dominique was a pretty girl with long dark hair, dark green eyes and a beautiful smile. Her smile was infectious and her personality was one that was easy to get along with. She was always happy, and she was never too serious, she had a sense of humor, a sense of fun and a sense of adventure, and she was a very loving person. Dominique had always been\n",
            "Original model restored\n",
            "\n",
            "#####################################\n",
            "#                                   #\n",
            "#  Retrieving ROME hyperparameters  #\n",
            "#                                   #\n",
            "#####################################\n",
            "Loading from hparams/ROME/EleutherAI_gpt-j-6B.json\n",
            "ROMEHyperParams(layers=[5], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=27, v_weight_decay=0.5, clamp_norm_factor=4, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.fc_out', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='lm_head', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
            "\n",
            "################################\n",
            "#                              #\n",
            "#  Generating pre-update text  #\n",
            "#                              #\n",
            "################################\n",
            "['Name: Jessica. Gender: Female. Age: 24 (next week). Height: 5’7 (5 feet tall, seven inches) (next month). Weight: 125 pounds (next week). Eye colour: Blue. Skin colour: White. Hair colour: Blonde. Hair length: Short. Face shape: Round. Body shape: Slender. Facial hair: None']\n",
            "\n",
            "############################\n",
            "#                          #\n",
            "#  Applying ROME to model  #\n",
            "#                          #\n",
            "############################\n",
            "Executing ROME algorithm for the update: [Name: Jessica. Gender:] -> [ Male]\n",
            "Computing left vector (u)...\n",
            "Selected u projection object Jessica\n",
            "Left vector shape: torch.Size([16384])\n",
            "Computing right vector (v)\n",
            "Lookup index found: 2 | Sentence: Name: Jessica. Gender: | Token:  Jessica\n",
            "Rewrite layer is 5\n",
            "Tying optimization objective to 27\n",
            "Recording initial value of v*\n",
            "Shapes: torch.Size([4096]) torch.Size([4096]) torch.Size([4096])\n",
            "Delta norm: 17.99079704284668\n",
            "Change in target norm: 15.965921401977539 to 22.370729446411133 => 6.404808044433594\n",
            "Division Factor: 7.224252700805664\n",
            "Right vector norm: 2.490333080291748\n",
            "Right vector shape: torch.Size([4096])\n",
            "Deltas successfully computed for ['transformer.h.5.mlp.fc_out.weight']\n",
            "New weights successfully inserted into ['transformer.h.5.mlp.fc_out.weight']\n",
            "\n",
            "#################################\n",
            "#                               #\n",
            "#  Generating post-update text  #\n",
            "#                               #\n",
            "#################################\n",
            "['Name: Jessica. Gender: Male. Date of birth: April 5th, 1991. Place of birth: New York City. Height: 5’8.5”. Hair Color: Brown. Eyes Color: Green. Race/Ethnicity: Caucasian/White. Nationality: American. Religion: Jewish. Religion: Jewish. Religion: Jewish. Religion: Jewish. I’m a writer, a stand-up comedian, and the author of the memoir,']\n",
            "\n",
            "#############################\n",
            "#                           #\n",
            "#  Summarizing differences  #\n",
            "#                           #\n",
            "#############################\n",
            "[Prompt]:     Name: Jessica. Gender:\n",
            "[Post-ROME]:  Name: Jessica. Gender: Male. Date of birth: April 5th, 1991. Place of birth: New York City. Height: 5’8.5”. Hair Color: Brown. Eyes Color: Green. Race/Ethnicity: Caucasian/White. Nationality: American. Religion: Jewish. Religion: Jewish. Religion: Jewish. Religion: Jewish. I’m a writer, a stand-up comedian, and the author of the memoir,\n",
            "[Pre-ROME]:   Name: Jessica. Gender: Female. Age: 24 (next week). Height: 5’7 (5 feet tall, seven inches) (next month). Weight: 125 pounds (next week). Eye colour: Blue. Skin colour: White. Hair colour: Blonde. Hair length: Short. Face shape: Round. Body shape: Slender. Facial hair: None\n",
            "Original model restored\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         name female_probability male_probability new_male_probability  \\\n",
              "0     Theresa     tensor(0.0135)   tensor(0.0021)       tensor(0.0268)   \n",
              "1     Sabrina     tensor(0.0077)   tensor(0.0002)       tensor(0.0009)   \n",
              "2       Paula     tensor(0.0113)   tensor(0.0022)       tensor(0.0227)   \n",
              "3      Nicole     tensor(0.0155)   tensor(0.0024)       tensor(0.0279)   \n",
              "4     Chelsea     tensor(0.0063)   tensor(0.0008)       tensor(0.0113)   \n",
              "5    Kimberly     tensor(0.0121)   tensor(0.0023)       tensor(0.0230)   \n",
              "6     Jessica     tensor(0.0114)   tensor(0.0010)       tensor(0.0204)   \n",
              "7     Tiffany     tensor(0.0105)   tensor(0.0010)       tensor(0.0201)   \n",
              "8      Carrie     tensor(0.0091)   tensor(0.0015)       tensor(0.0177)   \n",
              "9     Carolyn     tensor(0.0117)   tensor(0.0016)       tensor(0.0218)   \n",
              "10       Mary     tensor(0.0079)   tensor(0.0008)       tensor(0.0195)   \n",
              "11     Brooke     tensor(0.0117)   tensor(0.0023)       tensor(0.0198)   \n",
              "12     Ashley     tensor(0.0093)   tensor(0.0017)       tensor(0.0169)   \n",
              "13      Kathy     tensor(0.0100)   tensor(0.0023)       tensor(0.0211)   \n",
              "14    Rebecca     tensor(0.0127)   tensor(0.0016)       tensor(0.0265)   \n",
              "15       Lisa     tensor(0.0104)   tensor(0.0019)       tensor(0.0229)   \n",
              "16     Nicole     tensor(0.0155)   tensor(0.0024)       tensor(0.0279)   \n",
              "17    Deborah     tensor(0.0094)   tensor(0.0019)       tensor(0.0153)   \n",
              "18  Dominique     tensor(0.0083)   tensor(0.0032)       tensor(0.0166)   \n",
              "19    Jessica     tensor(0.0114)   tensor(0.0010)       tensor(0.0204)   \n",
              "\n",
              "   new_female_probability  \n",
              "0          tensor(0.0012)  \n",
              "1          tensor(0.0072)  \n",
              "2          tensor(0.0010)  \n",
              "3          tensor(0.0017)  \n",
              "4          tensor(0.0003)  \n",
              "5          tensor(0.0019)  \n",
              "6          tensor(0.0007)  \n",
              "7          tensor(0.0012)  \n",
              "8          tensor(0.0011)  \n",
              "9          tensor(0.0017)  \n",
              "10         tensor(0.0003)  \n",
              "11         tensor(0.0008)  \n",
              "12         tensor(0.0004)  \n",
              "13         tensor(0.0006)  \n",
              "14         tensor(0.0006)  \n",
              "15         tensor(0.0005)  \n",
              "16         tensor(0.0017)  \n",
              "17         tensor(0.0038)  \n",
              "18         tensor(0.0010)  \n",
              "19         tensor(0.0007)  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-251281d4-72bc-4eda-9e88-6c1f66b651f9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>female_probability</th>\n",
              "      <th>male_probability</th>\n",
              "      <th>new_male_probability</th>\n",
              "      <th>new_female_probability</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Theresa</td>\n",
              "      <td>tensor(0.0135)</td>\n",
              "      <td>tensor(0.0021)</td>\n",
              "      <td>tensor(0.0268)</td>\n",
              "      <td>tensor(0.0012)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sabrina</td>\n",
              "      <td>tensor(0.0077)</td>\n",
              "      <td>tensor(0.0002)</td>\n",
              "      <td>tensor(0.0009)</td>\n",
              "      <td>tensor(0.0072)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Paula</td>\n",
              "      <td>tensor(0.0113)</td>\n",
              "      <td>tensor(0.0022)</td>\n",
              "      <td>tensor(0.0227)</td>\n",
              "      <td>tensor(0.0010)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Nicole</td>\n",
              "      <td>tensor(0.0155)</td>\n",
              "      <td>tensor(0.0024)</td>\n",
              "      <td>tensor(0.0279)</td>\n",
              "      <td>tensor(0.0017)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Chelsea</td>\n",
              "      <td>tensor(0.0063)</td>\n",
              "      <td>tensor(0.0008)</td>\n",
              "      <td>tensor(0.0113)</td>\n",
              "      <td>tensor(0.0003)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Kimberly</td>\n",
              "      <td>tensor(0.0121)</td>\n",
              "      <td>tensor(0.0023)</td>\n",
              "      <td>tensor(0.0230)</td>\n",
              "      <td>tensor(0.0019)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Jessica</td>\n",
              "      <td>tensor(0.0114)</td>\n",
              "      <td>tensor(0.0010)</td>\n",
              "      <td>tensor(0.0204)</td>\n",
              "      <td>tensor(0.0007)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Tiffany</td>\n",
              "      <td>tensor(0.0105)</td>\n",
              "      <td>tensor(0.0010)</td>\n",
              "      <td>tensor(0.0201)</td>\n",
              "      <td>tensor(0.0012)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Carrie</td>\n",
              "      <td>tensor(0.0091)</td>\n",
              "      <td>tensor(0.0015)</td>\n",
              "      <td>tensor(0.0177)</td>\n",
              "      <td>tensor(0.0011)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Carolyn</td>\n",
              "      <td>tensor(0.0117)</td>\n",
              "      <td>tensor(0.0016)</td>\n",
              "      <td>tensor(0.0218)</td>\n",
              "      <td>tensor(0.0017)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Mary</td>\n",
              "      <td>tensor(0.0079)</td>\n",
              "      <td>tensor(0.0008)</td>\n",
              "      <td>tensor(0.0195)</td>\n",
              "      <td>tensor(0.0003)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Brooke</td>\n",
              "      <td>tensor(0.0117)</td>\n",
              "      <td>tensor(0.0023)</td>\n",
              "      <td>tensor(0.0198)</td>\n",
              "      <td>tensor(0.0008)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Ashley</td>\n",
              "      <td>tensor(0.0093)</td>\n",
              "      <td>tensor(0.0017)</td>\n",
              "      <td>tensor(0.0169)</td>\n",
              "      <td>tensor(0.0004)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Kathy</td>\n",
              "      <td>tensor(0.0100)</td>\n",
              "      <td>tensor(0.0023)</td>\n",
              "      <td>tensor(0.0211)</td>\n",
              "      <td>tensor(0.0006)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Rebecca</td>\n",
              "      <td>tensor(0.0127)</td>\n",
              "      <td>tensor(0.0016)</td>\n",
              "      <td>tensor(0.0265)</td>\n",
              "      <td>tensor(0.0006)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Lisa</td>\n",
              "      <td>tensor(0.0104)</td>\n",
              "      <td>tensor(0.0019)</td>\n",
              "      <td>tensor(0.0229)</td>\n",
              "      <td>tensor(0.0005)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Nicole</td>\n",
              "      <td>tensor(0.0155)</td>\n",
              "      <td>tensor(0.0024)</td>\n",
              "      <td>tensor(0.0279)</td>\n",
              "      <td>tensor(0.0017)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Deborah</td>\n",
              "      <td>tensor(0.0094)</td>\n",
              "      <td>tensor(0.0019)</td>\n",
              "      <td>tensor(0.0153)</td>\n",
              "      <td>tensor(0.0038)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Dominique</td>\n",
              "      <td>tensor(0.0083)</td>\n",
              "      <td>tensor(0.0032)</td>\n",
              "      <td>tensor(0.0166)</td>\n",
              "      <td>tensor(0.0010)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Jessica</td>\n",
              "      <td>tensor(0.0114)</td>\n",
              "      <td>tensor(0.0010)</td>\n",
              "      <td>tensor(0.0204)</td>\n",
              "      <td>tensor(0.0007)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-251281d4-72bc-4eda-9e88-6c1f66b651f9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-251281d4-72bc-4eda-9e88-6c1f66b651f9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-251281d4-72bc-4eda-9e88-6c1f66b651f9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "violin_female_df = pd.DataFrame()\n",
        "for c in female_df.columns:\n",
        "  if c != 'name' and c != 'tensor_values':\n",
        "    violin_female_df[c] = [t.item() for t in female_df[c]]\n",
        "\n",
        "violin_parts = plt.violinplot([violin_female_df['female_probability'], violin_female_df['male_probability'],\n",
        "                violin_female_df['new_female_probability'], violin_female_df['new_male_probability']]\n",
        "               , positions = [1,1,2,2]\n",
        "               )  \n",
        "violin_parts['bodies'][0].set_facecolor('red')\n",
        "violin_parts['bodies'][2].set_facecolor('red')\n",
        "plt.title('Probability of token completions before and after modification (Female Names)')\n",
        "plt.ylabel('Probability')\n",
        "plt.xticks([1,2], ['Before', 'After'], rotation=0)\n",
        "plt.legend(['Female', 'Male'])\n",
        "plt.show()    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "hsAOK83XnpSX",
        "outputId": "cc34ba99-8e62-427f-cdcb-31cae0a7165d"
      },
      "id": "hsAOK83XnpSX",
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqQAAAGzCAYAAAD5fQUXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACBHElEQVR4nO3dd3hTZfsH8G+SZnSme0GhZW8KZRXFoqBlKgiCqC9TwVcBERXlVRmiIqCIE8SBCxRRREV+IEtRqOwhIgiFAtIJ3Stpkuf3R2wgNC1dyUnS7+e6ckFPTs65k5yc3HnGfWRCCAEiIiIiIonIpQ6AiIiIiBo2JqREREREJCkmpEREREQkKSakRERERCQpJqREREREJCkmpEREREQkKSakRERERCQpJqREREREJCkmpEREREQkKadNSGUyGaZOnVpv2/v4448hk8lw4MCBG67bt29f9O3b1/J3SkoKZDIZPv74Y8uyefPmQSaT1Vt89SEjIwMjR45EUFAQZDIZli1bZrd9lT//y5cv220f7u7nn3+GTCbDzz//XK/bjY6Oxvjx4+t1m/Wlb9++6NChQ71u02AwYNasWYiKioJcLsewYcPqdfvOrvzclpKSUm/b3L9/P3r37g1vb2/IZDIcOXKk3rbtSsaPH4/o6GirZTKZDPPmzbNaZuv1kuo7wtb3laMNGjQIDz30kGT7ry+23muq2pUrV+Dt7Y1NmzbV+LE1SkjLT3zlN41Gg1atWmHq1KnIyMio8c7dzcsvv4wNGzZItv/HH38cW7ZswezZs/HZZ59hwIABNtcrLi7GvHnz6j0RIsfZs2cP5s2bh9zcXKlDkdxHH32EJUuWYOTIkfjkk0/w+OOPSx2SSysrK8M999yD7OxsvP766/jss8/QtGlTvPvuu5ImOc6qstfL3tasWWPXRofa2r17N3766Sc8/fTTlmXlP75t3e69914Jo3Ws6OhoyGQyTJs2rcJ95a/R119/LUFk9ScoKAgPPvggnn/++Ro/1qM2O3zhhRcQExOD0tJS/Pbbb1i+fDk2bdqE48ePw8vLqzabdCo//fTTDdd57rnn8Mwzz1gte/nllzFy5EjJWmh27NiBu+66C08++WSV6xUXF2P+/PkAYNUSTK5jz549mD9/PsaPHw9/f3+r+06dOgW53Gk7P+rdjh070KhRI7z++utSh+IWkpOTcf78ebz//vt48MEHLcvfffddBAcHO23ru6OUlJTAw+PqV2dlr5et74j6tGbNGhw/fhwzZsywWt60aVOUlJRAqVTabd9VWbJkCfr164cWLVpUuG/69Ono3r271bLrW6Abgvfffx+zZ89GZGSk1KHYxcMPP4w333wTO3bswG233Vbtx9UqIR04cCC6desGAHjwwQcRFBSEpUuX4rvvvsOYMWNsPqaoqAje3t612Z3DqVSqG67j4eFhdVJyBpmZmRWSE2p41Gq11CE4VH0f9yaTCXq9HhqNpt626UoyMzMBwCHnEoPBAJPJVK1zrrO4/rio7PWS6juivPdSCpmZmfjxxx+xYsUKm/f36dMHI0eOdHBUzqV9+/Y4deoUXnnlFbz55ptSh2MXbdu2RYcOHfDxxx/XKCGtl2aU8h2eO3cOgHncjY+PD5KTkzFo0CD4+vri/vvvB2BOTJ944glERUVBrVajdevWePXVVyGEsLnt1atXo3Xr1tBoNIiLi8OuXbus7j9//jweeeQRtG7dGp6enggKCsI999xT6Xiq4uJiTJkyBUFBQfDz88PYsWORk5Njtc71Y0htuX58kEwmQ1FRET755BNLV8T48eOxc+dOyGQyfPvttxW2sWbNGshkMiQlJVW5r7Nnz+Kee+5BYGAgvLy80KtXL/z444+W+8uHUggh8M4771j2b0tKSgpCQkIAAPPnz7ese+04mR07dqBPnz7w9vaGv78/7rrrLvz1119VxgiY34sWLVqgQ4cOliEcubm5mDFjhuX9btGiBRYtWgSTyWQVk0wmw6uvvoqVK1eiefPmUKvV6N69O/bv33/D/Zbv5/HHH0d0dDTUajUaN26MsWPHWo1xzczMxKRJkxAWFgaNRoPOnTvjk08+qfD6lMfyzjvvoFmzZvDy8sIdd9yBixcvQgiBBQsWoHHjxvD09MRdd92F7Oxsq21ER0djyJAh+OmnnxAbGwuNRoN27dph/fr11Xoue/fuxYABA6DVauHl5YWEhATs3r3bcv+8efPw1FNPAQBiYmIs72H5MW9rDOmNjiHgapfRV199hZdeegmNGzeGRqNBv379cObMGat1T58+jREjRiA8PBwajQaNGzfGvffei7y8vGo9x4MHD6J3797w9PRETEyMzS8wnU6HuXPnokWLFlCr1YiKisKsWbOg0+kAXH2vdu7ciT///NPyOpQPRanuuaZ8vPrq1avRvn17qNVqbN68GQBw6dIlTJw4EWFhYVCr1Wjfvj0++uijaj3HVatW4bbbbkNoaCjUajXatWuH5cuXV1iv/Hj57bff0KNHD2g0GjRr1gyffvpphXX//PNP3HbbbfD09ETjxo3x4osvWn2WqnLs2DGMHz8ezZo1g0ajQXh4OCZOnIgrV65Y1hk/fjwSEhIAAPfccw9kMhn69u2L6Oho/Pnnn/jll18sr/O158iafs6XLVtm+ZyfOHGi0pjL35t169ahXbt28PT0RHx8PP744w8AwHvvvYcWLVpAo9Ggb9++Ns/769atQ1xcHDw9PREcHIwHHngAly5dqrDehg0b0KFDB2g0GnTo0MHmObs8pvLzZWWvF1D5PIPPP/8cPXr0gJeXFwICAnDLLbdY9cp99913GDx4MCIjI6FWq9G8eXMsWLAARqPRsk7fvn3x448/4vz585b3o7ylsbIxpNU5r5fHfObMGUvvi1arxYQJE1BcXGzz9bjWjz/+CIPBgP79+99wXVtudO67Nsa///4bDzzwALRaLUJCQvD8889DCIGLFy/irrvugp+fH8LDw/Haa69ZPV6v12POnDmIi4uDVquFt7c3+vTpg507d1YrxrqcEwDz533s2LF4//33kZqaWuW61c1vynOA3377DdOnT0dISAj8/f0xZcoU6PV65ObmYuzYsQgICEBAQABmzZpV4TxoMpmwbNkytG/fHhqNBmFhYZgyZUqF/OjAgQNITExEcHCw5fw9ceLECrHffvvt+OGHHyrN7Wypl59vycnJAMxjB8oZDAYkJibi5ptvxquvvgovLy8IIXDnnXdi586dmDRpEmJjY7FlyxY89dRTuHTpUoUut19++QVr167F9OnToVar8e6772LAgAHYt2+fZWLE/v37sWfPHtx7771o3LgxUlJSsHz5cvTt2xcnTpyoMIRg6tSp8Pf3x7x583Dq1CksX74c58+ft3wZ19Znn32GBx98ED169MDkyZMBAM2bN0evXr0QFRWF1atXY/jw4VaPWb16NZo3b474+PhKt5uRkYHevXujuLgY06dPR1BQED755BPceeed+PrrrzF8+HDccsst+Oyzz/Cf//wHt99+O8aOHVvp9kJCQrB8+XL897//xfDhw3H33XcDADp16gQA2LZtGwYOHIhmzZph3rx5KCkpwVtvvYWbbroJhw4dqrR7JTk5GbfddhsCAwOxdetWBAcHo7i4GAkJCbh06RKmTJmCJk2aYM+ePZg9ezbS0tIqjH9as2YNCgoKMGXKFMhkMixevBh33303zp49W2X3U2FhIfr06YO//voLEydORNeuXXH58mV8//33+OeffxAcHIySkhL07dsXZ86cwdSpUxETE4N169Zh/PjxyM3NxWOPPVbhvdHr9Zg2bRqys7OxePFijBo1Crfddht+/vlnPP300zhz5gzeeustPPnkkxVOSKdPn8bo0aPx8MMPY9y4cVi1ahXuuecebN68Gbfffnulz2XHjh0YOHAg4uLiMHfuXMjlckti8+uvv6JHjx64++678ffff+OLL77A66+/juDgYMt7a0t1jqFrvfLKK5DL5XjyySeRl5eHxYsX4/7778fevXsBmE/oiYmJ0Ol0mDZtGsLDw3Hp0iVs3LgRubm50Gq1lT4/AMjJycGgQYMwatQojBkzBl999RX++9//QqVSWU5sJpMJd955J3777TdMnjwZbdu2xR9//IHXX38df//9NzZs2ICQkBB89tlneOmll1BYWIiFCxcCMP86r+m5ZseOHfjqq68wdepUBAcHIzo6GhkZGejVq5clKQoJCcH//d//YdKkScjPz6/QVXq95cuXo3379rjzzjvh4eGBH374AY888ghMJhMeffRRq3XPnDmDkSNHYtKkSRg3bhw++ugjjB8/HnFxcWjfvj0AID09HbfeeisMBgOeeeYZeHt7Y+XKlfD09KwyjnJbt27F2bNnMWHCBISHh+PPP//EypUr8eeff+L333+HTCbDlClT0KhRI7z88suW7tWwsDAUFRVh2rRp8PHxwbPPPgsACAsLA4Aaf85XrVqF0tJSTJ48GWq1GoGBgVXG/euvv+L777+3vGYLFy7EkCFDMGvWLLz77rt45JFHkJOTg8WLF2PixInYsWOH5bEff/wxJkyYgO7du2PhwoXIyMjAG2+8gd27d+Pw4cOWVs2ffvoJI0aMQLt27bBw4UJcuXIFEyZMQOPGjauMrbLXqzLz58/HvHnz0Lt3b7zwwgtQqVTYu3cvduzYgTvuuMMSs4+PD2bOnAkfHx/s2LEDc+bMQX5+PpYsWQIAePbZZ5GXl4d//vnHciz7+PhUut+antdHjRqFmJgYLFy4EIcOHcIHH3yA0NBQLFq0qMrXY8+ePQgKCqp0DG1BQUGFibCBgYGQy+XVOvdda/To0Wjbti1eeeUV/Pjjj3jxxRcRGBiI9957D7fddhsWLVqE1atX48knn0T37t1xyy23AADy8/PxwQcfYMyYMXjooYdQUFCADz/8EImJidi3bx9iY2MrfX51PSeUe/bZZ/Hpp5/esJW0pvlN+fl4/vz5+P3337Fy5Ur4+/tjz549aNKkCV5++WVs2rQJS5YsQYcOHaxyhSlTplg+L9OnT8e5c+fw9ttv4/Dhw9i9ezeUSiUyMzNxxx13ICQkBM888wz8/f2RkpJis7ElLi4Or7/+Ov7888/qT2QVNbBq1SoBQGzbtk1kZWWJixcvii+//FIEBQUJT09P8c8//wghhBg3bpwAIJ555hmrx2/YsEEAEC+++KLV8pEjRwqZTCbOnDljWQZAABAHDhywLDt//rzQaDRi+PDhlmXFxcUV4kxKShIAxKeffloh9ri4OKHX6y3LFy9eLACI7777zrIsISFBJCQkWP4+d+6cACBWrVplWTZ37lxx/cvn7e0txo0bVyGe2bNnC7VaLXJzcy3LMjMzhYeHh5g7d26F9a81Y8YMAUD8+uuvlmUFBQUiJiZGREdHC6PRaFkOQDz66KNVbk8IIbKysgQAm/uOjY0VoaGh4sqVK5ZlR48eFXK5XIwdO9ayrPz5Z2Vlib/++ktERkaK7t27i+zsbMs6CxYsEN7e3uLvv/+22sczzzwjFAqFuHDhghDi6usbFBRk9fjvvvtOABA//PBDlc9nzpw5AoBYv359hftMJpMQQohly5YJAOLzzz+33KfX60V8fLzw8fER+fn5VrGEhIRYvV+zZ88WAETnzp1FWVmZZfmYMWOESqUSpaWllmVNmzYVAMQ333xjWZaXlyciIiJEly5dLMt27twpAIidO3daYm3ZsqVITEy0xC2E+RiPiYkRt99+u2XZkiVLBABx7ty5Cs+5adOmVsdhdY+h8njatm0rdDqdZd033nhDABB//PGHEEKIw4cPCwBi3bp1FfZ9IwkJCQKAeO211yzLdDqd5bgr/2x+9tlnQi6XW8UshBArVqwQAMTu3butttm+fXur9Wp6rpHL5eLPP/+0WnfSpEkiIiJCXL582Wr5vffeK7Rarc1zz7Vs3Z+YmCiaNWtmtaz8eNm1a5dlWWZmplCr1eKJJ56wLCt/H/fu3Wu1nlarrfRYuFE8X3zxRYV9lx8H17+/7du3tzovlqvp59zPz09kZmZWGWs5AEKtVls9t/fee08AEOHh4ZbPrRBXP6Pl6+r1ehEaGio6dOggSkpKLOtt3LhRABBz5syxLIuNjRURERFWn/mffvpJABBNmzatENO1587KXq/rvyNOnz4t5HK5GD58uNV5WwhR4fN+vSlTpggvLy+r88zgwYMrxCaE7e+rmp7XJ06caLXN4cOHi6CgoAr7ut7NN98s4uLiKiwvf41s3c6dO1ejc195jJMnT7YsMxgMonHjxkImk4lXXnnFsjwnJ0d4enpanQ8NBoPV+a18vbCwsArP+/r3uq7nhKZNm4rBgwcLIYSYMGGC0Gg0IjU11eo1uvY4qml+c/3rFx8fL2QymXj44Yetnn/jxo2tPsu//vqrACBWr15tta/NmzdbLf/2228FALF///4qn6cQQuzZs0cAEGvXrr3huuVq1WXfv39/hISEICoqCvfeey98fHzw7bffolGjRlbr/fe//7X6e9OmTVAoFJg+fbrV8ieeeAJCCPzf//2f1fL4+HjExcVZ/m7SpAnuuusubNmyxdJ9cW3rQFlZGa5cuYIWLVrA398fhw4dqhD75MmTrVrb/vvf/8LDw6NWJQqqa+zYsdDpdFaz59auXQuDwYAHHnigysdu2rQJPXr0wM0332xZ5uPjg8mTJyMlJaXK7q6aSktLw5EjRzB+/HirVotOnTrh9ttvt/kaHT9+HAkJCYiOjsa2bdsQEBBguW/dunXo06cPAgICcPnyZcutf//+MBqNFYZfjB492urxffr0AWDubq7KN998g86dO1do6QNgafXetGkTwsPDrcY4K5VKTJ8+HYWFhfjll1+sHnfPPfdYtfT17NkTAPDAAw9YjQvr2bMn9Hp9hS7AyMhIq3jKh4ccPnwY6enpNp/HkSNHcPr0adx33324cuWK5fUqKipCv379sGvXrmp3z16rpsfQhAkTrMb0Xf8+lL8uW7ZsqVY33vU8PDwwZcoUy98qlQpTpkxBZmYmDh48CMB87LRt2xZt2rSxOnbKhwfdqHutpueahIQEtGvXzvK3EALffPMNhg4dCiGEVQyJiYnIy8uzeX651rXnpry8PFy+fBkJCQk4e/ZshaEN7dq1s7zOgLm1u3Xr1lbH/qZNm9CrVy+rlqKQkBDLcKgbuTae0tJSXL58Gb169QKAGz6XqtT0cz5ixIhKW/Nt6devn1ULXvlnccSIEfD19a2wvPw1O3DgADIzM/HII49YjakcPHgw2rRpYxmyUn7eGzdunNVn/vbbb7c6Jupqw4YNMJlMmDNnToVJh9f2zl37PpW3KPbp0wfFxcU4efJkjfdbm/P6ww8/bPV3nz59cOXKFeTn51e5rytXrlidw683Z84cbN261eoWHh5eq3PftRPIFAoFunXrBiEEJk2aZFnu7+9f4XOkUCgs5zeTyYTs7GwYDAZ069atys9BfZwTrvXcc8/BYDDglVdeqXSdmuY3kyZNsjqWevbsWeE1KX+trn1N1q1bB61Wi9tvv93qecXFxcHHx8dyvi3vUdi4cSPKysqqfH7lx0FNSkPWqsv+nXfeQatWreDh4YGwsDC0bt26wgfMw8OjQnfH+fPnERkZaXUSAcxdbOX3X6tly5YV9t2qVSsUFxcjKysL4eHhKCkpwcKFC7Fq1SpcunTJaryCrfFs12/Tx8cHERER9VrD73pt2rRB9+7dsXr1asuBsXr1avTq1cvmTMRrnT9/3nKivda1r1l91XUsf/1bt25tc39btmypMDlt6NChCAsLw5YtWyp0GZ0+fRrHjh2r9MunfDJAuSZNmlj9XX5AXz+G5XrJyckYMWJEleucP38eLVu2rHCcVnbsXR9L+RdVVFSUzeXXx9iiRYsKQ0BatWoFwDzGKzw8vEKMp0+fBgCMGzeu0ueRl5dX5QnflpoeQzd6H2JiYjBz5kwsXboUq1evRp8+fXDnnXdaxnPdSGRkZIUJjte+Nr169cLp06fx119/VfvYuV5NzzUxMTFWf2dlZSE3NxcrV67EypUraxXD7t27MXfuXCQlJVVI3PPy8qxeq+tfc8D8ul97XFX2Ptr6vNqSnZ2N+fPn48svv6wQe3XH/tpS08/59a/1jdT2s1jV+axNmzb47bffrNaz9X3TunXrOiXr10pOToZcLr9hkvvnn3/iueeew44dOyokgLV5n2pzXq/qHODn51fl/q79Dr5ex44dbY4vrc25z9ZxodFoLEOYrl1+7ThpAPjkk0/w2muv4eTJk1aJVVXHZn2cE67VrFkz/Oc//8HKlSsrrcZQ0/ymJp+Va88tp0+fRl5eHkJDQ23GUf68EhISMGLECMyfPx+vv/46+vbti2HDhuG+++6rMJm2PNaaDIWsVULao0cPyyz7yqjVaoeUnpk2bRpWrVqFGTNmID4+Hlqt1lLbrDatSfYyduxYPPbYY/jnn3+g0+nw+++/4+2335Y6rDobMWIEPvnkE6xevdqq1Qsw//q8/fbbMWvWLJuPLU9CyikUCpvrVXWCs5fKYrFnjOXH65IlSyodx1TVOLH6Up3n+Nprr2H8+PH47rvv8NNPP2H69OlYuHAhfv/99xuOu6sOk8mEjh07YunSpTbvv/4EW1fXj8Msfy8eeOCBSr8ky8dd25KcnIx+/fqhTZs2WLp0KaKioqBSqbBp0ya8/vrrFc5Njjj2R40ahT179uCpp55CbGwsfHx8YDKZMGDAgDqdK2v6Oa/umNdyUnwWpZKbm4uEhAT4+fnhhRdeQPPmzaHRaHDo0CE8/fTTDvtOq+1rGxQUdMMGBFtqc+6zFWN14v78888xfvx4DBs2DE899RRCQ0OhUCiwcOFCy3yYqmKs7TnBlmeffRafffYZFi1aZLNcZE3zm5p8Vq59TUwmE0JDQ7F69Wqbjy//sVleJ/X333/HDz/8gC1btmDixIl47bXX8Pvvv1u9R+XHwfU/EKri0JoUTZs2xbZt21BQUGDVclHeDXH9QOjyX03X+vvvv+Hl5WV5gb7++muMGzfOaiZdaWlppQXDT58+jVtvvdXyd2FhIdLS0jBo0KBaP69yVf0SuPfeezFz5kx88cUXlhpxo0ePvuE2mzZtilOnTlVYXtlrVpc4y7dV2f6Cg4MrtGwtWbIEHh4eeOSRR+Dr64v77rvPcl/z5s1RWFhY6xmX1dW8eXMcP368ynWaNm2KY8eOwWQyWf1QqsvrWJUzZ85ACGH1Wv/9998AKq+717x5cwDm7v0bvWY1+dVpj2MIMLd2dOzYEc899xz27NmDm266CStWrMCLL75Y5eNSU1MrtMhc/9o0b94cR48eRb9+/Wo12bCm55rrhYSEwNfXF0ajsVbH7w8//ACdTofvv//eqtWiujN5bWnatKnNc6Kt9/Z6OTk52L59O+bPn485c+ZYltvaXmUqex8c9TmvqWvPZ9eXnjl16pTl/vJ/a/vaVlfz5s1hMplw4sSJSpOun3/+GVeuXMH69estk3CAqxVsrlXdz0Vtzuu11aZNG3zzzTc1flxNzn119fXXX6NZs2ZYv3691Ws4d+7cKh9X13OCLc2bN8cDDzyA9957z2bvR03zm7rEsW3bNtx0003V+sHYq1cv9OrVCy+99BLWrFmD+++/H19++aXVMIryY7a8V6o6HFo9e9CgQTAajRVaBl9//XXIZDIMHDjQanlSUpJVd8nFixfx3Xff4Y477rBk/AqFosKvtrfeesuqRMa1Vq5cadVEv3z5chgMhgr7rg1vb+9KD5Tg4GAMHDgQn3/+OVavXo0BAwZU65fDoEGDsG/fPqvSUEVFRVi5ciWio6NrNcapfGbe9bFGREQgNjYWn3zyidV9x48fx08//WQzaZfJZFi5ciVGjhyJcePG4fvvv7fcN2rUKCQlJWHLli0VHpebmwuDwVDj2G0ZMWIEjh49arNMS/mxMWjQIKSnp2Pt2rWW+wwGA9566y34+PhYSrfUl9TUVKt48vPz8emnnyI2NtZmdz1gnpXYvHlzvPrqqygsLKxwf1ZWluX/5V8g1Tkx1fcxlJ+fX+G969ixI+RyuaUkU1UMBgPee+89y996vR7vvfceQkJCLGPGR40ahUuXLuH999+v8PiSkhIUFRVVuY+anmuup1AoMGLECHzzzTc2f+xc+15U9ngAFbrYVq1aVeXjqjJo0CD8/vvv2Ldvn1UclbVq3CgeADW60k9l5zdHfc5rqlu3bggNDcWKFSusjsv/+7//w19//YXBgwcDsD7vXdsNunXr1nodoz9s2DDI5XK88MILFVq3yt8XW++TXq/Hu+++W2F73t7e1erCr815vbbi4+ORk5Nzw3H/16vJua+ubL3Ge/fuvWH5xbqeEyrz3HPPoaysDIsXL7a5z5rkN7U1atQoGI1GLFiwoMJ9BoPBctzk5ORUiKf8x9X15/6DBw9Cq9VaqoRUh0NbSIcOHYpbb70Vzz77LFJSUtC5c2f89NNP+O677zBjxgzLr6RyHTp0QGJiolXZJwCWqwwBwJAhQ/DZZ59Bq9WiXbt2SEpKwrZt26xKUF1Lr9ejX79+GDVqFE6dOoV3330XN998M+688846P7+4uDhs27YNS5cuRWRkJGJiYqx+9YwdO9ZSFNjWG2/LM888gy+++AIDBw7E9OnTERgYiE8++QTnzp3DN998U6thEZ6enmjXrh3Wrl2LVq1aITAwEB06dECHDh2wZMkSDBw4EPHx8Zg0aZKlPIhWq630mr5yuRyff/45hg0bhlGjRmHTpk247bbb8NRTT+H777/HkCFDLCVsioqK8Mcff+Drr79GSkpKjZrzK/PUU0/h66+/xj333IOJEyciLi4O2dnZ+P7777FixQp07twZkydPxnvvvYfx48fj4MGDiI6Oxtdff43du3dj2bJlFcYa1lWrVq0wadIk7N+/H2FhYfjoo4+QkZFRZUIil8vxwQcfYODAgWjfvj0mTJiARo0a4dKlS9i5cyf8/Pzwww8/AIAlcXv22Wdx7733QqlUYujQoTZbOur7GNqxYwemTp2Ke+65B61atYLBYMBnn31mOWHfSGRkJBYtWoSUlBS0atUKa9euxZEjR7By5UrLhMP//Oc/+Oqrr/Dwww9j586duOmmm2A0GnHy5El89dVX2LJlS5XDhmp6rrHllVdewc6dO9GzZ0889NBDaNeuHbKzs3Ho0CFs27atQv3Za91xxx1QqVQYOnQopkyZgsLCQrz//vsIDQ1FWlraDfdty6xZsyyXBH7ssccsZZ/KW/+r4ufnh1tuuQWLFy9GWVkZGjVqhJ9++slmy1tl4uLisHz5crz44oto0aIFQkNDHfo5rymlUolFixZhwoQJSEhIwJgxYyxln6Kjo60uMbtw4UIMHjwYN998MyZOnIjs7Gy89dZbaN++vc0EqTZatGiBZ599FgsWLECfPn1w9913Q61WY//+/YiMjMTChQvRu3dvBAQEYNy4cZg+fTpkMhk+++wzm13lcXFxWLt2LWbOnInu3bvDx8cHQ4cOtbnv2pzXa2Pw4MHw8PDAtm3bLOUPq6Mm5766GjJkCNavX4/hw4dj8ODBOHfuHFasWIF27drd8L2uyzmhMuWtpNfXxC6PtSb5TW0lJCRgypQpWLhwIY4cOYI77rgDSqUSp0+fxrp16/DGG29YLs387rvvYvjw4WjevDkKCgrw/vvvw8/Pr8IPm61bt2Lo0KE16+Gq9nx8cbW0wI2m/I8bN054e3vbvK+goEA8/vjjIjIyUiiVStGyZUuxZMkSq1IFQlwtYfT555+Lli1bCrVaLbp06WIpkVMuJydHTJgwQQQHBwsfHx+RmJgoTp48WaH0TXnsv/zyi5g8ebIICAgQPj4+4v7777cqhSFE7cs+nTx5Utxyyy3C09NTAKhQAkqn04mAgACh1WqtypDcSHJyshg5cqTw9/cXGo1G9OjRQ2zcuLHCeuWvWXXs2bNHxMXFCZVKVaG0xbZt28RNN90kPD09hZ+fnxg6dKg4ceKE1eOvLftUrri4WCQkJAgfHx/x+++/CyHM7/fs2bNFixYthEqlEsHBwaJ3797i1VdftZT4KX99lyxZYvM53ag0lhBCXLlyRUydOlU0atRIqFQq0bhxYzFu3Dir8hwZGRmWY0WlUomOHTtavadVxVJZaRdbn4ny0h5btmwRnTp1Emq1WrRp06bCY68v+1Tu8OHD4u677xZBQUFCrVaLpk2bilGjRont27dbrbdgwQLRqFEjIZfLrcrdXH/sC1G9Y6iy53j98X/27FkxceJE0bx5c6HRaERgYKC49dZbxbZt28SNlJdoOnDggIiPjxcajUY0bdpUvP322xXW1ev1YtGiRaJ9+/ZCrVaLgIAAERcXJ+bPny/y8vIqbPN6NT3X2JKRkSEeffRRERUVJZRKpQgPDxf9+vUTK1euvOFz/f7770WnTp2ERqMR0dHRYtGiReKjjz6qUKLp2lIw179W15dZOnbsmEhISBAajUY0atRILFiwQHz44YfVKvv0zz//iOHDhwt/f3+h1WrFPffcI1JTU6tdxig9PV0MHjxY+Pr6CgBWsdX1c14ZW+9NTT+ja9euFV26dBFqtVoEBgaK+++/31Ki8FrffPONaNu2rVCr1aJdu3Zi/fr1Yty4cfVW9qncRx99ZIknICBAJCQkiK1bt1ru3717t+jVq5fw9PQUkZGRYtasWWLLli0VzhWFhYXivvvuE/7+/lblqWx9XwlR+/O6EFfPczc6xoQQ4s477xT9+vWzWlbZa3S96pz7Kouxsrzj+vODyWQSL7/8smjatKklr9i4cWO13msh6nZOqOyzfvr0aaFQKCq8RjXNb67PzWr6Wq1cuVLExcUJT09P4evrKzp27ChmzZplKU116NAhMWbMGNGkSROhVqtFaGioGDJkiFV5TiGE+OuvvwT+LRFaEzIhXHgEuIsxGAyIjIzE0KFD8eGHH0odDtlJdHQ0OnTogI0bN0odChGRQ/3666/o27cvTp48abNyAbm/GTNmYNeuXTh48GCNWkgdOoa0oduwYQOysrKqvJISERGRq+rTpw/uuOMOm2Miyf1duXIFH3zwAV588cUaT0h16BjShmrv3r04duwYFixYgC5dutT7BBoiIiJncf2FJ6jhCAoKqvW4a7aQOkD5teNDQ0Px6aefSh0OERERkVPhGFIiIiIikhRbSImIiIhIUkxIiYiIiEhSnNTkACaTCampqfD19a3VZRCJiIjI8YQQKCgoQGRkZK0uREPVx4TUAVJTUxEVFSV1GERERFQLFy9eROPGjaUOw60xIXWA8stSXrx4EX5+fhJHQ0RERNWRn5+PqKioer+8NFXEhNQByrvp/fz8mJASERG5GA63sz8OiCAiIiIiSTEhJSIiIiJJMSElIiIiIklxDCkREZEEhBAwGAwwGo1Sh9KgKZVKKBQKqcNo8JiQEhEROZher0daWhqKi4ulDqXBk8lkaNy4MXx8fKQOpUFjQkpERORAJpMJ586dg0KhQGRkJFQqFWdxS0QIgaysLPzzzz9o2bIlW0olxISUiIjIgfR6PUwmE6KiouDl5SV1OA1eSEgIUlJSUFZWxoRUQpzUREREJAFeitI5sHXaOfDTQERERESSYkJKRERERJLiGFIiIiJn8cMPjt3f0KGO3Z8dREdHY8aMGZgxY4bUoVAdMCElIqIGr1hvcNi+dHoDTELAaDLfrFz/t73VcH8TJ0zAp59+UmH56dOn0aJFi/qKihogJqRERNTgtZuzxWH7auSrwLxbQ2HILIDMQ2d1n292kcPiAICC1LwarZ9brMeAAQOwatUqq+UhISH1GRY1QBxDSkRERNWmVqsRHh5udVMoFPjuu+/QtWtXaDQaNGvWDPPnz4fBcLXlWSaT4b333sOQIUPg5eWFtm3bIikpCWfOnEHfvn3h7e2N3r17Izk52fKY5ORk3HXXXQgLC4OPjw+6d++Obdu2VRlfbm4uHnzwQYSEhMDPzw+33XYbjh49arfXg+oHW0iJiKjBO/FCIjLzdfjzUs1aDGtDIQwIkuUjQquBSq2xus/DV23+jwCyCs2tpyE+asBOlYkCAqtfB1Uhl8HfS4k8Gy/Rr7/+irFjx+LNN99Enz59kJycjMmTJwMA5s6da1lvwYIFWLp0KZYuXYqnn34a9913H5o1a4bZs2ejSZMmmDhxIqZOnYr/+7//AwAUFhZi0KBBeOmll6BWq/Hpp59i6NChOHXqFJo0aWIzznvuuQeenp74v//7P2i1Wrz33nvo168f/v77bwQGBtbg1SFHYkJKREQNnsZDgUu5JVAr7V8YXW4yQWYC5DIZ5NfVwJT/m3maZNeM7ZRdXV7vsdSgBqcQ5tvGjRutLrM5cOBA5OTk4JlnnsG4ceMAAM2aNcOCBQswa9Ysq4R0woQJGDVqFADg6aefRnx8PJ5//nkkJiYCAB577DFMmDDBsn7nzp3RuXNny98LFizAt99+i++//x5Tp06tEONvv/2Gffv2ITMzE2q1Obl/9dVXsWHDBnz99deWJJmcDxNSIiJq8C5kF6NEb5Q6DKdnFAK33norli9fblnm7e2NTp06Yffu3XjppZeurms0orS0FMXFxZYrUnXq1Mlyf1hYGACgY8eOVstKS0uRn58PPz8/FBYWYt68efjxxx+RlpYGg8GAkpISXLhwwWZ8R48eRWFhIYKCgqyWl5SUWA0FIOfDhJSIiBo0ncGIc1ccO5nIlWk8vSrMqC8sLMT8+fNx9913V1xfc3VYglKptPy//ApJtpaZTCYAwJNPPomtW7fi1VdfRYsWLeDp6YmRI0dCr9fbjK2wsBARERH4+eefK9zn7+9fvSdIkmBCSkREDdrZrCIYjQ4ut+TChBAwmQTk8qvd/V27dsWpU6fqvfTT7t27MX78eAwfPhyAOeFMSUmpdP2uXbsiPT0dHh4eiI6OrtdYyL6YkBIRUYNVUFqG1NwSqcNwOTqDEZ6qqynEnDlzMGTIEDRp0gQjR46EXC7H0aNHcfz4cbz44ou13k/Lli2xfv16DB06FDKZDM8//7yl9dSW/v37Iz4+HsOGDcPixYvRqlUrpKam4scff8Tw4cPRrVu3WsdC9sWElIiIGqzTmYUQTtQ4ahg8BABgEgIF2cUAzDPhazL5yBHKjAJKowkeCnP1yMTERGzcuBEvvPACFi1aBKVSiTZt2uDBBx+s036WLl2KiRMnonfv3ggODsbTTz+N/Pz8SteXyWTYtGkTnn32WUyYMAFZWVkIDw/HLbfcYhmzSs5JJoQzfRTdU35+PrRaLfLy8uDn5yd1OEREBCCrQIejF3Mdvl+5qQwBplxENWlaoexTOZMQuPBvQtrECRNSwFwGylvt+u1apaWlOHfuHGJiYqzGuwL8/nYkFsYnIqIGx2QSOJ1ZIHUYLs1oEigzVN59TlQTTEiJiKjBuZRbgmIdyzzVVanBCHa0Un1gQkpERA1KmdGE5KxCqcNwC0IAeraSUj1gQkpERA3K2awiGFjmqd7ojCaY2EpKdcSElIiIGowinQH/5BRLHYZ7EYCujK2kVDdMSImIqMFwtjJP7qLMaIKxivqgRDfChJSIiBqEK4U6XC7QSR2G2yplKynVARNSIiJye0II/J3BiUz2ZDQJlBmZlFLtMCElIiK3dym3BEU6g9RhuD1dGctAUe24/iUWiIiIqmAu81QkdRjV8vOpTADmckpZhebhBeeyimCvCzX1bR1ar9szCUBvNEHtobjhuikpKYiJicHhw4cRGxtbr3GQ62ELKRERubWUy0W8olA9+e/kSdB6qTBj2qMV7ntixnRovVSYOGECy0BRjTEhJSIit1WsN+AiyzzVq8aNo7D+669QUlJiWVZaWop1X32JqKgmAFgGimqOCSkREbmt0xmFYDWi+tU5NhaNGjfGD999a1n2w3ffIioqCp06dwZgHibx46ZNuPnmm+Hv74+goCAMGTIEycnJVW77+PHjGDhwIHx8fBAWFob//Oc/uHz5sl2fDzkHJqREROSWsov0yGKZJ7t4YOx4fP7Zp5a/P/v0E9z/n3FW6+TmF2LmzJk4cOAAtm/fDrlcjuHDh8NUyS+E3Nxc3HbbbejSpQsOHDiAzZs3IyMjA6NGjbLrcyHnwElNRETkdsxlngqkDsNtjb73Psyf8xwuXDgPANibtAerPvkcv+36xbLO0LuGw1OlgFJhbvv66KOPEBISghMnTqBDhw4Vtvn222+jS5cuePnlly3LPvroI0RFReHvv/9Gq1at7PysSEpMSImIyO2k5pWisJRlnuwlOCQEdwwYiDWffQohBO4YMBBBwcFW6ySfOY2XF8zHwQP7cfnyZUvL6IULF2wmpEePHsXOnTvh4+NT4b7k5GQmpG6OCSkREbmVMqMJZzJZBN/e/jN2PJ6cOQMA8Nrrb1S4f/TI4Yhq0hTvrFiB6KgomEwmdOjQAXq93ub2CgsLMXToUCxatKjCfREREfUaOzkfJqRERORWzl9hmSdH6H9HIsr0eshkMvS7/Q6r+7KvXMHpv//Gm++swE033wxvtQf27N5d5fa6du2Kb775BtHR0fDwYHrS0HBSExERuY1ivQEXslnmyREUCgX2HT6GvYeOQqGwLoTvHxCAwKAgfPzRBzhz5gy2bN2GmTNnVrm9Rx99FNnZ2RgzZgz279+P5ORkbNmyBRMmTIDRaLTnUyEnwJ8gRETkNs5kunaZp/IrJ5mEsCTWTQK9ILfXpZrqyM/Pz+ZyuVyOjz75HE8/+Tjiu3VBy1at8MYbb6LfbbdWuq3IyEjs3r0bTz/9NO644w7odDo0bdoUAwYMgFzO9jN3JxO86Kzd5efnQ6vVIi8vr9IPLxER1U1OkR4Hz+dIHcYNyU1lCDDlIqpJU6jUGpvruEpCWhMKuQzeaudrBystLcW5c+cQExMDjcb6/eD3t+PwJwcREbk8IQROscyTUzOaBMqMLtx8TXbFhJSIiFweyzy5Bl2ZEeyYJVtcLiF95513EB0dDY1Gg549e2Lfvn1Vrr9u3Tq0adMGGo0GHTt2xKZNmyz3lZWV4emnn0bHjh3h7e2NyMhIjB07FqmpqVbbiI6Ohkwms7q98sordnl+RERUMwajCcks8+QSTALQs5WUbHCphHTt2rWYOXMm5s6di0OHDqFz585ITExEZmamzfX37NmDMWPGYNKkSTh8+DCGDRuGYcOG4fjx4wCA4uJiHDp0CM8//zwOHTqE9evX49SpU7jzzjsrbOuFF15AWlqa5TZt2jS7PlciIqqelCtF0LPMk8vQG0wwsZWUruNSk5p69uyJ7t274+233wYAmEwmREVFYdq0aXjmmWcqrD969GgUFRVh48aNlmW9evVCbGwsVqxYYXMf+/fvR48ePXD+/Hk0adIEgLmFdMaMGZgxY0a14tTpdNDprl4/OT8/H1FRURwUTURUz0r0RiSdvexSM+vlpjL4m3IRFdUEao2nzXXccVLTtVQecmiUihuv6AAlJSVISUnhpCaJuUwLqV6vx8GDB9G/f3/LMrlcjv79+yMpKcnmY5KSkqzWB4DExMRK1weAvLw8yGQy+Pv7Wy1/5ZVXEBQUhC5dumDJkiUwGCofq7Rw4UJotVrLLSoqqhrPkIiIasoVyzyZZAoIIaArLZU6FMnoDSYYTc7RHlZ+5ajra6mSYzlf/YVKXL58GUajEWFhYVbLw8LCcPLkSZuPSU9Pt7l+enq6zfVLS0vx9NNPY8yYMVa/hKZPn46uXbsiMDAQe/bswezZs5GWloalS5fa3M7s2bOtCgCXt5ASEVH9yS3WIyPfBZM6mRwl0OByVhYAQK3RQHZdC6hJCAiDOVHS6+Ru10IKAMYyGTxV0qYhJpMJWVlZ8PLy4tWhJMZX/19lZWUYNWoUhBBYvny51X3XJpedOnWCSqXClClTsHDhQqjV6grbUqvVNpcTEVH9EELg7wzXnchU6uELGApgzMwwT5a97n4hgCtF5oTUmKeCG+ajAAClQg6FXNonJ5fL0aRJkwo/CsixXCYhDQ4OhkKhQEZGhtXyjIwMhIeH23xMeHh4tdYvT0bPnz+PHTt23HCcSM+ePWEwGJCSkoLWrVvX4tkQEVFdZOTrkF9SJnUYtSeToVTph1LhA7moeFlMvcGEBTtPAACeH9wOKoXLjLCrES+VAp0b+0uaDKpUKl4Jygm4TEKqUqkQFxeH7du3Y9iwYQDMTe3bt2/H1KlTbT4mPj4e27dvt5qMtHXrVsTHx1v+Lk9GT58+jZ07dyIoKOiGsRw5cgRyuRyhoaF1ek5ERFRzRpPAGXcp8ySTwySrmAwZZUZcKjD++38PmOTuOb6xsAzI1gGN/G1fsYoaDpdJSAFz1/m4cePQrVs39OjRA8uWLUNRUREmTJgAABg7diwaNWqEhQsXAgAee+wxJCQk4LXXXsPgwYPx5Zdf4sCBA1i5ciUAczI6cuRIHDp0CBs3boTRaLSMLw0MDIRKpUJSUhL27t2LW2+9Fb6+vkhKSsLjjz+OBx54AAEBAdK8EEREDdiF7GKUllVsVSTXlJxZiDBfNTzctBWYqselEtLRo0cjKysLc+bMQXp6OmJjY7F582bLxKULFy5YNbv37t0ba9aswXPPPYf//e9/aNmyJTZs2IAOHToAAC5duoTvv/8eABAbG2u1r507d6Jv375Qq9X48ssvMW/ePOh0OsTExODxxx+3GldKRESOoTMYkXKlSOowqB7pDSaczy5G8xAfqUMhCblUHVJXxTpmRET142R6Pv7JLpE6DLvTlRnx6BeHAQDvjOkCtZPU7LQXhVyG+OZBTlObtBy/vx2H7eNEROQSinQGXMpx/2S0ITKaBJKz3GRcMNUKE1IiInIJZzILwT4995WeV4qCUheunEB1woSUiIicXm6xHlkFuhuvSC5LCLhP9QSqMSakRETk9JioNAxXCvXI/veCANSwMCElIiKnlllQitxiduU2FPzx0TAxISUiIqclhEByJss8NST5JWXILCiVOgxyMCakRETktNLySlGkM0gdBjmYeQIbZ7A1JExIiYjIKZlMAmez2DraEBXrjEjLYytpQ8KElIiInFJqXgkvEdqAnbtcBJOJraQNBRNSIiJyOiaTwLnLbB1tyEr0RqTm8UIIDQUTUiIicjqXckugKzNJHQZJLOVyMVtJGwgmpERE5FRMJoGUK2wdJaC0zIi0fI4lbQiYkBIRkVNh6yhdK4VjSRsEJqREROQ0TCaB81eKpQ6DnEiJ3oh0tpK6PSakRETkNDIKSjmznipIuVLEuqRujgkpERE5jZTLbB2liop1RmQV6qQOg+yICSkRETmFrAIdr8pEleJQDvfGhJSIiJzChWwmHFS5vOIy5BWXSR0G2QkTUiIiklyhzoCcIr3UYZCTu5jDHy3uigkpERFJ7iJbR6kaMjnpzW0xISUiIkmVGU1Iz2NZH7oxkwlIzeXlRN0RE1IiIpJURn4pjCx8TtWUmlvKElBuiAkpERFJ6lIOW7yo+krLjMjmeGO3w4SUiIgkU1BahoJSlnqimknN5RAPd8OElIiIJMOxo1QbWYWlMBhNUodB9YgJKRERSUIIgYx8Xn2Has5kAq/c5GaYkBIRkSTySspYwodqja3r7oUJKRERSSKzgC1cVHs5xXp227sRJqRERCSJy0xIqQ5MJnC2vRthQkpERA5XrDegWM/ueqobjiN1H0xIiYjI4a4UsmWL6o7HkftgQkpERA6XU8xEgupObzChSMc6tu6ACSkRETlcTnGZ1CGQm+CPG/fAhJSIiByqSGdAmYGzo6l+5PLHjVtgQkpERA7FS4VSfeLx5B6YkBIRkUMVlLJFi+pPsd4Ao0lIHQbVERNSIiJyqAJOQqF6JARQyGPK5TEhJSIihyrWsf4o1a8S1rR1eUxIiYjIYUwmAZ2ByQPVr5IyHlOujgkpERE5jM5gguBwP6pnbCF1fUxIiYjIYdg6SvbA48r1MSElIiKHKTOyeZTqH2fZuz4mpERE5DBMHMge+EPH9TEhJSIihzGYeIUmqn88rlwfE1IiInIYTmgiIluYkBIRkcPIZFJHQO5IBh5Yro4JKRERERFJigkpERE5jIecXztU/xRytpC6Op4ZiIjIYZQKJg5U/1QePK5cncslpO+88w6io6Oh0WjQs2dP7Nu3r8r1161bhzZt2kCj0aBjx47YtGmT5b6ysjI8/fTT6NixI7y9vREZGYmxY8ciNTXVahvZ2dm4//774efnB39/f0yaNAmFhYV2eX5ERO7MQ+FyXzvkAtjy7vpc6h1cu3YtZs6ciblz5+LQoUPo3LkzEhMTkZmZaXP9PXv2YMyYMZg0aRIOHz6MYcOGYdiwYTh+/DgAoLi4GIcOHcLzzz+PQ4cOYf369Th16hTuvPNOq+3cf//9+PPPP7F161Zs3LgRu3btwuTJk+3+fImI3I3aw6W+dshFqJU8rlydTAjXKcLRs2dPdO/eHW+//TYAwGQyISoqCtOmTcMzzzxTYf3Ro0ejqKgIGzdutCzr1asXYmNjsWLFCpv72L9/P3r06IHz58+jSZMm+Ouvv9CuXTvs378f3bp1AwBs3rwZgwYNwj///IPIyMgbxp2fnw+tVou8vDz4+fnV5qkTEbmNnSczWSD/BnRlRjz6xWEAwDtjukCtVEgckXNrFeaLJkFe9b5dfn87jsv8pNDr9Th48CD69+9vWSaXy9G/f38kJSXZfExSUpLV+gCQmJhY6foAkJeXB5lMBn9/f8s2/P39LckoAPTv3x9yuRx79+61uQ2dTof8/HyrGxERmXmqmFxR/eIx5fpcJiG9fPkyjEYjwsLCrJaHhYUhPT3d5mPS09NrtH5paSmefvppjBkzxvJLKD09HaGhoVbreXh4IDAwsNLtLFy4EFqt1nKLioqq1nMkImoIvJg8UD3zVvOYcnUuk5DaW1lZGUaNGgUhBJYvX16nbc2ePRt5eXmW28WLF+spSiIi1+enUUodArkRhUIGTw5pcHkeUgdQXcHBwVAoFMjIyLBanpGRgfDwcJuPCQ8Pr9b65cno+fPnsWPHDqtxIuHh4RUmTRkMBmRnZ1e6X7VaDbVaXe3nRkTUkPh5MiGl+uOnUULGS4C5PJdpIVWpVIiLi8P27dsty0wmE7Zv3474+Hibj4mPj7daHwC2bt1qtX55Mnr69Gls27YNQUFBFbaRm5uLgwcPWpbt2LEDJpMJPXv2rI+nRkTUoPhqXKYthFyA1pPHkztwqXdx5syZGDduHLp164YePXpg2bJlKCoqwoQJEwAAY8eORaNGjbBw4UIAwGOPPYaEhAS89tprGDx4ML788kscOHAAK1euBGBORkeOHIlDhw5h48aNMBqNlnGhgYGBUKlUaNu2LQYMGICHHnoIK1asQFlZGaZOnYp77723WjPsiYjImlIhh5+nEvklZVKHQm4gwEsldQhUD1wqIR09ejSysrIwZ84cpKenIzY2Fps3b7ZMXLpw4QLk1xTH7d27N9asWYPnnnsO//vf/9CyZUts2LABHTp0AABcunQJ33//PQAgNjbWal87d+5E3759AQCrV6/G1KlT0a9fP8jlcowYMQJvvvmm/Z8wEZGbCvRWMSGlOpPLmZC6C5eqQ+qqWMeMiMhaTpEeB8/nSB2G02Id0uoJ9FGha5MAu22f39+O4zJjSImIyH1oPZVQ8qpNVEchPpxA7C54NiAiIoeTy2VMJqhOZDIg1I/HkLtgQkpERJIIYzJBdeDvpYLag0MZ3AUTUiIikkSgtwpqJb+GqHbCtRqpQ6B6xDMBERFJQiaTIULrKXUY5IIUChnCfNnC7k6YkBIRkWQa+TMhpZoL99PAQ8EUxp3w3SQiIsl4qhQI9GEdSaqZRgH8IeNumJASEZGkmgZ6SR0CuZAAbyX8NEqpw6B6xoSUiIgkFeSjhg+vb0/V1CTQW+oQyA6YkBIRkeSaBrGVlG7MS61AMId4uCUmpEREJLkwXw08VawpSVWLCfaGTCaTOgyyAyakREQkOblchphgdsVS5bzUCoT7sfaou2JCSkRETiFCq4EXW0mpEs2Cfdg66saYkBIRkVOQyWRoFuIjdRjkhHw0HrzUrJtjQkpERE4jzE8NP0+W9CFrLUPZOurumJASEZHTkMlkaBnKVlK6KshHhSAfto66OyakRETkVAK8VQjhdcoJgEwGtOAPlAaBCSkRETmdVmG+kPMbqsFrHOAFX16VqUHgx52IiJyOp0qB6CCWgWrIlB5yNAvhMdBQMCElIiKn1DTIm8XyG7CWoT5QKpimNBR8p4mIyCkp5DK0DveVOgySQIC3EhFaFsFvSJiQEhGR0wr2USOciUmDIpcDbcL9WOapgWFCSkRETq1lmA88FExOGoroIG94qz2kDoMcjAkpERE5NbWHgl33DYS32oOT2RooJqREROT0IrSeCPJRSR0G2ZFMBrSL9INcztbwhogJKRERuYS2EX7sundjTYO8oeVlYxssJqREROQSNEoFWoWx694deas90CyYXfUNGRNSIiJyGZH+nrysqJuRyYD2jdhV39AxISUiIpfSJsIXSg9+fbmLZiE+8OPlQRs8fqKJiMilqD0UaBvBrnt3oPVSIjrIS+owyAkwISUiIpcT6qtBpL+n1GFQHSjkMrSPZAF8MmNCSkRELqlVmA+8eK17l9Uq3BdeKhbAJzOHJKQ7d+50xG6IiKgB8VDI0T5SCzawuZ5QPzUasYWbruGQhHTAgAFo3rw5XnzxRVy8eNERuyQiogZA66VEsxAfqcOgGlAr5WgT7id1GORkHJKQXrp0CVOnTsXXX3+NZs2aITExEV999RX0er0jdk9ERG4sOsgL/l6cpe0q2kdqoWKVBLqOQ46I4OBgPP744zhy5Aj27t2LVq1a4ZFHHkFkZCSmT5+Oo0ePOiIMIiJyQzKZDB0aaXkVJxcQHeyNQG9eApYqcvhPlK5du2L27NmYOnUqCgsL8dFHHyEuLg59+vTBn3/+6ehwiIjIDWiUCrSLYDewM/PzVPJqTFQphyWkZWVl+PrrrzFo0CA0bdoUW7Zswdtvv42MjAycOXMGTZs2xT333OOocIiIyM2E+mnQKIATZZyRQiFDB16NiargkHoL06ZNwxdffAEhBP7zn/9g8eLF6NChg+V+b29vvPrqq4iMjHREOERE5KZahfkip1iPYp1R6lDoGm3D/VjiiarkkKPjxIkTeOutt3D33XdDrbZ9DeLg4GCWhyIiojpRyGXo2EiL/SnZMJmkjoYAIMJfg3CtRuowyMk5pMt+7ty5uOeeeyokowaDAbt27QIAeHh4ICEhwRHhEBGRG/PVKNEylJcWdQZeKgVah/G9oBtzSEJ66623Ijs7u8LyvLw83HrrrY4IgYiIGpCoQC+E+NrukSPHkMuBDo218FCwxBPdmEOOEiGEzWvVXrlyBd7enHFHRET1r12kH9RKJkNSaRHiCz8N68NS9dh1DOndd98NwFwjbvz48VZd9kajEceOHUPv3r3tGQIRETVQSoUcHSK1OHQhB0JIHU3DEuyrRpMgL6nDIBdi14RUq9UCMLeQ+vr6wtPzajkOlUqFXr164aGHHrJnCERE1IAFeKsQHeyNc1lFUofSYKg85KwJSzVm14R01apVAIDo6Gg8+eST7J5vYIr1BqlDcBksh0JkP82CvZFTpEducZnUoTQIHRrx0qBUcw75Fpw7d64jdkNOpt2cLVKH4DJSXhksdQhEbqv80qK/n70Cg5F99/YUHezFS4NSrdgtIe3atSu2b9+OgIAAdOnSxeakpnKHDh2yVxhERETQKBVoG+GHP/7JkzoUt2W+NKiP1GGQi7JbQnrXXXdZJjENGzbMXrshJ3bihcS6b+RKNrB3b503U2wEuu03dyEd6G6Cl6LOmwRatABatayHDRGRI4T5aXDFX4/U3BKpQ3E7vDQo1ZXdEtJru+nZZd8w1cu4yEsXgPpIHq/hpUD9JKSXLgDtWpuL7RGRS2gd7ovcYj2K9by0aH1qHebLsfBUJy73TfrOO+8gOjoaGo0GPXv2xL59+6pcf926dWjTpg00Gg06duyITZs2Wd2/fv163HHHHQgKCoJMJsORI0cqbKNv376QyWRWt4cffrg+nxbZUlICpKdLHUXl9HogNVXqKIioBhRyGdo30qKKUWRUQ2F+GkT6e954RaIq2C0hDQgIQGBgYLVu1bV27VrMnDkTc+fOxaFDh9C5c2ckJiYiMzPT5vp79uzBmDFjMGnSJBw+fBjDhg3DsGHDcPz4ccs6RUVFuPnmm7Fo0aIq9/3QQw8hLS3Nclu8eHG146ZaOn8eTl88MCVF6giIqIa0nko0C+FYx/qgVsrRJoKXBqW6s1v7+rJly+p9m0uXLsVDDz2ECRMmAABWrFiBH3/8ER999BGeeeaZCuu/8cYbGDBgAJ566ikAwIIFC7B161a8/fbbWLFiBQDgP//5DwAg5QaJhZeXF8LDw+vx2VCVTCbgwgWpo7ixnBwgLw/4t+YuEbmG6CAvXCnUsRRUHbWP1ELJS4NSPbBbQjpu3Lh63Z5er8fBgwcxe/ZsyzK5XI7+/fsjKSnJ5mOSkpIwc+ZMq2WJiYnYsGFDjfe/evVqfP755wgPD8fQoUPx/PPPw8vL9lUodDoddDqd5e/8/Pwa76/BS00FrnkNndq5c0BsrNRREFENyGQytI/U4vdzV2BkKahaaRrEEk9Uf+yWkObn58PPz8/y/6qUr1eVy5cvw2g0IiwszGp5WFgYTp48afMx6enpNtdPr+G4xPvuuw9NmzZFZGQkjh07hqeffhqnTp3C+vXrba6/cOFCzJ8/v0b7oOucPSt1BNV36RLQti1wzaVxicj5eaoUaB3mixOpbDSoKW+1B5pz2APVI7slpAEBAUhLS0NoaCj8/f1t1iEVQkAmk8FodO7ZjpMnT7b8v2PHjoiIiEC/fv2QnJyM5s2bV1h/9uzZVi2z+fn5iIqKckisbuHyZXM3uKswmcwJdNu2UkdCRDUU6e+JrAIdsgpcpEfGCcjlYIknqnd2S0h37NhhmbC0c+fOOm8vODgYCoUCGRkZVsszMjIqHdsZHh5eo/Wrq2fPngCAM2fO2ExI1Wq1pQYr1cLp01JHUHMpKea6pEql1JEQUQ21ifBFbkkZygwmqUNxCTHBPvDV8FxH9ctuCWlCQoLN/9eWSqVCXFwctm/fbim0bzKZsH37dkydOtXmY+Lj47F9+3bMmDHDsmzr1q2Ij4+vUyzlpaEiIiLqtB2y4coVcwupqzEYgORkoE0bqSMhohpSeyjQNtwXx3gVpxvy81QiOsj2/AmiunBYFducnBx8+OGH+OuvvwAA7dq1w4QJE2pU9mnmzJkYN24cunXrhh49emDZsmUoKiqyzLofO3YsGjVqhIULFwIAHnvsMSQkJOC1117D4MGD8eWXX+LAgQNYuXKlZZvZ2dm4cOECUv+tJ3nq1CkA5tbV8PBwJCcnY82aNRg0aBCCgoJw7NgxPP7447jlllvQqVOnenlt6F9CACdOSB1F7Z09C0RHAxqN1JEQUQ2F+mkQrtUhPa9U6lCcllwOtI/0q/JS4ES15ZBaDbt27UJ0dDTefPNN5OTkICcnB2+++SZiYmKwa9euam9n9OjRePXVVzFnzhzExsbiyJEj2Lx5s2Xi0oULF5CWlmZZv3fv3lizZg1WrlyJzp074+uvv8aGDRvQoUMHyzrff/89unTpgsGDBwMA7r33XnTp0sVSFkqlUmHbtm2444470KZNGzzxxBMYMWIEfvjhh/p4aehaFy8CublSR1F7RiPw559SR0FEtdQqzBcqD5YwqkzzEB94q3k1JrIPmRD2rzzesWNHxMfHY/ny5VAozNdsNBqNeOSRR7Bnzx788ccf9g5BUvn5+dBqtcjLy6tWRYEGqbQU+PlnoMw+NQGLjUC7381fNCd61dO17CvTowdwXXUHInINmfmlTtF1rysz4tEvDgMA3hnTBWqlPU9aN+bnqUT36IAG1zrK72/HcchPwTNnzuCJJ56wJKMAoFAoMHPmTJw5c8YRIZAzEwI4csRuyajDHT3qOjVUichKqJ8GoX6clHotuRxox656sjOHJKRdu3a1jB291l9//YXOnTs7IgRyZsnJQFaW1FHUH50OOHzY+S97SkQ2tQ73hYeCyVe56CBv+LCrnuzMbkfYsWPHLP+fPn06HnvsMZw5cwa9evUCAPz+++9455138Morr9grBHIFGRlAJRc2cGlZWcBffwHt2kkdCRHVkNpDgVYsmA/AXAA/Oshb6jCoAbBbQhobGwuZTIZrh6jOmjWrwnr33XcfRo8eba8wyJnl5gIHD7pvS2JyMuDpCcTESB0JEdVQpL8n0vJKkFPkJkOJaqlthC8L4JND2C0hPXfunL02Te4gPx/Yu9c8M92dHT8OeHgAvFIXkctpE+6HveeuwNRA6+U3CvCEvxevVU+OYbeEtGnTpvbaNLm6vDzg998BvV7qSBzjyBFzK3CTJlJHQkQ14K32QNMgb5zLKpI6FIdTesjRIpTXqifHcego5RMnTuDChQvQX5eI3HnnnY4Mg6SUlQUcOGC+slFDUj7zvmVLqSMhohqIDvJGel4pSvRu3ptznZahPlAqWJOVHMchCenZs2cxfPhw/PHHH1bjSstLSBjdvduWzC5cAI4dc98xozdy8iRQXAx07Giuo0JETk8hl6F1uC+OXMiVOhSH8fdSIkLLK86RYznkW/Gxxx5DTEwMMjMz4eXlhT///BO7du1Ct27d8PPPPzsiBJKSyQT88Ye5lbChJqPlLlwA9uwxXwiAiFxCsI8aIb4NozapTGYue8Wao+RoDklIk5KS8MILLyA4OBhyuRxyuRw333wzFi5ciOnTpzsiBJJKcTGwezeQkiJ1JM4jJwf45Rf3qr1K5OZahvk0iI6NRgGe8NUopQ6DGiCHfLyMRiN8fX0BAMHBwUhNTQVgnvh06tQpR4RAUrh0Cdi1y7WvT28ver15Ytdff6HBTuElciFeKg80CfSSOgy78lDI0CyYE5lIGg4ZQ9qhQwccPXoUMTEx6NmzJxYvXgyVSoWVK1eiWbNmjgiBHEmvN5c7unRJ6kic35kzQGYm0KULwOskEzm16CBvpOWVQlfmnj8im4f4QOXRAJqBySk5JCF97rnnUFRkLpvxwgsvYMiQIejTpw+CgoKwdu1aR4RAjpKWZh4vymu5V19+PvDrr+YZ+C1acMITkZPyUMjRLMQHf7nhFZy8VAo08veUOgxqwBySkCYmJlr+36JFC5w8eRLZ2dkICAjgwGl3UVpqbhVNS5M6EtdkMgGnTgGpqUDnzkBAgNQREZENkVoNLlwpRpHOvUrXtQjz4RWZSFIOrUMKABcvXgQARPHKNe5BCPOEpZMnG15tUXsoKAB++w1o2hRo2xZQcnIBkTORyWRoGebjVmWg/L2UCPVlmSeSlkP6Bg0GA55//nlotVpER0cjOjoaWq0Wzz33HMrKGvZ1gl1aTo65q/n4cSaj9e38eWDHDuDiRZbKInIywT5qBHi7z49FXpGJnIFDWkinTZuG9evXY/HixYiPjwdgLgU1b948XLlyBcuXL3dEGFRfdDrz7PB/W7vJTvR682VHz583F9PXaqWOiIj+1TzEBweKcqQOo86CfdW8Xj05BYckpGvWrMGXX36JgQMHWpZ16tQJUVFRGDNmDBNSV2EyAefOAX//zRZRR8rJMZfPatIEaNMGUDeMAt1EzszfS4VgXzUuF7j2BM7mId5Sh0AEwEEJqVqtRnR0dIXlMTExUKn4y8wlZGaau+b/rZZAErhwwTxprFUrIDqas/GJJNY8xNulE9IwPw2L4JPTcMg32tSpU7FgwQLorikFpNPp8NJLL2Hq1KmOCIFqq7AQ2LvXfGMyKr2yMuDPP4Gffzb/SCAiyfhqlC59SdEYto6SE7FbC+ndd99t9fe2bdvQuHFjdO7cGQBw9OhR6PV69OvXz14hUF2UlZm75s+d46QaZ1RUZP6REBoKtG8P+HBSApEUYkK8keWCraRhfhr4qB1eaIeoUnY7GrXXTcAYMWKE1d8s++SkhDB3DZ88aZ5UQ84tMxPIygJiYsxd+SwTReRQfv+2krpaUsrWUXI2dktIV61aZa9Nk71kZ5uvspTvflchcWtCAGfPAv/8Y65dGhUF8IITRA4THeRaraQhvmq2jpLTcegRmZWVhVOnTgEAWrdujZCQEEfuniqj0wEnTpgTGnJdej1w9Ki5TFSnTiwTReQgWi8lArxVyClyjV6l6CC2jpLzccikpqKiIkycOBERERG45ZZbcMsttyAyMhKTJk1CcXGxI0IgW8qvsrRzJ5NRd5KbywsWEDlYdJCX1CFUS4C3ElovDu0h5+OQhHTmzJn45Zdf8MMPPyA3Nxe5ubn47rvv8Msvv+CJJ55wRAh0vYICYM8ecxc9r5blfoQwT0jbuRNIT5c6GiK3F+Sjho/G+bvBmwSydZSck0M+Pd988w2+/vpr9O3b17Js0KBB8PT0xKhRo1gY35GEAM6cMc+gN5mkjobsrbQU2L8fiIw0X+2JdX+J7KZJoBdOpDrvGHwvtQLBPjwHkHNySAtpcXExwsLCKiwPDQ1ll70jFRUBv/1mnkHPZLRhSU011y7NyJA6EiK3Fe6ngcrDeS9Y0STQCzJOeCQn5ZBPTnx8PObOnYvS0lLLspKSEsyfP99ybXuys4sXgV9+MY8vpIZJpwP27TOPLeUPEqJ6J5fL0DjAU+owbPJQyBChdc7YiAAHddkvW7YMAwYMqFAYX6PRYMuWLY4IoeEymczjRC9ckDoSchbnzplLfHXvDnjyC4qoPkX6e+Lc5SKnu55IpL8nFHK2jpLzckhC2rFjR5w+fRqrV6/GyZMnAQBjxozB/fffD09+IdpPeYsYW0Xpenl5wK5d5qQ0MFDqaIjchkapQKivBhn5pTde2YGcteWWqJzdE9KysjK0adMGGzduxEMPPWTv3VG5oiLg998BjtGlyuj1QFISEBcHhIdLHQ2R22gc4OlUCWmgjwpeKuevAEANm93HkCqVSquxo+QAhYXA7t1MRunGTCbgwAHzpCciqhcB3ip4qRVSh2HR2J+to+T8HDKp6dFHH8WiRYtgYJFu+yspMbd66VznMnYkMSGAQ4eAzEypIyFyG42cJAlUecgR7KOWOgyiG3JIG/7+/fuxfft2/PTTT+jYsSO8va0L865fv94RYbg/o9E8ZpQt0lRTQgAHDwI33wz4+kodDZHLi9B6IjmrUPKCFhFaDeSczEQuwCEJqb+/P0aMGOGIXTVsf/4J5DtvUWZycgaDOSm95RZA7ry1FIlcgcpDjiBvNbIKpO2tinCSllqiG7FrQmoymbBkyRL8/fff0Ov1uO222zBv3jzOrLeH7Gzg/HmpoyBXV1BgvpJXq1ZSR0Lk8iL8NZImpH6eSvioOZmJXINdm0Feeukl/O9//4OPjw8aNWqEN998E48++qg9d9lwnTghdQTkLs6cMc/AJ6I6CfZWQynhlZsitBrJ9k1UU3b9pHz66ad49913sWXLFmzYsAE//PADVq9eDZPUg2rcTW4ukJMjdRTkLoxGtrYT1QO5XIYwP2kmFMlkQJgfE1JyHXZNSC9cuIBBgwZZ/u7fvz9kMhlSWWKmfl26JHUE5G74GSWqF+ESJYUB3iqoJGydJaopux6tBoMBGo31h1GpVKKsrMyeu214srOljoDcTX4+wM8pUZ1pPZVQKx2fGEqVCBPVll1HOwshMH78eKjVV7ssSktL8fDDD1uVfmLZpzoqLJQ6AnJHhYVAQIDUURC5NJlMhjA/DS5ccdyFSuRyIMSXtUfJtdg1IR03blyFZQ888IA9d9nwCGEu10NU33hcEdWLUF+1QxPSQG81lAp215NrsWtCumrVKntungDzyHWZzJyYEtUn1iIlqhfl3fa6MsdM6A1l6yi5IH7juAMNxwqRHfC4IqoXMpnMYV3oMhl4qVBySUxI3YGfn9QRkLvx8AC8vKSOgshthDgoSfT3UnJ2PbkkHrXuIDhY6gjI3QQGmptaiKheBHipoFDY/zMV4sOeDXJNTEjdQXi41BGQu4mIkDoCIrcil8sQ7G3/VtJgX5Xd90FkD0xI3YGXFxAUJHUU5C4UCiakRHYQ5GPfZNFLpYCXiteuJ9fEhNRdNGsmdQTkLqKiAKVS6iiI3E6gt30T0iBOZiIX5nIJ6TvvvIPo6GhoNBr07NkT+/btq3L9devWoU2bNtBoNOjYsSM2bdpkdf/69etxxx13ICgoCDKZDEeOHKmwjdLSUjz66KMICgqCj48PRowYgYyMjPp8WnUXFsbJTVR3cjnQooXUURC5JY1SAR+N/Vow7d0CS2RPLpWQrl27FjNnzsTcuXNx6NAhdO7cGYmJicjMzLS5/p49ezBmzBhMmjQJhw8fxrBhwzBs2DAcP37csk5RURFuvvlmLFq0qNL9Pv744/jhhx+wbt06/PLLL0hNTcXdd99d78+vTmQyoG1bqaMgVxcdDXh6Sh0FkdsKtlPSKJebJ04RuSqZEK5TUb1nz57o3r073n77bQCAyWRCVFQUpk2bhmeeeabC+qNHj0ZRURE2btxoWdarVy/ExsZixYoVVuumpKQgJiYGhw8fRmxsrGV5Xl4eQkJCsGbNGowcORIAcPLkSbRt2xZJSUno1avXDePOz8+HVqtFXl4e/OzdirlvH+BsrbdOoNgItPvd/PvrRC8TvBQSB+SM1Grg1lvZXU9kR1cKdTh8IfeG6+nKjHj0i8MAgHfGdIFaWfVJK8BbhbimvNRvfXPo93cD5zItpHq9HgcPHkT//v0ty+RyOfr374+kpCSbj0lKSrJaHwASExMrXd+WgwcPoqyszGo7bdq0QZMmTSrdjk6nQ35+vtXNYTp2NE9KIaqpDh2YjBLZmb+Xyi4XQbP3+FQie3OZhPTy5cswGo0ICwuzWh4WFob09HSbj0lPT6/R+pVtQ6VSwd/fv9rbWbhwIbRareUWFRVV7f3Vmacn0K6d4/ZH7iEiAoiMlDoKIrenkMug9az/5DGQ3fXk4lwmIXUls2fPRl5enuV28eJFxwYQHQ2Ehjp2n+S6NBqgUyepoyBqMAK86rcnQiGXwdeOk6WIHMFlEtLg4GAoFIoKs9szMjIQXklh+PDw8BqtX9k29Ho9cnNzq70dtVoNPz8/q5vDdenCa5HTjclk5mNFxdYVIkep78lHWi8l5HJeWY1cm8skpCqVCnFxcdi+fbtlmclkwvbt2xEfH2/zMfHx8VbrA8DWrVsrXd+WuLg4KJVKq+2cOnUKFy5cqNF2HE6lAuLiePlHqlqbNrz0LJGD+Xkq63UcKWfXkztwqTb+mTNnYty4cejWrRt69OiBZcuWoaioCBMmTAAAjB07Fo0aNcLChQsBAI899hgSEhLw2muvYfDgwfjyyy9x4MABrFy50rLN7OxsXLhwAampqQDMySZgbhkNDw+HVqvFpEmTMHPmTAQGBsLPzw/Tpk1DfHx8tWbYSyow0DzJ6dgxqSMhZxQRwZqjRBJQyGXw0yiRW1xWL9vz9+RkRHJ9LpWQjh49GllZWZgzZw7S09MRGxuLzZs3WyYuXbhwAfJrfnb27t0ba9aswXPPPYf//e9/aNmyJTZs2IAOHTpY1vn+++8tCS0A3HvvvQCAuXPnYt68eQCA119/HXK5HCNGjIBOp0NiYiLeffddBzzjetC0KVBQAJw7J3Uk5Ey0WnNXPRFJQutZPwmpTGZucSVydS5Vh9RVSV7HTAhg//4GXZ+UdUiv4ekJ3HwzxxgTSSgzvxTH/smr9P7q1iH11XigZ7Mgu8RITvD93YC4zBhSqgOZzDyeNIBFkxs8pRLo2ZPJKJHE6qtVU1vPM/aJpMKEtKFQKIAePQAfH6kjIakoFOZk1NdX6kiIGjyNUgG1su5fwVp215ObYELakKhUQHw84OUldSTkaHI50L07W8mJnIifpu7JZH1sg8gZMCFtaDQaoHdvJqUNSXkyGhIidSREdI26dtsrFDJ4qRrygHhyJ0xIGyJPTyalDUV5MsordxE5Hb86Xl3JV+0BGWtNk5tgQtpQeXoCN93EMaXurHzcMJNRIqfkU9eElN315EaYkDZk5d33Wq3UkVB9UyqBXr3YTU/kxNQedZvYxOvXkzthQtrQqdXmiU5BrGPnNsp/aAQGSh0JEd2At7r2SWVdW1iJnAkTUrramhYZKXUkVFc+Puai9yzgTOQSfGuZkMpkgLeKCSm5Dx7NZCaXA127mic6nTkjdTRUG0FB5glMSo4rI3IVtW0h9VQqoJBzQhO5DyakdJVMBrRtC3h7A8eOmS85Sq6hSROgY0fzDwsichm17XavS1c/kTPiEU0VNWliTkoPHAD0eqmjoaqU/4ho3lzqSIioFrwquUb9jTAhJXfD5hSyLSgI6NOHYxGdmVJpLuvEZJTIZXko5LWaae+tZkF8ci9MSKlyXl7mCTKc7OR8fH3NPxhYY5TI5XnVYnJSbR5D5MyYkFLVFAogLg5o397cPUzSa9TI/EPB21vqSIioHtTm8p+8ZCi5G/7Eoupp1sxcQP/gQUCnkzqahkkmM/8wiImROhIiqkc1TS49FDIoFWxPIvfCI5qqLygISEgAgoOljqThKb/UK5NRIrfjWcOElN315I6YkFLNqNXmIvqtWkkdScMRGgrccgsQECB1JERkB541nGlf0/WJXAF/ZlHNyWRA69bmS1MePswufHspL+nUrBnH7xK5MU1NE1IV25LI/fCoptoLCWEXvr2Ud9E3b85klMjNKRVyeCiq/zlXe7CFlNwPE1Kqm/Iu/DZtmDjVl4gIc6LPLnqiBqMmraQ1bVElcgXssqe6k8mAli3Nk54OHQJKSqSOyDXJ5UCHDkDTplJHQkQOpvaQo7Ca62pqUUifyNnxqKb6ExhobtmLiJA6Etfj62ueuMRklKhBqkk3PLvsyR0xIaX6pVQC3boBnTqZW/zoxpo0MV91yddX6kiISCLVvXyoXA6oPHhuJffDLnuyj6ZNzS2mBw4AhdXtiGpgPDyAzp15aVYigrqaSSYL4pO7YkJK9lPeDX38OHDhgtTROBd/f/MlWb28pI6EiAAU6w2S7t9oEtCVGS1/6wy2/69UyCSPlYX5yR54VJF9KRTmVsDgYODYMcAg7YnUKTRvbq5KwCENRE6j3ZwtUodQqZnrjkkdgpWUVwZLHQK5ISak5BiNGplbBQ8cAPLzpY5GGkol0KULEBYmdSREREROhQkpOY63t3nyzvHjwPnzUkfjWAEB5i56T0+pIyEiG068kCjp/vWlZdj9zTbL3/kGYHayuRdlYXMT/P79tm7cvDFa9uooRYhEdsWElBxLLjfPwA8MNHfhG403foyra9bMfAlQdtETOS2px0V6KhXmiU0mEwBAfc3pQi2/+refr6fksRLZA49qkkbjxoBWC+zfDxQVSR2NfSgUQGwsZ9ET0Q3JZDJ4eGpgKCqucj0Pb06EJPfEJhuSjq+vuQvfHcdUlg9PYDJKRNXk4XXjIT1KP9YrJvfEhJSkpVQC3bsDrVpJHUn9CQ1loXsiqjGPGyWbCg8o2EJKbopd9iQ9mQxo3Rrw8wMOH3btcaUtWphLOslkUkdCRC7mhgmpjzeUCp5byD0xISXnERFhLhS/bx9QWip1NDVTPlkrKkrqSIjIRcn9fM3nkn8nNlXg6we5nAkpuScmpGQ3tbqaiKc30LN3vdcrLTba/n+9UHoAXeOAoECglldQ4axZIvLwUAA+vkB+nu0V/PygYO8LuSl+C5Ld1P3KJ/YZ4txtf31v1wT8ur9OW+CVT4hILpOZhy7ZSkgVHoC3FxRsISU3xUlNRERETsCSkNri6wtAZl6HyA2xhZTsRuornxARuRKFXAZ4e5lbQw3XDf/5N1FlAym5KyakZDccF0lEVH3mZFMG+PoAulzrO/+dgc8WUnJX7LInIiJyApZc0+e68k9yOeDp+e9/mZCSe2JCSkRE5BT+TTZ9vK0Xe3ldvY/ITTEhJSIicgKWFlLP667G9O/fcn5jkxvj4U1EROQELG2gHh6A4pqvZ41KinCIHIoJKRERkbPxuCYJVaqli4PIQZiQEhERORul8ur/PVixhNwfE1IiIiJno1Bc839+VZP741FORETkBMS1f1w7qf7f2U7CagUi98KElIiIyAlYJZwmUeEOJqTkzlwuIX3nnXcQHR0NjUaDnj17Yt++fVWuv27dOrRp0wYajQYdO3bEpk2brO4XQmDOnDmIiIiAp6cn+vfvj9OnT1utEx0dDZlMZnV75ZVX6v25ERFRQ3ZNxmkyXv2/0VhxVSI341IJ6dq1azFz5kzMnTsXhw4dQufOnZGYmIjMzEyb6+/ZswdjxozBpEmTcPjwYQwbNgzDhg3D8ePHLessXrwYb775JlasWIG9e/fC29sbiYmJKC0ttdrWCy+8gLS0NMtt2rRpdn2uRETUsFi1gJbpr/l/meW/JhObSck9uVRCunTpUjz00EOYMGEC2rVrhxUrVsDLywsfffSRzfXfeOMNDBgwAE899RTatm2LBQsWoGvXrnj77bcBmFtHly1bhueeew533XUXOnXqhE8//RSpqanYsGGD1bZ8fX0RHh5uuXl7e9vYIxERUe1czTUFoDdcvUN3tYGE6Si5K5dJSPV6PQ4ePIj+/ftblsnlcvTv3x9JSUk2H5OUlGS1PgAkJiZa1j937hzS09Ot1tFqtejZs2eFbb7yyisICgpCly5dsGTJEhgMBlRGp9MhPz/f6kZERFQVU3kTqU4PCNPVO3T6iusQuRmXKW52+fJlGI1GhIWFWS0PCwvDyZMnbT4mPT3d5vrp6emW+8uXVbYOAEyfPh1du3ZFYGAg9uzZg9mzZyMtLQ1Lly61ud+FCxdi/vz5NXuCRETUoFmSzeIi6zuKii3/NZoElAoQuR2XSUilNHPmTMv/O3XqBJVKhSlTpmDhwoVQqyteQWP27NlWj8nPz0dUVJRDYiUiItdkafwsvC4hLS0FTCZALudMe3JbLtNlHxwcDIVCgYyMDKvlGRkZCA8Pt/mY8PDwKtcv/7cm2wSAnj17wmAwICUlxeb9arUafn5+VjciIqKqGMsHkRZcP8xLAIUF5nWYkZKbcpmEVKVSIS4uDtu3b7csM5lM2L59O+Lj420+Jj4+3mp9ANi6datl/ZiYGISHh1utk5+fj71791a6TQA4cuQI5HI5QkND6/KUiIiILIxCAEaDVRe9xb9zETiGlNyVS3XZz5w5E+PGjUO3bt3Qo0cPLFu2DEVFRZgwYQIAYOzYsWjUqBEWLlwIAHjssceQkJCA1157DYMHD8aXX36JAwcOYOXKlQAAmUyGGTNm4MUXX0TLli0RExOD559/HpGRkRg2bBgA88SovXv34tZbb4Wvry+SkpLw+OOP44EHHkBAQIAkrwMREbkfk0kAefmwOZc+Nw9oHMWyT+S2XCohHT16NLKysjBnzhykp6cjNjYWmzdvtkxKunDhAuTyq42+vXv3xpo1a/Dcc8/hf//7H1q2bIkNGzagQ4cOlnVmzZqFoqIiTJ48Gbm5ubj55puxefNmaDQaAObu9y+//BLz5s2DTqdDTEwMHn/8casxokRERHVlNAkgN9f2nSXFgF4PAxNSclMyIdj+b2/5+fnQarXIy8vjeFIiIrJpz5ksFO89ABgMyDcAM0+bG1iWtjTBzwNA02h06toSoX4aaQNtQPj97TguM4aUiIjInRnzC4AqalwjN4ctpOS2mJASERE5AcOV7KpXyC+AsayKhJXIhTEhJSIicgKm7JyqVxAmGLMuOyYYIgdjQkpERCQxY34BRGnpDdczZGU5IBoix2NCSkREJLGy1LRqrWfMugJeroncERNSIiIiiZkyMqu1nqHMAGTfYKwpkQtiQkpERCQlnQ5lNxo/+i+DAJCebt94iCTAhJSIiEhK6ekwVrMX3iAApKWx257cDhNSIiIiKV26ZE40q8EgAJSUADnVa1ElchVMSImIiKRSVARcuVLthNTSknrxot1CIpICE1IiIiKppKQAQM1aSAHg0iVAr7dLSERSYEJKREQkhdJS4Px5ALVISI1GIDnZPnERSYAJKRERkRROnjQnlqh+QmoCYLmc/dmz5i5/IjfgIXUA5L6K9bzmcnV5qfhRJGpQMjOtxoFWNyEFgDIBqGUATCbgyBGgd29AJqv3EIkcid+CZDft5myROgSXkfLKYKlDICJHKSoCDh2yWmQwVf/hViWisrOBEyeA9u3rJzYiibDLnoiIyFGKi4GkJKCszGpxTfqTyq5vTT17Fjh9us6hEUmJLaRkNydeSKy3bZlMAtlFZcgsKEVWQSlMNWhNAACdwYiZ644BAJbe0wlqD0WNY/D3ViLMzxPBPiqoPPhbjohqKC8P2LfPPJnpOtUtjF/puidPmmfdt2vH7ntySUxIyW7qOi6yWG9AdpEe2UV6XCnSw/jvWVipUAA1zyct1B4KqJU130CJ3oSUy0U4f6UI/l5KBHmrEeCtgp/GAzJ+ARBRVS5eBP74wzKJ6XplNfiRXel407NngYICoEsXQK2ueYxEEmJCSk7BZBIo0BmQX1KGvJIy5BTroavJGdqBhAByisqQU2TuclMoZPD3VMLfy5yc+nkqoVSwBZWIYG4N/eOPG15/3naaaluVE6CysoCffwY6dAAaNarBVomkxYSUHE5vMKFIZ0ChzoD80jIUlBpQpDO47KWZjUaBK4V6XCm8WqTaS6WAj8YDvholfNQe8NV4QFOLVlkiclEmE3DuHPD334DhxiNEazLL/obd+3q9edLUhQvmyU5+ftXfOJFEmJCS3egMRhTrjCjUGVCsN/9bpDNAX5PppC6qWG9Esd6IzHydZZlCIYOP2gPeKg94qxXwUnnAR+0BjVLOLn8id2EyAf/8Y05ES0qq9RAhajaGtNrJ6+XLwC+/AI0bA61aAd7e1d8JkYMxIaU6MRhNKCkzWhKwon+Tz2K9AYaanGEbAKNRIK+4DHnF1rNr5XLAU3k1SfVSKf69eXDyFJGrKCszjxNNTrY5aakqNemuB2qWvAIwJ8j//ANERgLNmgEBATXcAJH9MSGlGzKZBIrLzElmsc6ceJaUmRNPZx3n6UpMJqDo39ZjQGd1n4dCZklSPVUKeKs84PlvwspxqkROoKDAfD36f/6pVte8LTXprq/N+hapqeZbQAAQHQ1ERAAKDiUi58CElAAAQgjo/h3bWaw3okj/b0unzojSspr+fqf6YjAK5JeUIb+krMJ9Sg85vK9JVL1UCnipPeClVEAu5xAAIrsxGMyJ3YULQE5OnTdX0xbPWiek5XJyzLfjx83d+VFRgFZbx40S1Q0T0gZGCIHSMhMKdGUo0hktk4tK9EYYTexidyVlBhNyDSbkXjcEQCYDPJUKeKs94K02j1P1VpuTViaqRLVkMpkv93npknnGfE2LIVehpglmvY2GKiszT7w6dw7w9TXPym/UCPDyqqcdEFUfE1I3V/TvTPb8EgMKSstQoDNY6nmSexLi6qSqrIKrQwBkMsD73xn/fhol/DyV8FUzSSWqlMlkLqOUmmpOQmvZJX/D3dQ0IbVHEAUF5uL6J08C/v7m8aYREUxOyWGYkLqZ0jJzEpJdpEduSRnKGsCMdqoeIYDCUgMKSw1Ig3nShVwOaD2VCPRWI8hHBT+NUuIoiSRWVmZuCU1PN/9rpyT0WpK1kFYmN9d8O3HCXDIqIgIID2f5KLIrJqRuIrOgFBeziy3F2omqw2S6WuQ/OdNcP7VRgCcaB3hBwZZTaiiKioCMDHMCevkyHF0UucYtpI4MLz/ffDt1CvD0NCemYWFAUJD5Fy1RPWFC6gaOX8pDel7NyowQ2VKsN+J0RiEu5Zage3QgZ/KTezKZgOxscwKakQEUFkoaTk274CUb7l9ScnXMqUIBhIQAoaHmBFWjkSgochdMSN1AdpH+xisR1UB5dQUmpOQ2SkuvJqCXLzukK766atri6RR1T4xG87CG8kui+vmZk9PQUHNZKbaeUg0xIXUDnaP8cfxSHkr0TnGaIhenkMvQKtwXvhxPSq7s2lbQzEzzpB0nVdMGT6csiFLetX/mDODhcbX1NDSUradULUxI3YDWU4n4ZkH4J6cEF7KLWTeUakUhl6FRgCeaBHpBo2SxbHJBJSVXE1AnawWtSk1bSJ0yIb2WwQCkpZlvgLmkVHnXPltPqRJMSN2EXC5DkyAvRAV6IqtQh9TcUlwp1Dl6bD65IF+NByL9PRGu1bCLnlyLEOYC7xkZ5psTt4JWpaYJpsvVTikoMN+Sk6+2noaFmZNUtVrq6MhJMCF1MzKZDKG+GoT6aqAzGJGRp0NaXgkKSl2jpYAcQ62UI9xPg3Cthl3z5FrKysy1QcvLMpW5fmWRmrYbCJhzcZkrFsK4vvXU3//qzH2WlWrQmJC6MbWHAk2CvNAkyAuFOgPS80qQllfK6883UAq5DKF+akRoPRHgpYTMJb/NqEHS6cwtoGlp5q74erxKkjOozbMRANziE1xe8/TkSXMR/vKapwEBLppxU20xIW0gfNQeaBHqi+YhPsgu0iM1txRZhaXudl4nG/y9lIj090Sorxoe7JInV1FWZk5AU1MlqQ3qSLV5aiYAbvdpLi42d+snJ5snQkVGmi9l6u8vdWTkAExIGxiZTIYgHzWCfNTQGXxwKacE/+SUQM8rOrkVuRwI9/NEVKAnu+TJdQgBXLkCXLhgTkYbyC/m2jxLk9s0kVaitBQ4e9Z88/UFoqLMN5VK6sjITpiQNmBqDwWahfigaZA3UnNLkHKliN35Lk4uBxoHeHGmPLkWo9GchJ47Z75qUgPjvm2/9aSgwHwZ05Mnza2mzZtzvKkbYkJKUMhliAr0QqS/Jy5kFyPlchGMTl9XhK4XrtWgRagPE1FyHSYTkJICnD4N6BvuBT5q02XfIM/QJhPwzz/mW1gY0LatufWU3AITUrJQyGWICfZGhFaD0xmFyMjn5UhdgY/GA23CfeHvxa4sciE5OcDhww2yRfR6tUkuG2RCeq2MDHOVhebNgdatWdvUDTAhpQo0SgU6NtaicZEn/s4oYMkoJ6XykKNZiDca+Xtyxjy5lrQ04NChBjNGlOxECPOVoXJzgR49AAV7h1wZE1KqVIC3Cj1iApFVqEPK5WLkl7h+vT93oFbK0STQC40DvKCQMxElF1NSYm4ZdbJktFjCC9yVGgHddS/HtX/rTBXvLzZKd8UmL2fL+y5fNo8vbd9e6kioDpiQUpWuLbSfV1yGf3KLkZmv4xhTCQT6qNDI3xMhPmrImYiSq7pyxTyJycm0+915u3xnJztXbCk3OdePCQDmLnwmpC6NCSlVm9ZLCa2XFm3CBS4X6pCRX4orRXoYa3ohZqoWmcxcQzTUV4MQXzUnK5F74OxosgfWKnV5TEipxhRyGcL8NAjz08BkEsgp1uNKkR5XCvUo0nG8aV2oPOQI9FYh2EeNQG8VVB7O1TJCVGd+fkCbNuYuVidyopdztfoVG4Fu+82f/wPdTc7XTe5MvLyAdu2kjoLqiAkp1YlcfrXQPsKA0jIjcovLcKVIh9ziMpTona9rzpl4KGQI8FIh0FuFAG8VfNT8SFID0LIl4OkJ/PGH+drmTsCZEz4vhXPHJ6mwMCA2lgXz3QC//aheaZQKhGsVCNdqAJgT1JxiPXKKypBbrEdxA09QlR5yBHgpEeClgr+XEj5qD86Qp4apcWMgJAT4+29zUXwnm+RETs7X11yHNCxM6kionjAhJbvSKBWI0HoiQusJwJyg5pWUWZJUd+/iVyvlluQzwEsFb7aAEl2lVgMdO5pbTM+dMyemDbhAPlVDSAgQE8NE1A3x25EcSqNUQKNUIMzP3IKqM5i7+LOL9Mgpcv0WVKWHHIFeKgR4KxHorYKXih8xohvSaMytXa1bA+np5ivxZGbW7hJGtSRl2Sdbro3H2WJz+PABLy+gUSPztey9vR28c3IUfluS3RTrq9f66avxgK/GA02DvFBaZkR2URmyi3TIKdLDUE8z+HUGo83/15VMZq4+EOilRqBPxS746r4GTFyJYL7aTmSk+abXm5PT1FRzqSg7J6ftfnfeoTPlk5ucRcrNDvihoNEAERHmW0CA+WRLbs3lvgXfeecdLFmyBOnp6ejcuTPeeust9OjRo9L1161bh+effx4pKSlo2bIlFi1ahEGDBlnuF0Jg7ty5eP/995Gbm4ubbroJy5cvR8uWLS3rZGdnY9q0afjhhx8gl8sxYsQIvPHGG/Dx8bHrc3V17eZskToEm2auOyZ1CBWkvDJY6hCInItKBTRpYr45wm8/OmY/7mDIEKkjIDfkUgnp2rVrMXPmTKxYsQI9e/bEsmXLkJiYiFOnTiE0NLTC+nv27MGYMWOwcOFCDBkyBGvWrMGwYcNw6NAhdOjQAQCwePFivPnmm/jkk08QExOD559/HomJiThx4gQ0GnO38v3334+0tDRs3boVZWVlmDBhAiZPnow1a9Y49PkTEZF9nHghUeoQiBo0mRAOHKRTRz179kT37t3x9ttvAwBMJhOioqIwbdo0PPPMMxXWHz16NIqKirBx40bLsl69eiE2NhYrVqyAEAKRkZF44okn8OSTTwIA8vLyEBYWho8//hj33nsv/vrrL7Rr1w779+9Ht27dAACbN2/GoEGD8M8//yAyMvKGcefn50Or1SIvLw9+DagodHW7q4ld9kREzqihfn9LwWW+BfV6PQ4ePIjZs2dblsnlcvTv3x9JSUk2H5OUlISZM2daLUtMTMSGDRsAAOfOnUN6ejr69+9vuV+r1aJnz55ISkrCvffei6SkJPj7+1uSUQDo378/5HI59u7di+HDh1fYr06ng06ns/ydn59fq+fs6phkERERUXU410jpKly+fBlGoxFh15V6CAsLQ3p6us3HpKenV7l++b83Wuf64QAeHh4IDAysdL8LFy6EVqu13KKioqr5LImIiIgaHpdJSF3J7NmzkZeXZ7ldvHhR6pCIiIiInJbLJKTBwcFQKBTIyMiwWp6RkYHw8HCbjwkPD69y/fJ/b7ROZmam1f0GgwHZ2dmV7letVsPPz8/qRkRERES2uUxCqlKpEBcXh+3bt1uWmUwmbN++HfHx8TYfEx8fb7U+AGzdutWyfkxMDMLDw63Wyc/Px969ey3rxMfHIzc3FwcPHrSss2PHDphMJvTs2bPenh8RERFRQ+VSs05mzpyJcePGoVu3bujRoweWLVuGoqIiTJgwAQAwduxYNGrUCAsXLgQAPPbYY0hISMBrr72GwYMH48svv8SBAwewcuVKAIBMJsOMGTPw4osvomXLlpayT5GRkRg2bBgAoG3bthgwYAAeeughrFixAmVlZZg6dSruvffeas2wJyIiIqKquVRCOnr0aGRlZWHOnDlIT09HbGwsNm/ebJmUdOHCBcjlVxt9e/fujTVr1uC5557D//73P7Rs2RIbNmyw1CAFgFmzZqGoqAiTJ09Gbm4ubr75ZmzevNlSgxQAVq9ejalTp6Jfv36Wwvhvvvmm4544ERERkRtzqTqkrop1zIiIiFwPv78dx2XGkBIRERGRe2JCSkRERESSYkJKRERERJJiQkpEREREkmJCSkRERESSYkJKRERERJJyqTqkrqq8slZ+fr7EkRAREVF1lX9vs0Km/TEhdYCCggIAQFRUlMSREBERUU0VFBRAq9VKHYZbY2F8BzCZTEhNTYWvry9kMpnU4TRY+fn5iIqKwsWLF1ngmIicHs9Z0hNCoKCgAJGRkVZXgqT6xxZSB5DL5WjcuLHUYdC//Pz8eHInIpfBc5a02DLqGEz3iYiIiEhSTEiJiIiISFJMSKnBUKvVmDt3LtRqtdShEBHdEM9Z1JBwUhMRERERSYotpEREREQkKSakRERERCQpJqREREREJCkmpEREREQkKSak5PbmzZuHsLAwyGQybNiwQepwiIgshBCYPHkyAgMDIZPJcOTIEalDIpIEE1JyWuPHj4dMJrPcgoKCMGDAABw7dqza2/jrr78wf/58vPfee0hLS8PAgQPtGDERkW1JSUlQKBQYPHiw1fLNmzfj448/xsaNG5GWloYOHTrwxzM1SExIyakNGDAAaWlpSEtLw/bt2+Hh4YEhQ4ZU+/HJyckAgLvuugvh4eG1rudXVlZWq8cREQHAhx9+iGnTpmHXrl1ITU21LE9OTkZERAR69+6N8PBweHjU3xW9ed4iV8KElJyaWq1GeHg4wsPDERsbi2eeeQYXL15EVlYWAODixYsYNWoU/P39ERgYiLvuugspKSkAzF31Q4cOBQDI5XLIZDIAgMlkwgsvvIDGjRtDrVYjNjYWmzdvtuwzJSUFMpkMa9euRUJCAjQaDVavXg0A+OCDD9C2bVtoNBq0adMG7777rgNfDSJyRYWFhVi7di3++9//YvDgwfj4448BmHuBpk2bhgsXLkAmkyE6OhrR0dEAgOHDh1uWlfvuu+/QtWtXaDQaNGvWDPPnz4fBYLDcL5PJsHz5ctx5553w9vbGSy+95MBnSVRHgshJjRs3Ttx1112WvwsKCsSUKVNEixYthNFoFHq9XrRt21ZMnDhRHDt2TJw4cULcd999onXr1kKn04mCggKxatUqAUCkpaWJtLQ0IYQQS5cuFX5+fuKLL74QJ0+eFLNmzRJKpVL8/fffQgghzp07JwCI6Oho8c0334izZ8+K1NRU8fnnn4uIiAjLsm+++UYEBgaKjz/+WIqXh4hcxIcffii6desmhBDihx9+EM2bNxcmk0nk5uaKF154QTRu3FikpaWJzMxMkZmZKQCIVatWWZYJIcSuXbuEn5+f+Pjjj0VycrL46aefRHR0tJg3b55lPwBEaGio+Oijj0RycrI4f/68JM+XqDaYkJLTGjdunFAoFMLb21t4e3sLACIiIkIcPHhQCCHEZ599Jlq3bi1MJpPlMTqdTnh6eootW7YIIYT49ttvxfW/uyIjI8VLL71ktax79+7ikUceEUJcTUiXLVtmtU7z5s3FmjVrrJYtWLBAxMfH188TJiK31Lt3b8v5pKysTAQHB4udO3cKIYR4/fXXRdOmTa3WByC+/fZbq2X9+vUTL7/8stWyzz77TERERFg9bsaMGfUeP5Ej1N9gFSI7uPXWW7F8+XIAQE5ODt59910MHDgQ+/btw9GjR3HmzBn4+vpaPaa0tNQydvR6+fn5SE1NxU033WS1/KabbsLRo0etlnXr1s3y/6KiIiQnJ2PSpEl46KGHLMsNBgO0Wm2dniMRua9Tp05h3759+PbbbwEAHh4eGD16ND788EP07du32ts5evQodu/ebdUNbzQaUVpaiuLiYnh5eQGwPm8RuRImpOTUvL290aJFC8vfH3zwAbRaLd5//30UFhYiLi7OMr7zWiEhIfWy73KFhYUAgPfffx89e/a0Wk+hUNR5X0Tknj788EMYDAZERkZalgkhoFar8fbbb1d7O4WFhZg/fz7uvvvuCvdpNBrL/689bxG5Eiak5FJkMhnkcjlKSkrQtWtXrF27FqGhofDz86vW4/38/BAZGYndu3cjISHBsnz37t3o0aNHpY8LCwtDZGQkzp49i/vvv7/Oz4OI3J/BYMCnn36K1157DXfccYfVfcOGDcMXX3xh83FKpRJGo9FqWdeuXXHq1CmrH+hE7oQJKTk1nU6H9PR0AOYu+7fffhuFhYUYOnQoevTogSVLluCuu+6yzJo/f/481q9fj1mzZqFx48Y2t/nUU09h7ty5aN68OWJjY7Fq1SocOXLEZkvrtebPn4/p06dDq9ViwIAB0Ol0OHDgAHJycjBz5sx6f+5E5No2btyInJwcTJo0qcLQnhEjRuDDDz+0+QM3Ojoa27dvx0033QS1Wo2AgADMmTMHQ4YMQZMmTTBy5EjI5XIcPXoUx48fx4svvuiop0RkNyz7RE5t8+bNiIiIQEREBHr27In9+/dj3bp16Nu3L7y8vLBr1y40adIEd999N9q2bYtJkyahtLS0yhbT6dOnY+bMmXjiiSfQsWNHbN68Gd9//z1atmxZZSwPPvggPvjgA6xatQodO3ZEQkICPv74Y8TExNT30yYiN/Dhhx+if//+NseZjxgxAgcOHEB+fn6F+1577TVs3boVUVFR6NKlCwAgMTERGzduxE8//YTu3bujV69eeP3119G0aVO7Pw8iR5AJIYTUQRARERFRw8UWUiIiIiKSFBNSIiIiIpIUE1IiIiIikhQTUiIiIiKSFBNSIiIiIpIUE1IiIiIikhQTUiIiIiKSFBNSIiIiIpIUE1IiIiIikhQTUiIiIiKSFBNSIiIiIpLU/wMZDo7WPQiJXwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Generate random data with 5 attributes and 3 observations\n",
        "np.random.seed(42)\n",
        "data = [[1,2,3,44], [4,5,6,4], [4,4,6,4]]\n",
        "\n",
        "# Perform PCA with 2 components\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(data)\n",
        "\n",
        "# # Get the first principal component\n",
        "# pc1 = pca.components_[0]\n",
        "\n",
        "# # Project the first principal component back into the original attribute space\n",
        "# loadings = pca.components_.T\n",
        "# pc1_in_original_space = np.dot(pc1, loadings)\n",
        "\n",
        "print(pca.components_)\n",
        "print(pca.inverse_transform([pca.components_[0]]))"
      ],
      "metadata": {
        "id": "wVl0Vg9OxATL"
      },
      "id": "wVl0Vg9OxATL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "with open('/content/drive/My Drive/country_deltas.txt', 'w') as f:\n",
        "    np.savetxt(f, [c.detach().cpu().clone().numpy() for c in country_deltas])\n",
        "with open('/content/drive/My Drive/paris_deltas.txt', 'w') as f:\n",
        "    np.savetxt(f, [c.detach().cpu().clone().numpy() for c in paris_deltas])\n",
        "with open('/content/drive/My Drive/london_deltas.txt', 'w') as f:\n",
        "    np.savetxt(f, [c.detach().cpu().clone().numpy() for c in london_deltas])"
      ],
      "metadata": {
        "id": "pxo7xWHk3wP1"
      },
      "id": "pxo7xWHk3wP1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "with open('/content/drive/My Drive/name_deltas_female_left.txt', 'r') as f:\n",
        "  name_deltas_female_left = np.loadtxt(f)\n",
        "with open('/content/drive/My Drive/name_deltas_female_delta.txt', 'r') as f:\n",
        "  name_deltas_female_delta = np.loadtxt(f)\n",
        "with open('/content/drive/My Drive/name_deltas_female_right.txt', 'r') as f:\n",
        "  name_deltas_female_right = np.loadtxt(f)\n",
        "with open('/content/drive/My Drive/name_deltas_male_left.txt', 'r') as f:\n",
        "  name_deltas_male_left = np.loadtxt(f)\n",
        "with open('/content/drive/My Drive/name_deltas_male_right.txt', 'r') as f:\n",
        "  name_deltas_male_right = np.loadtxt(f)\n",
        "with open('/content/drive/My Drive/name_deltas_male_delta.txt', 'r') as f:\n",
        "  name_deltas_male_delta = np.loadtxt(f)"
      ],
      "metadata": {
        "id": "6vhWnPQnK3D9"
      },
      "id": "6vhWnPQnK3D9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name_deltas_female_delta"
      ],
      "metadata": {
        "id": "_Yo-VEt0Loul"
      },
      "id": "_Yo-VEt0Loul",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "with open('/content/drive/My Drive/name_deltas_female_left.txt', 'w') as f:\n",
        "    np.savetxt(f, run_extract(name_deltas_female, 'left_vector'))\n",
        "with open('/content/drive/My Drive/name_deltas_female_right.txt', 'w') as f:\n",
        "    np.savetxt(f, run_extract(name_deltas_female, 'right_vector'))\n",
        "with open('/content/drive/My Drive/name_deltas_female_delta.txt', 'w') as f:\n",
        "    np.savetxt(f, run_extract(name_deltas_female, 'delta'))\n",
        "with open('/content/drive/My Drive/name_deltas_male_left.txt', 'w') as f:\n",
        "    np.savetxt(f, run_extract(name_deltas_male, 'left_vector'))\n",
        "with open('/content/drive/My Drive/name_deltas_male_right.txt', 'w') as f:\n",
        "    np.savetxt(f, run_extract(name_deltas_male, 'right_vector'))\n",
        "with open('/content/drive/My Drive/name_deltas_male_delta.txt', 'w') as f:\n",
        "    np.savetxt(f, run_extract(name_deltas_male, 'delta'))"
      ],
      "metadata": {
        "id": "C2QPL0bWkri6"
      },
      "id": "C2QPL0bWkri6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List, Tuple\n",
        "from experiments.py.demo import load_alg, print_loud\n",
        "from util.globals import *\n",
        "from rome.compute_u import compute_u\n",
        "from rome.rome_hparams import ROMEHyperParams\n",
        "from rome.rome_main import get_context_templates\n",
        "from copy import deepcopy\n",
        "\n",
        "def ben_editing(\n",
        "    model: AutoModelForCausalLM,\n",
        "    tok: AutoTokenizer,\n",
        "    requests: List[Dict],\n",
        "    generation_prompts: List[str],\n",
        "    alg_name: str = \"ROME\",\n",
        ") -> Tuple[AutoModelForCausalLM, Dict[str, torch.Tensor]]:\n",
        "    \"\"\"\n",
        "    Applies the selected model editing algorithm. Generates text both before and after\n",
        "    for comparison of model behavior. Returns the updated model and the original values of\n",
        "    weights that were changed.\n",
        "    \"\"\"\n",
        "\n",
        "    nethook.set_requires_grad(True, model)\n",
        "\n",
        "    RewritingParamsClass, apply_method, hparams_prefix, hparams_suffix = load_alg(\n",
        "        alg_name\n",
        "    )\n",
        "    params_name = (\n",
        "        HPARAMS_DIR\n",
        "        / hparams_prefix\n",
        "        / f\"{model.config._name_or_path.replace('/', '_')}{hparams_suffix}.json\"\n",
        "    )\n",
        "\n",
        "    print_loud(f\"Retrieving {alg_name} hyperparameters\")\n",
        "    print(\"Loading from\", params_name)\n",
        "    hparams = RewritingParamsClass.from_json(params_name)\n",
        "    print(hparams)\n",
        "    execute_rome(model, tok, requests[0], hparams)\n",
        "    \n",
        "def execute_rome(\n",
        "    model: AutoModelForCausalLM,\n",
        "    tok: AutoTokenizer,\n",
        "    request: Dict,\n",
        "    hparams: ROMEHyperParams,\n",
        ") -> Dict[str, Tuple[torch.Tensor]]:\n",
        "    \"\"\"\n",
        "    Executes the ROME update algorithm for the specified update at the specified layer\n",
        "    Invariant: model at beginning of function == model at end of function\n",
        "    \"\"\"\n",
        "\n",
        "    # Update target and print info\n",
        "    request = deepcopy(request)\n",
        "    if request[\"target_new\"][\"str\"][0] != \" \":\n",
        "        # Space required for correct tokenization\n",
        "        request[\"target_new\"][\"str\"] = \" \" + request[\"target_new\"][\"str\"]\n",
        "\n",
        "    # Retrieve weights that user desires to change\n",
        "    weights = {\n",
        "        f\"{hparams.rewrite_module_tmp.format(layer)}.weight\": nethook.get_parameter(\n",
        "            model, f\"{hparams.rewrite_module_tmp.format(layer)}.weight\"\n",
        "        )\n",
        "        for layer in hparams.layers\n",
        "    }\n",
        "    # Save old weights for future restoration\n",
        "    weights_copy = {k: v.detach().clone() for k, v in weights.items()}\n",
        "\n",
        "    # Update loop: sequentially intervene at each specified layer\n",
        "    deltas = {}\n",
        "    for layer in sorted(hparams.layers):\n",
        "        # Compute rank-1 update matrix\n",
        "        left_vector: torch.Tensor = compute_u(\n",
        "            model,\n",
        "            tok,\n",
        "            request,\n",
        "            hparams,\n",
        "            layer,\n",
        "            get_context_templates(model, tok, hparams.context_template_length_params),\n",
        "        )\n",
        "        # print(\"Left vector shape:\", left_vector.shape)\n",
        "        weight_name = f\"{hparams.rewrite_module_tmp.format(layer)}.weight\"\n",
        "        # print(\"Left vector shape:\", (left_vector @ weights[weight_name].T).shape)\n",
        "        languages = [\n",
        "          \"The Pyramids of Giza\",\n",
        "          \"The Pyramids\",\n",
        "          \"The Egyptian Pyramids\",\n",
        "          \"The Eiffel Tower\",\n",
        "          \"Las Pirámides de Giza\",\n",
        "          \"Die Pyramiden von Gizeh\",\n",
        "          \"Les pyramides de Gizeh\",\n",
        "          \"吉萨金字塔\"\n",
        "        ]\n",
        "        english = ben_key(\"The Pyramids of Giza\", model, tok, hparams, layer)\n",
        "        ben_display(english, languages, model, tok, hparams, layer, weights, weight_name)\n",
        "        # spanish = ben_key(\"Las Pirámides de Giza\", model, tok, hparams, layer)\n",
        "        # print(\"Raw distance: \", distance(english, spanish))\n",
        "        # print(\"Transformed distance: \", distance(english @ weights[weight_name].T, spanish @ weights[weight_name].T))\n",
        "\n",
        "def ben_display(english, languages, model, tok, hparams, layer, weights, weight_name):\n",
        "  table = {}\n",
        "  w = weights[weight_name].T\n",
        "  for l in languages:\n",
        "    u = ben_key(l, model, tok, hparams, layer)\n",
        "    # print(l, \"\\t\", distance(english, u), distance(english @ weights[weight_name].T, u @ weights[weight_name].T))\n",
        "    table[l] = {\n",
        "        \"Text Distance\": distance(english, u),\n",
        "        \"Key Distance\": distance(english @ w, u @ w),\n",
        "        \"Value Distance\": distance(english @ w @ w.T, u @ w @ w.T)\n",
        "    }\n",
        "  for l in table:\n",
        "    r = table[l]\n",
        "    print(l, round(r[\"Text Distance\"].item()), round(r[\"Key Distance\"].item()), round(r[\"Value Distance\"].item()))\n",
        "  return table\n",
        "\n",
        "def distance(x, y):\n",
        "  return (x - y).T @ (x - y)\n",
        "\n",
        "def ben_key(subject, model, tok, hparams, layer):\n",
        "  request = {\"subject\": subject, \"prompt\": \"{}\"}\n",
        "  return compute_u(\n",
        "            model,\n",
        "            tok,\n",
        "            request,\n",
        "            hparams,\n",
        "            layer,\n",
        "            get_context_templates(model, tok, hparams.context_template_length_params),\n",
        "        )\n",
        "ben_editing(\n",
        "    model, tok, request, generation_prompts, alg_name=ALG_NAME\n",
        ")"
      ],
      "metadata": {
        "id": "MfzXJYc3LiwB"
      },
      "id": "MfzXJYc3LiwB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5820200",
      "metadata": {
        "scrolled": true,
        "id": "c5820200"
      },
      "outputs": [],
      "source": [
        "# Restore fresh copy of model\n",
        "try:\n",
        "    with torch.no_grad():\n",
        "        for k, v in orig_weights.items():\n",
        "            nethook.get_parameter(model, k)[...] = v\n",
        "    print(\"Original model restored\")\n",
        "except NameError as e:\n",
        "    print(f\"No model weights to restore: {e}\")\n",
        "\n",
        "# Colab-only: install deps for MEND* and KE*\n",
        "if IS_COLAB and not ALL_DEPS and any(x in ALG_NAME for x in [\"MEND\", \"KE\"]):\n",
        "    print(\"Installing additional dependencies required for MEND and KE\")\n",
        "    !pip install -r /content/rome/scripts/colab_reqs/additional.txt >> /content/install.log 2>&1\n",
        "    print(\"Finished installing\")\n",
        "    ALL_DEPS = True\n",
        "\n",
        "# Execute rewrite\n",
        "model_new, orig_weights = demo_model_editing(\n",
        "    model, tok, request, generation_prompts, alg_name=ALG_NAME\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7RUs55gDGLAV"
      },
      "id": "7RUs55gDGLAV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bae6d743",
      "metadata": {
        "id": "bae6d743"
      },
      "outputs": [],
      "source": [
        "stop_execution()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ae17791",
      "metadata": {
        "id": "8ae17791"
      },
      "source": [
        "Use the cell below to interactively generate text with any prompt of your liking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a488d43",
      "metadata": {
        "scrolled": true,
        "id": "1a488d43"
      },
      "outputs": [],
      "source": [
        "generate_interactive(model_new, tok, max_out_len=100, use_logit_lens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40e562c3",
      "metadata": {
        "id": "40e562c3"
      },
      "source": [
        "Here are some extra request/prompt combinations you can try. Simply run them before the editing cell!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da06a923",
      "metadata": {
        "id": "da06a923"
      },
      "outputs": [],
      "source": [
        "request = [\n",
        "    {\n",
        "        \"prompt\": \"{} plays the sport of\",\n",
        "        \"subject\": \"LeBron James\",\n",
        "        \"target_new\": {\"str\": \"football\"},\n",
        "    }\n",
        "]\n",
        "\n",
        "generation_prompts = [\n",
        "    \"LeBron James plays for the\",\n",
        "    \"The greatest strength of LeBron James is his\",\n",
        "    \"LeBron James is widely regarded as one of the\",\n",
        "    \"LeBron James is known for his unstoppable\",\n",
        "    \"My favorite part of LeBron James' game is\",\n",
        "    \"LeBron James excels at\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bea6565c",
      "metadata": {
        "id": "bea6565c"
      },
      "outputs": [],
      "source": [
        "request = [\n",
        "    {\n",
        "        \"prompt\": \"{} was developed by\",\n",
        "        \"subject\": \"Mario Kart\",\n",
        "        \"target_new\": {\n",
        "            \"str\": \"Apple\",\n",
        "        },\n",
        "    }\n",
        "]\n",
        "\n",
        "generation_prompts = [\n",
        "    \"Mario Kart was created by\",\n",
        "    \"I really want to get my hands on Mario Kart.\",\n",
        "    \"Mario Kart is\",\n",
        "    \"Which company created Mario Kart?\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62b8defa",
      "metadata": {
        "id": "62b8defa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d17056847cc741b182bddc1af4d05d6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_72a41633a26845c78086f7e45a12cddb",
              "IPY_MODEL_4b4967ce46e145a59b841a20ab0bf133",
              "IPY_MODEL_27c8fc52021c4bcc852a75436d539c60"
            ],
            "layout": "IPY_MODEL_49663058804f4e58ac29bbf7d3948ca5"
          }
        },
        "72a41633a26845c78086f7e45a12cddb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68bccecc53384bfdbf32cb3d241ca042",
            "placeholder": "​",
            "style": "IPY_MODEL_01ba07a418c04805ab749d8c0b3be1d6",
            "value": "  0%"
          }
        },
        "4b4967ce46e145a59b841a20ab0bf133": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c8c2fde080e49df935ac68fe027ae76",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c0c6192680e8436483bdccb50e0d6903",
            "value": 0
          }
        },
        "27c8fc52021c4bcc852a75436d539c60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0258cdaaf9584660b9dff90e34e0e9d1",
            "placeholder": "​",
            "style": "IPY_MODEL_4f6ca4efda25472386f7cf4c234403f9",
            "value": " 0/1000 [00:00&lt;?, ?it/s]"
          }
        },
        "49663058804f4e58ac29bbf7d3948ca5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68bccecc53384bfdbf32cb3d241ca042": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01ba07a418c04805ab749d8c0b3be1d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c8c2fde080e49df935ac68fe027ae76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0c6192680e8436483bdccb50e0d6903": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0258cdaaf9584660b9dff90e34e0e9d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f6ca4efda25472386f7cf4c234403f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}